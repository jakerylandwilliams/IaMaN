# It's a Machine and Natural (IaMaN)
It's a Machine and Natural (IaMaN) is a framework for training artificial intelligence (AI) models on predictive tasks and operating them with deployable interaction. It uses an alternative approach to deep learning based on analytical optimization, i.e., without using stochastic procedures like gradient descent (and hence backpropagation). These approaches are based on some of the conclusions drawn from these works: 
- https://arxiv.org/pdf/2205.00148.pdf
- https://arxiv.org/pdf/2205.04376.pdf

An architectural diagram for this project's closed-form transformer can be seen below, followed by a table of this system's prototyping experiments, and then by a light description of its statistical foundations alongside design notes for each of its most salient components. Further documentation will accompany software improvements in the future.

---

<img align="right" src="https://github.com/jakerylandwilliams/IaMaN/blob/main/architecture.jpg" width="500" height="450"><br>
__Figure__: Pre-inductive layers of the prototype (the only ones currently implemented) are illustrated with context radius $r=2$, positional encodings resonating at unigram frequencies to a sinusoidal wavefunction, $\psi$, and $b=3$-bit contexts bringing target and prediction vectors, $y_i,\hat y_i\in R^{|W|}$ down to a $b$/low-dimensional size for contexts: $\vec h_i^{\psi,b}\in R^{b(2r+1)}$. When used for language modeling, the last $r+1$ contexts are suppressed (dotted sections of $\vec h_i^{\psi,b}$) as features, and all are used when transferring LM statistics for targets: $y$ or arbitrary tags: $\ell\in L$. Superscripts indicate layers numbers from bottom-to-top in, e.g., for decoder matrices: $U_C^{(k),\psi,b}\in R^{|W|\times b(2r+1)}$. Bottom layers computed sparsely: $F_{\mathcal{D},r,y}^{(0),\psi,b} = \sum y_i\otimes \vec h_i^{(0),\psi,b}$, while deeper layers (larger parenthetical superscripts) are computed via dense-context co-occurrences, thus making ad hoc depth possible via an expectation-maximization computation: $F_{\mathcal{D},r,y}^{(k),\psi,b} = \sum y_i\otimes \vec h_i^{(k),\psi,b}$. <br><br><br>

---

## Table of Prototyping Experiments
|Model                                                |Generation (perplexity)      |Segmentation (F-score)     |Part-of-speech (accuracy)     |
|:----------------------------------------------------|----------------------------:|--------------------------:|----------------------------:|
|Ind., $r:10$, $k:0\rightarrow1$                      |6951.23 $\rightarrow$ ?      | 62.45 $\rightarrow$ 63.50 | 17.99 $\rightarrow$ 17.10   |
|Ind., $r:10$, $k:0\rightarrow1$, $\psi$              |5951.55 $\rightarrow$ ?      | 64.23 $\rightarrow$ 64.84 | 18.79 $\rightarrow$ 17.69   |
|Ind., $r:10$, $k:0\rightarrow1$, $\psi$, IFE         |1798.39 $\rightarrow$ 790.40 | 51.74 $\rightarrow$ 53.67 | 15.30 $\rightarrow$ 17.93   |
|Ind., $r:10$, $k:0\rightarrow1$, $\psi$, $b:500$     |754.82 $\rightarrow$ 658.37  | 63.49 $\rightarrow$ 63.58 | 16.66 $\rightarrow$ 15.74   |
|Ind., $r:20$, $k:0\rightarrow1$, $\psi$, $b:250$     |667.39 $\rightarrow$ 593.56  | 61.82 $\rightarrow$ 61.47 | 15.0 $\rightarrow$ 13.83    |
|Dep., $r:10$, $k:0\rightarrow1$                      |? $\rightarrow$ ?            | 95.09 $\rightarrow$ ?     | 76.38 $\rightarrow$ ?       |
|Dep., $r:10$, $k:0\rightarrow1$, $\psi$              |? $\rightarrow$ ?            | 95.08 $\rightarrow$ ?     | 76.58 $\rightarrow$ ?       |
|Dep., $r:10$, $k:0\rightarrow1$, $\psi$, IFE         |666.49 $\rightarrow$ ?       | 88.43 $\rightarrow$ 88.39 | 70.46 $\rightarrow$ 63.39   |
|Dep., $r:10$, $k:0\rightarrow1$, $\psi$, $b:500$     |268.9 $\rightarrow$ 248.9    | 90.32 $\rightarrow$ 89.0  | 68.94 $\rightarrow$ 65.42   |
|Dep., $r:20$, $k:0\rightarrow1$, $\psi$, $b:250$     |354.01 $\rightarrow$ 360.57  | 92.06 $\rightarrow$ 89.05 | 63.96 $\rightarrow$ 59.75   |

Prototyping results for text __Generation__, token __Segmentation__ , and __Part-of-speech__ tagging. Evaluations for generation are measured by _perplexity_, segmentation's performance is evaluated by the _F-statistic_, and tagging performance is evaluated by _accuracy_. Arrows between performance values (and $k$) represent the transitions of models from single-layer feed-forward models, into two-layer networks with ad hoc attention distributions, which appears to provide smoothing benefits.

## Installation
To install IaMaN (and HR-BPE), run:
```
>> git clone https://github.com/jakerylandwilliams/IaMaN.git
>> cd src/utils/
>> git clone https://github.com/jakerylandwilliams/hr_bpe.git
>> cd ../../
```
After this, you should be able to use IaMaN as a prototype like in the demos ([tokenization](https://github.com/jakerylandwilliams/IaMaN/blob/main/demo-tok.ipynb), [generation](https://github.com/jakerylandwilliams/IaMaN/blob/main/demo-gen.ipynb), and [tagging](https://github.com/jakerylandwilliams/IaMaN/blob/main/demo-tag.ipynb)) from the initial directory where IaMaN was cloned.

## Closed Form Solutions to NLP Systems
Research has historically approached the characterization of language statistics by counting the occurrence of symbols. While _frequency_ can be measured at different levels, e.g., characters, tokens, or phrases, a statistical ubiquity was discovered early on for tokens&mdash;in some sense, harmonic relationships are 'normal distributions' for the usage of _any document's vocabulary_. To understand harmonic relationships, suppose a vocabulary $W$ of $|W| = N$ distinct _types_ is used to convey a document that is $M$ tokens long: $d = \{t_1, \cdots, t_M\}$. A harmonic analysis of $d$ first _ranks_ each $t\in V$ with a positive integer $n_t$ that sorts the vocabulary from high-to-low by frequency. Intuitively a rank, $n_t$ indicates the number of _other_ types which occur at least as often as $t$ (without loss of generality). By this ranking, the frequency for any type, $f_d(t)$, can be mathematically approximated by the harmonic function, where $C_d = N$ makes the model physically possible: 
$$f_d(t)\approx\hat{f}_d(n_t; N) = C_d\cdot n_t^{-1}$$
Non-harmonic subtleties of token distributions have long confounded the use of frequencies in engineering applications, leaving most studies of harmonic linguistic structure asking why it exists at all, rather than leverage it for applications. 

Vector representations allow modern deep learning systems to approximate the meanings of tokens, and the common source of statistical information that these algorithms use comes from co-frequency, or, _co-occurrence_&mdash;local distributions between tokens. Co-occurrence matrices measure the number of times tokens appear 'near' one another&mdash;for types $t$ and $s$, we will denote the occurrence of $s$ in a fixed window of size $\pm r$ tokens around $t$ across a collection, $\mathcal{D}$, by $F_{\mathcal{D},r}(t,s)$.

### Making Sense of the Space Effect
Preferential-selection, or, 'rich-get-richer' models produce harmonic token distributions that [necessarily deviate strongly for high frequencies](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.95.052301). [We've argued](https://arxiv.org/pdf/1710.07729.pdf) that these _negative_-shift deviations are in fact implicitly known to studies that proposed a _positive_-shift to token ranks to accommodate a different high-frequency deviation&mdash;in the wrong direction. Under this hypothesis, the root cause of the discrepancy is due to the exclusion of naturally-produced text from token-frequency distributions, particularly space and punctuation (which occur with extremely high frequencies). From this finer-grained analysis of the preferential-selection model, we've derived that a modification to the standard harmonic function is required to encompass high-frequency outliers in a rich-get-richer vocabulary: 
$$f_d(t)\approx\hat{f}_d(n_t; N,\theta) = C_d\cdot (n_t - \theta)^{-\theta}$$

with, $C_d = (N - \theta)^\theta$. The preference parameter, $\theta$, is a replication rate, i.e., time spent re-using tokens from the generator's history. The simple harmonic function's exponent of $-1$ is replaced by $-\theta$, making a shift of $n_t$ by $-\theta$, and top-rank frequencies _very large_ when $\theta$ is large, e.g., in spaced languages.

### Utilizing the Mixing Effect
Low-frequency divergences from the harmonic distribution were first observed for large documents collections, and [we've exhibited](https://journals.aps.org/pre/pdf/10.1103/PhysRevE.91.052811) that low-frequency divergences for large document collections are actually due to a 'mixing', where the sparsity in overlap of rare words _between_ documents produces less-than-harmonic frequencies _across_ a collection. This was recently extended to find an algebraic model that builds from the space-modified harmonic model to explain low-frequency divergences via a factor that is [well parameterized by mixture statistics](https://jakerylandwilliams.github.io/documents/williams2021a.pdf). Specifically, the token frequencies for a collection of documents $\mathcal{D}$ utilizing a _mixed_ vocabulary of $N$ types are described by a convolution of their harmonic frequencies&mdash;shifted by the space-effect&mdash;with a high-pass filter to define: 
$$f_\mathcal{D}(t) \approx  C_{\mathcal{D}}\cdot (n_t-\theta)^{-\theta} \left[1 - \left(1 + \frac{\langle n \rangle}{n_t}\right)^{-\frac{\langle N\rangle}{\langle n \rangle}}\right]$$

This equation's constant factor, $C_\mathcal{D}$, is determined as the reciprocal of the remaining factors under the substitution $n_t = N$. [Observing evidence](https://jakerylandwilliams.github.io/documents/williams2021a.pdf) that the $N^\text{th}$ harmonic number, $H_N$, and harmonic average of mixture ranks, $\langle n\rangle = \frac{N}{H_N}$ relate to the mean vocabulary size of the collection's documents: $\langle N\rangle$, this is then the only 'new' parameter, with respect to the space effect. Moreover, in experimentation $\theta= 1 - H_N^{-1}$ appears strongly predictive of frequency, making it a regression-free (naturally parameterized) model, i.e., one that is 'solved' by physical constants. This parameterization is especially powerful for regularized tokenization.

### Segmentation with Distributional Regularization
The widely-used [WordPiece algorithm](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf) can be seen as a regularized form of byte-pair encoding (BPE), which learns sub-token merge rules according to radius-1 co-occurence frequencies. While the regularization that makes WordPiece function is a language model, HR-BPE's regularization assumes only [the stationary form of a document-mixture's vocabulary](https://jakerylandwilliams.github.io/documents/williams2021a.pdf) for unsupervised learning information, and augments standard BPE by allowing split-actions. These differences, and a hyper-efficient frequency-based implementation, allow for HR-BPE to process arbitrary character systems under a highly expressive framework that is lossless, requires no pre-processing, and is likewise supervise-able. In particular, HR-BPE is capable of channeling external data-information through gold standard tokenizations, while still retaining sub-word expressivity and a hyper-efficient frequency-based loss function, the normalized cross-entropy loss function, which describes how strongly a document collection's vocabulary has its empirical frequency distribution predicted by a frequency model:
$$\mathcal{E}= \sum_{t\in W}p_{\mathcal{D}}(t)\log_{|W|}\hat p_{\mathcal{D}}(t; N,\theta,\langle N\rangle)$$

Iteratively, standard BPE maps a highest-co-occurring pair: $\hat{t},\hat{s}$ by merging: $t,s\mapsto ts$. We define harmonically regularized BPE (HR-BPE) to augment standard BPE via a different ranking of token pairs, $t,s$. Instead of a greedy, highest-frequency ranking, HR-BPE's pairs are chosen to minimize the normalized entropy of the current vocabulary's frequency statistics:
$$\hat{t},\hat{s} = \underset{t, s}{\text{argmin}} \left[\mathcal{E}\middle| f^{k+1}_\mathcal{D}(ts) \right]$$

Normalized entropy estimates can be computed for all pairs of tokens by maintaining updated databases of frequency and co-occurrence counts. Optimal-update pairs can be efficiently identified by assuming all change in the loss is due to incrementing/decrementing $ts$ and $t,s$ (respectively) by the co-occurrence frequency shift: $F_{\mathcal{D},1}(t,s)$. We also introduce split operations to allow models to expressively pare down vocabularies. Split minimization is expressed similarly to the merges:
$$\hat{ts} = \underset{ts}{\text{argmin}}\left[\mathcal{E}\middle| f^{k+1}_\mathcal{D}(t), f^{k+1}_D(s) \right]$$

### Word2Vec's Softmax Factorization
The matrix factorization behind Word2Vec's softmax objective was [first derived in 2021](https://arxiv.org/pdf/2205.00148.pdf). This cuts the learning costs associated with the softmax objective's complexity and optimization, and _determines interpretable Bayesian parameters without gradient descent or backpropagation_.

__Theorem__: The Word2Vec $\log$-softmax objective: 
$$L_{soft} = -\sum_{t\in W}\sum_{s\in W}F_{\mathcal{D},r}(t,s)\log\varphi\left(U(t,s)\right)$$

has inner-product parameters, $U(t,s)$, converge for all non-zero co-occurrences towards the values:
$$U(t,s) = \log \frac{F_{\mathcal{D},r}(t,s)}{f_{\mathcal{D},r}(t)},$$

where $f_{\mathcal{D},r}(t) = \sum_{s\in W}F_{\mathcal{D},r}(t,s)$.

### Attention As The Dual Of Representation
To understand attention in transformers we only need assume _any_ sequence of initial prediction vectors for the tokens of a document: $\hat y_1,\cdots,\hat y_{M_d}\in R^{|W|}$; these inform the next-order model's features. We define local attention over a window around any $i^\text{th}$ prediction point. In simplest form, we also aggregate vectors within the _radius_ of $\pm r$ tokens by solving for _optimal_ scalar weights: $a_{i-r},\cdots,a_{i+r}\in R$ that then aggregate around each $i$, over the $\delta$-offsets: $\overline y_i = \sum a_\delta\hat y_\delta$.

For local attention, a weight matrix is learned: $V_r\in R^{(2r+1)\times |W|}$, parameterizing a separate _attention layer_, whose output is normalized in the standard fashion, via the _softmax_ function: $[a_{i-r},\cdots,a_{i+r}] = \varphi(V_r^T\hat y_i)$. To prototype local attention, we again utilize the _softmax_ solution (above). For any position within the radius: $\delta\in\{-r,\cdots,0,\cdots,r\}$, we now track _positional_ co-occurrences: $F_{\mathcal{D},r}(t,\delta)$, or, the number of times $t$ is exactly $\delta$ tokens away from _another_ token. For this _simplest_ form of attention, _the position is the context_, and since _nearly_ all tokens are _nearly all_ surrounded by $2r+1$ other tokens (some are not due to document-edges), it produces a _nearly-uniform_ set of _column-normalized_ probability distributions for the likelihood of context-positions:
$$V_r(t,\delta) = \log \frac{F_{\mathcal{D},r}(t,\delta)}{\phi_{\mathcal{D},r}(\delta)}$$

where $\phi_{\mathcal{D},r}(\delta)$ denotes a _column_-sum, and $V_r\in R^{(2r+1)\times |W|}$ is densified by column-uniform noise. This strictly-positional attention model is accompanied by more-precise (but still ad hoc) positional models. The forward action of this first attention model relies upon two independent context distributions&mdash;one for position and the other for semantics&mdash;so it's named the _independent-positional model_. It is the simplest extension of Word2Vec into a closed-form, local-context transformer. To feed-forward from the previous sequence of vectors, this simple model combines point-wise semantic weighting via a Hadamard product, denoted by $\circ$, using concatenation to stage a positional inner product:
$$\overline y_i = \varphi\left(V^T_r\hat y_i\right)\cdot\left[\left(\varphi(V^T\hat y_i)\circ \hat y_{i-r}\right)^T,\cdots, \left(\varphi(V^T\hat y_i)\circ\hat y_{i+r}\right)^T\right]$$

### Positionally-Dependent Semantic Attention
One should note key differences between what standard transformers do, and what the simplest (above) model _does_ with attention. The standard transformer reserves a separate weight matrix for each position in input. We mirror this by re-defining each context as a tuple: $c=(s,k)$ in the Cartesian product between the vocabulary and all integer positions of the sliding window: $C = W\times\{-r, \cdots,r\}$. This _positionally-dependent_ model of context is _wide_, with wide context vectors: $\vec h_i\in R^{|W|(2r+1)}$ that require re-definition by positional concatenation: 
$\vec h_i = [y_{i-r},\cdots,y_{i+r}]$, and thus incur a larger forward-action parameter matrix: $U_C\in R^{|W|\times|W|(2r+1)}$&mdash;points that when taken together highlight the need for dimensionality reduction.

The positionally-dependent context model can use the joint positional-semantic statistics in $W\times C$'s co-occurrences for an attention model, $V_C\in R^{|W|\times|C|}$, using the softmax solution, for the same attention formula, _now_ replacing the context set with $C$ for positionally-dependent, semantic types. The model's forward action proceeds by first attending (blending) predictions:
$$\overline y_i = \sum_{\delta = -r}^r\left(\varphi\left(V^T_C\hat y_i\right)\circ \left[\hat y_{i-r},\cdots, \hat y_{i+r}\right]\right)_{(r+\delta)|W|}^{(r+\delta+1)|W|}$$

which allows one to form $1^\text{st}$-order, _dense_ context vectors: $\vec h_i = \text{Normalize}([\overline y_{i-r},\cdots,\overline y_{i+r}] + \vec h_i)$. The first-order model's output head is then determined by the expected (first-order, dense) co-occurrence distribution: via the _forward-action_ softmax formula. Finally, just as with the positionally-independent model, the context-stabilizing 'residual connection' is followed by one for the output, averaging and normalizing values with the original prediction, thus forming a second-order prediction: $\hat y^ = \text{Normalize}(\varphi(U_C\vec h_i) + \hat y$.

### Harmonic Positional Encoding
A major criticism of unigram frequency models focuses on their failure to express, e.g., positional information, or, dependencies between tokens. We present _harmonic positional encoding_ as one that operates empirical unigram frequency to encode relative positions in all orders in the prototype, increasing performance in-line with positional encoding for standard transformers. We ask: how can a frequency model define a meaningful wave frequency? First, denote the probabilistic, rate of occurrence for each token in $\mathcal{D}$ as $p_\mathcal{D}(t)$; this records the expected rate of $t$'s occurrence across a sample of text. _If these appearances occur evenly spaced and as expected, then the phases of their corresponding waves could reinforce_. Defining a wave function, $\psi$, on all co-occurrences within any document, $d\in\mathcal{D}$, by the distance between in-radius tokens is critical: $\psi^2\left(t_i,s_j\right) = \cos^2\left(\pi|i-j|p_\mathcal{D}(s)\right)$. We construct this probability density function as a quantum model of tokens-as-particles, i.e., by taking the square of the absolute value of a wave function. The chosen wave function&mdash;the cosine half angle formula&mdash;weights context tokens by the unigram-likelihood that $s$ will recur instead of $t$, making large values of $\psi$ indicative of positional dependence between the center token on its context. To implement harmonic positional encoding we form weighted context vectors: $\vec h_i^\psi = [\psi^2(t_i,s_{i-r})y_{i-r},\cdots,\psi^2(t_i,s_{i+r})y_{i+r}]$, and encoded co-occurrences: $F^\psi_{\mathcal{D},r} = \sum y_i\otimes\vec h^\psi_i$ and derived parameters: $U_C^\psi$ and $V_C^\psi$.

### Multi-Task Encoding
To predict human annotated labels: $\ell^g_i$ (and contexts) for a document, $d$'s $i = 1\cdots, M^g_d$ 'gold standard' tokens, $t^g_i$ , we assume labels belong to a tag-set: $\ell_j^g\in L$, like part-of-speech (POS). HR-BPE is supervised by treating the gold standard as a _covering tokenization_, where each of HR-BPE's tokens: $t_1, \cdots ,t_{M_d}$ is contained within exactly one of the gold standard's. For each, we thus define the _implied labels_ by identifying each $j_i$ with $t_i\in t^g_{j_i}$, so that: $\ell_i = \ell^g_{j_i}$. The HR-BPE tokenization then has its implied labels crossed with contexts for co-occurrences: $F_{\mathcal{D},r,L} = \sum\ell_i\otimes\vec h_i$, and derived model parameters: $U_{C_L}$ and $V_{C_L}$.

### Dimensionality Reduction
Standard-basis encoding is unavoidable for any LM, because LMs predict over the model's whole vocabulary: $W$. This makes dimensionality reduction _necessary_, as the combinatorial overhead on model parameters becomes tremendous: $\mathcal{O}(r|W|^2$), when $(2r+1)|W|$-dimensional contexts are used. Unfortunately, the random nature of NLP and deep learning's standards obfuscate the _meaning_ of context-dimensions. Preliminarily, we considered an _ansatz_, or, guess, that a salient form of _compressed identity_ can be provided by empirical, _integer frequencies_. Musically, this identifies each token by its 'note' of occurrence across the collection, $\mathcal{D}$. We describe the function: $\chi_f(t) = f_\mathcal{D}(t)$ as a _cipher_ of $W$, mapping the categorical, one-hot vector information, $y_i$, to _lower-dimensional_, 'frequency-hot' vectors: $y_i \overset{\chi}{\mapsto} f_i = h_i$, used for context&mdash;an algorithm that we refer to as _integer-frequency encipherment_ (IFE). IFE helps IaMaN abstract lower perplexities. Instructively, we note that this _high-frequency distinguishability retains model performance_.

Supposing each token $t$ in $W$ has identity modified from the usual one-hot/standard-basis vector as follows: 1) select a 'low' dimension: $b\leq|W|$, and 2) assign a unique bit-vector, $\eta_{t}\in\{0,1\}^b$ to each. According to the _distinguishability hypothesis_ on dimensionality reduction from the IFE prototyping experiments, a 'good' order for the bits distinguishes the highest-frequency tokens best, and has latitude to give similar-frequency tokens similar IDs. Working along these lines, we define _b-bit encipherment_ as the process of assigning _probabilistically normalized_ $b$-bit vectors in a 'smooth' order, by inducting the order that $i=1$: assigns the set of $b$ standard basis vectors: $\mathcal{V}^b_1$ to the $b$ most-frequent tokens (generalizing one-hots/standard bases); $i=2$: adds standard-basis vectors to those from $\mathcal{V}^b_{k-1}$ _in reverse order of assignment_, while filtering for unique bit-vectors in $\{0,1\}^b$; $i=3$: repeats step $i=2$. $b$-bit enciphered context vectors are weighted probabilistically:  

$$\vec h_i^{\psi,b} = \left[\psi^2(t_i,s_{i-r})\frac{\eta_{t_{i-r}}}{\|\eta_{t_{i-r}\|_1}},\cdots,\psi^2(t_i,s_{i+r})\frac{\eta_{t_{i+r}}}{\|\eta_{t_{i+r}}\|_1}\right] \in R^{b(2r+1)}$$

### Multiple Layers
In addition to enhancing the transformer's embedding layer into a _sparse transformer_, we likewise consider how to capacitate arbitrary numbers of dense layers, just as in standard DL. To form a $k^\text{th}$ layer for any given label, $L$ (including $W$), one can compute co-occurrences between the target label's one-hot vectors, $\ell_i$, and their $k^\text{th}$-order contexts: $\vec h_i^{(k)}$. Multiple layers like this are possible, but don't yet form a compositional 'deep' optimization, i.e., are ad hoc (like this system's attention initial layers). Expect differentially-optimzed multi-layer networks (depth) and feature importanace layers (convolution/attention parameters) as core aspects of future releases.