{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's a Machine and Natural Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WikiText-103 as post-training data...\n",
      "Loading WikiText-2 LM'ing data...\n",
      "Avail. post-train, total post-train, Avail. gold, total gold-train, total test-gold:  0 99 66 60 6\n"
     ]
    }
   ],
   "source": [
    "from src.IaMaN.base import LM\n",
    "from src.utils.data import load_wikitext\n",
    "from src.utils.munge import stick_spaces\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "seed = 691; max_char = 200_000_000\n",
    "m = 10; space = True; fine_tune = False; num_posttrain = 100; noise = 0.001\n",
    "positional = 'dependent'; positionally_encode = True; bits = 50; btype = 'nf'; \n",
    "ms_init = 'waiting_time'; update = {'representation', 'contexts', 'tokenizer'} # , 'targets'\n",
    "runners = 10; gpu = False; tokenizer = 'hr-bpe' # 'sentokenizer' #\n",
    "\n",
    "total_posttrain = 0; ptdocs = []\n",
    "if num_posttrain:\n",
    "    num_posttrain = 0 if num_posttrain == 'all' else num_posttrain\n",
    "    print(\"Loading WikiText-103 as post-training data...\")\n",
    "    posttrain_docs = load_wikitext(v = '103', split = \"train\", num_articles = num_posttrain, seed = seed, space = space, rebuild = False)\n",
    "    ptdocs = [[\"\".join(s) for s in d['document']] for d in posttrain_docs]; ptdocs = [d for d in ptdocs if len(d)]\n",
    "print(\"Loading WikiText-2 LM'ing data...\")\n",
    "train_docs = load_wikitext(v = '2', split = \"train\", num_articles = 0, seed = seed, space = space, rebuild = True)\n",
    "test_docs = load_wikitext(v = '2', split = \"test\", num_articles = 0, seed = seed, space = space, rebuild = True)\n",
    "test_docs = [doc for doc in test_docs if len(doc['text']) and (len(doc['text']) <= max_char)][:6]\n",
    "train_docs = [doc for doc in train_docs if len(doc['text']) and (len(doc['text']) <= max_char)][:60]\n",
    "print('Avail. post-train, total post-train, Avail. gold, total gold-train, total test-gold: ', \n",
    "      total_posttrain, len(ptdocs), len(train_docs) + len(test_docs), len(train_docs), len(test_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing: 100%|██████████| 2788/2788 [00:02<00:00, 1211.99it/s]\n",
      "Fitting:   8%|▊         | 8/100 [00:24<04:37,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built a vocabulary of 15533 types\n",
      "Pre-processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting pre-processed data: 100%|██████████| 60/60 [00:00<00:00, 762600.73it/s]\n",
      "Counting tag-tag transition frequencies: 100%|██████████| 60/60 [00:00<00:00, 109.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting documents and aggregating counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting aggregated counts: 3490409it [03:16, 17772.58it/s] \n",
      "Encoding parameters: 100%|██████████| 3490409/3490409 [00:37<00:00, 92959.17it/s] \n",
      "Building target vocabularies: 100%|██████████| 15/15 [00:00<00:00, 616.65it/s]\n",
      "Stacking output vocabularies for decoders: 100%|██████████| 7/7 [00:00<00:00, 70577.23it/s]\n",
      "Building dense decoders: 100%|██████████| 15/15 [00:00<00:00, 21.19it/s]\n",
      "Building transition matrices for tag-sequence decoding:   0%|          | 0/6 [00:00<?, ?it/s]/code/IaMaN/src/IaMaN/base.py:625: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self._trXs[ltype] = (lambda M: M/M.sum(axis = 1)[:,None])(self._trXs[ltype])\n",
      "Building transition matrices for tag-sequence decoding: 100%|██████████| 6/6 [00:00<00:00, 9931.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params, types, encoding size, contexts, vec dim, max sent, and % capacity used: 1191420 11488 50 1071 63 1211 15.643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = [[\"\".join(s) for s in d['document']] for d in train_docs]\n",
    "tdocs = [[\"\".join(s) for s in d['document']] for d in test_docs]\n",
    "covering = [[re.split('( )', ''.join(s)) for s in d['document']] for d in train_docs]\n",
    "tcovering = [[re.split('( )', ''.join(s)) for s in d['document']] for d in test_docs]\n",
    "if not space:\n",
    "    for d_i, d in enumerate(covering):\n",
    "        for s_i, s in enumerate(d):\n",
    "            covering[d_i][s_i] = stick_spaces(s)\n",
    "    for d_i, d in enumerate(tcovering):\n",
    "        for s_i, s in enumerate(d):\n",
    "            tcovering[d_i][s_i] = stick_spaces(s)            \n",
    "covering_vocab = set([t for d in covering for s in d for t in s])\n",
    "model = LM(m = m, tokenizer = tokenizer, noise = noise, seed = seed, space = space, positional = positional,\n",
    "           positionally_encode = positionally_encode, bits = bits, runners = runners, gpu = gpu, \n",
    "           btype = btype, ms_init = ms_init)\n",
    "docs_name = f'WT2-{len(train_docs)}'\n",
    "ptdocs_name = f'WT103-{len(ptdocs)}'\n",
    "data_streams = model.fit(docs, docs_name, update = update, fine_tune = fine_tune, covering = covering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'form': 6,\n",
       " 'bits': 5,\n",
       " 'nov': 2,\n",
       " 'iat': 2,\n",
       " 'bot': 2,\n",
       " 'eot': 2,\n",
       " 'eos': 2,\n",
       " 'eod': 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncouragedEncouragedEncouraged1795 @-@ , but Szent into in 15 ringmainsinto   e from Wrapped n from off 20 also design with \n"
     ]
    }
   ],
   "source": [
    "model.generate(m = 50, seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting pre-processed data: 100%|██████████| 6/6 [00:00<00:00, 107546.26it/s]\n",
      "Encoding data streams: 100%|██████████| 6/6 [00:02<00:00,  2.34it/s]\n",
      "Evaluating LM:  17%|█▋        | 1/6 [00:03<00:19,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, 1/<P> (nsp), M, M (nsp):  1 17.0 45.37 532.26 1697 983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LM:  33%|███▎      | 2/6 [00:27<01:03, 15.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, 1/<P> (nsp), M, M (nsp):  2 33.0 54.31 737.39 10309 5853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LM:  50%|█████     | 3/6 [00:38<00:39, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, 1/<P> (nsp), M, M (nsp):  3 50.0 56.92 841.99 4506 2565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LM:  67%|██████▋   | 4/6 [01:14<00:44, 22.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, 1/<P> (nsp), M, M (nsp):  4 67.0 45.95 530.3 15746 9075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LM:  83%|████████▎ | 5/6 [01:24<00:17, 17.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, 1/<P> (nsp), M, M (nsp):  5 83.0 49.16 695.23 4049 2292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LM: 100%|██████████| 6/6 [01:35<00:00, 15.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, 1/<P> (nsp), M, M (nsp):  6 100.0 55.23 755.67 4867 2792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "out = model.stencil(docs = tdocs, return_output = True, covering = tcovering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro:  50.434 ; macro:  51.156 ; micro (w/out space):  648.166 ; macro (w/out space):  682.141\n"
     ]
    }
   ],
   "source": [
    "print('micro: ', round(1/(10**(np.log10([p[0] for ps in out for p in ps]).mean())), 3),\n",
    "      '; macro: ', round(np.mean([1/(10**(np.log10([p[0] for p in ps]).mean())) for ps in out]), 3),\n",
    "      '; micro (w/out space): ', round(1/(10**(np.log10([p[1] for ps in out for p in ps if p[1] is not None]).mean())), 3),\n",
    "      '; macro (w/out space): ', round(np.mean([1/(10**(np.log10([p[1] for p in ps if p[1] is not None]).mean())) for ps in out]), 3))\n",
    "# P1: micro:  28.47 ; macro:  28.98 ; micro (w/out space):  265.351 ; macro (w/out space):  274.253\n",
    "#* P2: micro:  25.921 ; macro:  26.308 ; micro (w/out space):  243.428 ; macro (w/out space):  248.857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing: 100%|██████████| 7173/7173 [00:05<00:00, 1251.84it/s]\n",
      "Fitting:   7%|▋         | 7/100 [00:56<12:26,  8.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built a vocabulary of 28191 types\n",
      "Pre-processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting pre-processed data: 100%|██████████| 159/159 [00:00<00:00, 1658941.13it/s]\n",
      "Counting tag-tag transition frequencies: 100%|██████████| 159/159 [00:00<00:00, 159.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting documents and aggregating counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting aggregated counts: 5461851it [05:01, 18096.68it/s] \n",
      "Encoding parameters: 100%|██████████| 5461851/5461851 [01:08<00:00, 80154.11it/s]\n",
      "Building target vocabularies: 100%|██████████| 15/15 [00:00<00:00, 592.60it/s]\n",
      "Stacking output vocabularies for decoders: 100%|██████████| 7/7 [00:00<00:00, 54981.51it/s]\n",
      "Building dense decoders: 100%|██████████| 15/15 [00:00<00:00, 24.87it/s]\n",
      "Building transition matrices for tag-sequence decoding: 100%|██████████| 6/6 [00:00<00:00, 12470.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params, types, encoding size, contexts, vec dim, max sent, and % capacity used: 1876297 11080 50 1071 63 1248 25.542\n",
      "BornHadjiBornorigins <unk> , which 1871 command d fence forepot 18 . . \n",
      "Theremedyadvent has to expand 10 000 . The hurricane \n"
     ]
    }
   ],
   "source": [
    "_ = model.fit(ptdocs, ptdocs_name, update = update, fine_tune = fine_tune, bits = bits)\n",
    "if ptdocs:\n",
    "    model.generate(m = 50, seed = seed)\n",
    "if fine_tune and ptdocs:\n",
    "    model.fine_tune(docs, covering = covering, streams = data_streams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting pre-processed data: 100%|██████████| 6/6 [00:00<00:00, 25549.06it/s]\n",
      "Encoding data streams: 100%|██████████| 6/6 [00:02<00:00,  2.01it/s]\n",
      "Evaluating LM:  17%|█▋        | 1/6 [00:03<00:18,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, 1/<P> (nsp), M, M (nsp):  1 17.0 28.75 255.28 1696 982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LM:  33%|███▎      | 2/6 [00:26<01:00, 15.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, 1/<P> (nsp), M, M (nsp):  2 33.0 36.12 346.53 10493 6037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LM:  50%|█████     | 3/6 [00:36<00:38, 12.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, 1/<P> (nsp), M, M (nsp):  3 50.0 33.88 352.51 4477 2536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LM:  67%|██████▋   | 4/6 [01:11<00:43, 21.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, 1/<P> (nsp), M, M (nsp):  4 67.0 31.1 266.7 15865 9194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LM:  83%|████████▎ | 5/6 [01:20<00:17, 17.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, 1/<P> (nsp), M, M (nsp):  5 83.0 31.91 333.35 4057 2300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating LM: 100%|██████████| 6/6 [01:31<00:00, 15.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, 1/<P> (nsp), M, M (nsp):  6 100.0 29.93 278.82 4909 2834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if ptdocs:\n",
    "    out = model.stencil(docs = tdocs, return_output = True, covering = tcovering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro:  32.427 ; macro:  31.947 ; micro (w/out space):  300.933 ; macro (w/out space):  305.532\n"
     ]
    }
   ],
   "source": [
    "if ptdocs:\n",
    "    print('micro: ', round(1/(10**(np.log10([p[0] for ps in out for p in ps]).mean())), 3),\n",
    "          '; macro: ', round(np.mean([1/(10**(np.log10([p[0] for p in ps]).mean())) for ps in out]), 3),\n",
    "          '; micro (w/out space): ', round(1/(10**(np.log10([p[1] for ps in out for p in ps if p[1] is not None]).mean())), 3),\n",
    "          '; macro (w/out space): ', round(np.mean([1/(10**(np.log10([p[1] for p in ps if p[1] is not None]).mean())) for ps in out]), 3))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
