{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's a Machine and Natural Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 2758.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gold-tagged UDs data...\n",
      "Avail. pre-train, total pre-train, Avail. gold, total gold-train, total test-gold:  14198 5000 150 132 18\n"
     ]
    }
   ],
   "source": [
    "from src.IaMaN.base import LM\n",
    "from src.utils.data import load_ud\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "seed = 691\n",
    "\n",
    "print(\"Loading pre-training data...\")\n",
    "pretrain_path = '/data/newstweet/week_2019-40_article_texts/'\n",
    "total_pretrain = len([pretrain_file for pretrain_file in os.listdir(pretrain_path) if re.search(\"^\\d+.txt$\", pretrain_file)])\n",
    "num_pretrain = 5000 # total_pretrain\n",
    "\n",
    "all_pretrain_files = [pretrain_file for pretrain_file in os.listdir(pretrain_path) if re.search(\"^\\d+.txt$\", pretrain_file)]\n",
    "if num_pretrain:\n",
    "    np.random.seed(seed)\n",
    "    pretrain_files = np.random.choice(all_pretrain_files, size=num_pretrain, replace=False)\n",
    "else:\n",
    "    pretrain_files = np.array([])\n",
    "\n",
    "ptdocs = [[[open(pretrain_path+pretrain_file).read()]] for pretrain_file in tqdm(pretrain_files)]\n",
    "\n",
    "print(\"Loading gold-tagged UDs data...\")\n",
    "max_char = 200_000_000\n",
    "load_set = 'GUM'; fine_tune = False; do_ife = True; update_ife = False; runners = 10\n",
    "all_docs = load_ud(\"English\", num_articles = 0, seed = 691, load_set = load_set, rebuild = True)\n",
    "test_docs = [doc for doc in all_docs if 'test' in doc['id'] and len(doc['text']) <= max_char]# [:1]\n",
    "train_docs = [doc for doc in all_docs if 'test' not in doc['id'] and len(doc['text']) <= max_char]# [:4]\n",
    "nsamp = len(test_docs)\n",
    "print('Avail. pre-train, total pre-train, Avail. gold, total gold-train, total test-gold: ', \n",
    "      total_pretrain, len(ptdocs), len(all_docs), len(train_docs), len(test_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing: 100%|██████████| 6503/6503 [00:02<00:00, 2853.10it/s]\n",
      "Fitting:  18%|█▊        | 18/100 [00:47<03:38,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built a vocabulary of 10703 types\n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [00:20<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting documents and aggregating counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5787912it [07:03, 13652.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [00:10<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [00:00<00:00, 398.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5787912/5787912 [00:39<00:00, 145149.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing marginal statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3096777/3096777 [00:16<00:00, 190081.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dense output heads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:21<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Model params, types, encoding size, contexts, vec dim, max sent, and % capacity used: 5787912 10704 485 10185 12054 177 5.309\n",
      "Processing pre-training documents...\n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [28:04<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting documents and aggregating counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8958836it [29:52, 4998.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:29<00:00, 169.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 2548.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13442208/13442208 [01:14<00:00, 181547.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-computing marginal statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8564400/8564400 [00:26<00:00, 326886.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-building dense output heads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:27<00:00,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Model params, types, encoding size, contexts, vec dim, max sent, and % capacity used: 13442208 10704 485 10185 12054 177 12.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = [[\"\".join([row[1] for row in s]) for s in d['conllu']] for d in train_docs]\n",
    "tdocs = [[\"\".join([row[1] for row in s]) for s in d['conllu']] for d in test_docs]\n",
    "covering = [[[row[1] for row in s] for s in d['conllu']] for d in train_docs]\n",
    "covering_vocab = set([t for d in covering for s in d for t in s])\n",
    "\n",
    "all_layers = {d_i: {'lem': [[row[2] for row in s] for s in d['conllu']], \n",
    "                    'pos': [[row[3] for row in s] for s in d['conllu']], \n",
    "                    'sup': [[(str(int(row[6]) - int(row[0])) if int(row[6]) else row[6]) for row in s] for s in d['conllu']], \n",
    "                    'dep': [[row[7] for row in s] for s in d['conllu']],\n",
    "                    'sty': [[d['s_type'][s_i] for row in s] for s_i, s in enumerate(d['conllu'])]}\n",
    "              for d_i, d in enumerate(train_docs)}\n",
    "\n",
    "model = LM(covering_vocab = covering_vocab)\n",
    "model.init(m = 10, noise = 0.001, positional = True, seed = seed, do_ife = do_ife, runners = runners)\n",
    "model.fit(docs, f'{load_set}-{nsamp}', covering = covering, all_layers = all_layers)\n",
    "model.pre_train(ptdocs, update_ife = update_ife)\n",
    "if fine_tune:\n",
    "    model.fine_tune(docs, covering = covering, all_layers = all_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Currently__: ordering for the current fine tuning process:\n",
    "1. train tokenizer and fit model to GUM\n",
    "2. process NewsTweet documents to integrate sparse post-training statistics (requires mr implementation and updates to the vocabularies/indices)\n",
    "3. update the ife and dense model, i.e., produce new statistics and dimensionalities\n",
    "4. fine tune output heads to GUM, and _combine_ them with the dense model from (3), i.e., don't just replace as is current.\n",
    "\n",
    "__Preliminarily__: this does seem to present performance benefits, but as is usual will require 'big data' statistics to become competitive. In particular, the (tokenization, least of all), counting, sorting, and aggregation of co-occurrence counts must all be distributed for the statistical resolution required to approach performance gains aking to more-advanced systems. Currently, a spark-based MR system is implemented for these (all but tokenization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " who have describeded by al health carefirem.”SinceStill\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(m = 25, seed = seed, return_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((' ', 'form'), 0.32389457628790824),\n",
       " (('e', 'form'), 0.07128479788751822),\n",
       " (('t', 'form'), 0.04682393813109191),\n",
       " (('s', 'form'), 0.04024055275105154),\n",
       " (('o', 'form'), 0.029840061056482355)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][1].most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this season growth of the post.\"This\"We'rethinknothappensurprisingrevolutionizingide5\n"
     ]
    }
   ],
   "source": [
    "model.generate(m = 25, seed = seed, top = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the first half of the season.S. I think that’s why\n"
     ]
    }
   ],
   "source": [
    "model.generate(m = 25, seed = seed, top = 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the season in the UnitedeStates.\"Our W said that would \n"
     ]
    }
   ],
   "source": [
    "model.generate(m = 25, seed = seed, top = [0.05, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In the 2016, accordingsto the unique for document season?outHefive \n"
     ]
    }
   ],
   "source": [
    "model.generate(prompt = \" In the\", m = 25, seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hey, what are you know?\n"
     ]
    }
   ],
   "source": [
    "model.generate(prompt = \" Hey, what are you thinking?\", seed = seed, revise = [19,24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In the most of pregnancy season and impeachment communityearein NewandYorkattemptcar,\n"
     ]
    }
   ],
   "source": [
    "model.generate(prompt = \" In the\", m = 25, seed = seed, rhyme = 0.000000001)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.generate(prompt = \" In the\", m = 25, seed = seed, slang = 0.0001)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.generate(prompt = \" In the\", m = 25, seed = seed, focus = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In the NFL, whichingwill be tor the nexterefluidene \n"
     ]
    }
   ],
   "source": [
    "model.generate(prompt = \" In the\", m = 25, seed = seed, prose = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In the U.S.Ftroops TEXSYRYININYINYTY#ASY\n"
     ]
    }
   ],
   "source": [
    "model.generate(prompt = \" In the\", m = 25, seed = seed, style = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In the 2016, accordingsto the unique for document season?outHefive \n"
     ]
    }
   ],
   "source": [
    "model.generate(prompt = \" In the\", m = 25, seed = seed, chunk = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In the 2016, accordingsto the campaign her two touchs, according \n"
     ]
    }
   ],
   "source": [
    "model.generate(prompt = \" In the\", m = 25, seed = seed, punct = 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In the NFL, whichashed blood,\"ashe add dirtail.\"For example temperature not athletic headst d by the UnitedoStates.\n"
     ]
    }
   ],
   "source": [
    "model.generate(prompt = \" In the\", m = 50, seed = seed, \n",
    "               prose = 0.75, style = 0.25, chunk = 0.9, punct = 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening next doc:\n",
      "opening next sent:\n",
      "opening next token:\n",
      "(' ', ' ', 'PUNCT', False, '1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('In', 'in', 'ADP', False, '4', 'case', 'decl')\n",
      "opening next token:\n",
      "(' ', ' ', 'PUNCT', False, '1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('the', 'the', 'DET', False, '2', 'det', 'decl')\n",
      "opening next token:\n",
      "(' ', ' ', 'PUNCT', False, '1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('NFL', 'world', 'PROPN', False, '5', 'obl', 'decl')\n",
      "opening next token:\n",
      "(',', ',', 'PUNCT', False, '-1', 'punct', 'decl')\n",
      "opening next token:\n",
      "(' ', ' ', 'PUNCT', False, '1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('whi', 'and', 'PROPN', False, '5', 'appos', 'decl')\n",
      "opening next token:\n",
      "('cha', ' ', 'PUNCT', False, '1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('sh', 'she', 'PRON', False, '4', 'nsubj', 'decl')\n",
      "opening next token:\n",
      "('e', 'she', 'VERB', False, '3', 'nsubj', 'decl')\n",
      "opening next token:\n",
      "('d', 'die', 'VERB', False, '2', 'compound', 'decl')\n",
      "opening next token:\n",
      "(' ', ' ', 'PUNCT', False, '1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('blood', 'say', 'VERB', False, '9', 'conj', 'decl')\n",
      "opening next token:\n",
      "(',', ',', 'PUNCT', False, '-1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('\"', \"''\", 'PUNCT', False, '1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('a', 'a', 'DET', False, '4', 'det', 'decl')\n",
      "opening next token:\n",
      "('sh', 'she', 'PUNCT', False, '3', 'punct', 'decl')\n",
      "opening next token:\n",
      "('e', 'she', 'PRON', False, '2', 'nsubj', 'decl')\n",
      "opening next token:\n",
      "(' ', ' ', 'PUNCT', False, '1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('add', 'say', 'VERB', False, '0', 'root', 'decl')\n",
      "opening next token:\n",
      "(' ', ' ', 'PUNCT', False, '1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('d', 'desire', 'VERB', False, '2', 'amod', 'decl')\n",
      "opening next token:\n",
      "('ir', ' ', 'PUNCT', False, '1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('tail', \"''\", 'NOUN', False, '-4', 'obj', 'decl')\n",
      "opening next token:\n",
      "('.', '.', 'PUNCT', False, '-1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('\"', \"''\", 'PUNCT', False, '1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('For', 'if', 'SCONJ', False, '2', 'case', 'decl')\n",
      "opening next token:\n",
      "(' ', ' ', 'PUNCT', False, '1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('example', 'example', 'CCONJ', False, '2', 'nsubj', 'decl')\n",
      "opening next token:\n",
      "(' ', ' ', 'PUNCT', False, '1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('temperature', 'of', 'VERB', False, '21', 'aux', 'decl')\n",
      "opening next token:\n",
      "(' ', ' ', 'PUNCT', False, '1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('not', 'and', 'ADP', False, '4', 'case', 'decl')\n",
      "opening next token:\n",
      "(' ', ' ', 'PUNCT', False, '1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('athletic', 'have', 'ADJ', False, '2', 'advmod', 'decl')\n",
      "opening next token:\n",
      "(' ', ' ', 'PUNCT', False, '1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('h', 'he', 'VERB', False, '2', 'obj', 'decl')\n",
      "opening next token:\n",
      "('e', 'he', 'VERB', False, '-6', 'nsubj', 'decl')\n",
      "opening next token:\n",
      "('ad', 'head', 'NOUN', False, '2', 'obj', 'decl')\n",
      "opening next token:\n",
      "('s', ' ', 'VERB', False, '-4', 'obj', 'decl')\n",
      "opening next token:\n",
      "('t', 'start', 'VERB', False, '2', 'obj', 'decl')\n",
      "opening next token:\n",
      "(' ', ' ', 'PUNCT', False, '1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('d', 'start', 'VERB', False, '-12', 'advcl', 'decl')\n",
      "opening next token:\n",
      "(' ', ' ', 'PUNCT', False, '1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('by', 'in', 'ADP', False, '6', 'case', 'decl')\n",
      "opening next token:\n",
      "(' ', ' ', 'PUNCT', False, '1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('the', 'the', 'DET', False, '4', 'det', 'decl')\n",
      "opening next token:\n",
      "(' ', ' ', 'PUNCT', False, '1', 'punct', 'decl')\n",
      "opening next token:\n",
      "('United', 'Unite', 'NOUN', False, '2', 'amod', 'decl')\n",
      "opening next token:\n",
      "('o', ' ', 'PUNCT', False, '-6', 'punct', 'decl')\n",
      "opening next token:\n",
      "('States', 'State', 'PROPN', False, '-8', 'obl', 'decl')\n",
      "opening next token:\n",
      "('.', '.', 'PUNCT', True, '-37', 'punct', 'decl')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.interpret([[\"\".join(model._s)]], seed = seed)\n",
    "for doc in model._documents:\n",
    "    print('opening next doc:')\n",
    "    for s in doc._sentences:\n",
    "        print('opening next sent:')\n",
    "        for t in s._tokens:\n",
    "            print('opening next token:')\n",
    "            print((str(t), t._lem, t._pos, t._sep, t._sup, t._dep, s._sty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing documents..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:02<00:00,  8.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating language model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2664/2664 [07:23<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, M:  1 6.0 101.64 2664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2540/2540 [06:57<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, M:  2 11.0 1224.9 2540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1869/1869 [05:16<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, M:  3 17.0 869.15 1869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2596/2596 [07:07<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, M:  4 22.0 198.22 2596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2014/2014 [05:30<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, M:  5 28.000000000000004 59.24 2014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2610/2610 [07:21<00:00,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, M:  6 33.0 218.71 2610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2894/2894 [07:56<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, M:  7 39.0 82.13 2894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2099/2099 [05:48<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, M:  8 44.0 99.2 2099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2299/2299 [06:30<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, M:  9 50.0 280.71 2299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3463/3463 [09:31<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, M:  10 56.00000000000001 164.21 3463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1656/1656 [04:49<00:00,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, M:  11 61.0 194.34 1656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2576/2576 [08:26<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, M:  12 67.0 89.3 2576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2726/2726 [07:50<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, M:  13 72.0 147.42 2726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2154/2154 [06:19<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, M:  14 78.0 38.17 2154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2945/2945 [10:30<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, M:  15 83.0 117.08 2945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1804/1804 [12:28<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, M:  16 89.0 142.28 1804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1683/1683 [06:10<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, M:  17 94.0 100.67 1683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2749/2749 [08:16<00:00,  5.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document no., pct. complete, 1/<P>, M:  18 100.0 78.67 2749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.generate(docs = tdocs, m = 50, seed = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below models are summarized as:\n",
    "```\n",
    "- Model params, types, encoding size, contexts, vec dim, max sent, and % capacity used: 5787912 10704 485 10185 12054 177 5.309\n",
    "- Model params, types, encoding size, contexts, vec dim, max sent, and % capacity used: 13442208 10704 485 10185 12054 177 12.33\n",
    "- Model params, types, encoding size, contexts, vec dim, max sent, and % capacity used: 15964670 10704 485 10185 12054 177 14.644\n",
    "- Model params, types, encoding size, contexts, vec dim, max sent, and % capacity used: 20152225 10704 485 10185 12054 177 18.485\n",
    "```\n",
    "\n",
    "- With IFE, trained on 132 GUM train documents, then stenciled on all 18 GUM test documents:\n",
    "\n",
    "```\n",
    "Tokenizing documents..\n",
    "100%|██████████| 18/18 [00:02<00:00,  8.81it/s]\n",
    "Evaluating language model..\n",
    "100%|██████████| 2664/2664 [04:52<00:00,  9.12it/s]\n",
    "document no., pct. complete, 1/<P>, M:  1 6.0 435.18 2664\n",
    "100%|██████████| 2540/2540 [04:27<00:00,  9.50it/s]\n",
    "document no., pct. complete, 1/<P>, M:  2 11.0 2654.38 2540\n",
    "100%|██████████| 1869/1869 [03:15<00:00,  9.55it/s]\n",
    "document no., pct. complete, 1/<P>, M:  3 17.0 1822.74 1869\n",
    "100%|██████████| 2596/2596 [04:33<00:00,  9.48it/s]\n",
    "document no., pct. complete, 1/<P>, M:  4 22.0 415.06 2596\n",
    "100%|██████████| 2014/2014 [03:43<00:00,  9.01it/s]\n",
    "document no., pct. complete, 1/<P>, M:  5 28.000000000000004 81.67 2014\n",
    "100%|██████████| 2610/2610 [04:36<00:00,  9.43it/s]\n",
    "document no., pct. complete, 1/<P>, M:  6 33.0 579.85 2610\n",
    "100%|██████████| 2894/2894 [05:05<00:00,  9.46it/s]\n",
    "document no., pct. complete, 1/<P>, M:  7 39.0 180.72 2894\n",
    "100%|██████████| 2099/2099 [03:41<00:00,  9.50it/s]\n",
    "document no., pct. complete, 1/<P>, M:  8 44.0 369.72 2099\n",
    "100%|██████████| 2299/2299 [04:03<00:00,  9.43it/s]\n",
    "document no., pct. complete, 1/<P>, M:  9 50.0 825.7 2299\n",
    "100%|██████████| 3463/3463 [06:19<00:00,  9.14it/s]\n",
    "document no., pct. complete, 1/<P>, M:  10 56.00000000000001 745.92 3463\n",
    "100%|██████████| 1656/1656 [02:55<00:00,  9.42it/s]\n",
    "document no., pct. complete, 1/<P>, M:  11 61.0 649.48 1656\n",
    "100%|██████████| 2576/2576 [04:33<00:00,  9.42it/s]\n",
    "document no., pct. complete, 1/<P>, M:  12 67.0 263.56 2576\n",
    "100%|██████████| 2726/2726 [04:49<00:00,  9.42it/s]\n",
    "document no., pct. complete, 1/<P>, M:  13 72.0 285.6 2726\n",
    "100%|██████████| 2154/2154 [04:01<00:00,  8.91it/s]\n",
    "document no., pct. complete, 1/<P>, M:  14 78.0 75.1 2154\n",
    "100%|██████████| 2945/2945 [05:13<00:00,  9.40it/s]\n",
    "document no., pct. complete, 1/<P>, M:  15 83.0 328.98 2945\n",
    "100%|██████████| 1804/1804 [03:12<00:00,  9.37it/s]\n",
    "document no., pct. complete, 1/<P>, M:  16 89.0 324.12 1804\n",
    "100%|██████████| 1683/1683 [02:59<00:00,  9.40it/s]\n",
    "document no., pct. complete, 1/<P>, M:  17 94.0 251.48 1683\n",
    "100%|██████████| 2749/2749 [05:04<00:00,  9.02it/s]\n",
    "document no., pct. complete, 1/<P>, M:  18 100.0 150.55 2749\n",
    "```\n",
    "\n",
    "- With IFE, trained on 132 GUM train documents, then 5,000 NewsTweet Pre/Post-training articles, then stenciled on all 18 GUM test documents:\n",
    "\n",
    "```\n",
    "Tokenizing documents..\n",
    "100%|██████████| 18/18 [00:02<00:00,  8.80it/s]\n",
    "Evaluating language model..\n",
    "100%|██████████| 2664/2664 [07:23<00:00,  6.01it/s]\n",
    "document no., pct. complete, 1/<P>, M:  1 6.0 101.64 2664\n",
    "100%|██████████| 2540/2540 [06:57<00:00,  6.08it/s]\n",
    "document no., pct. complete, 1/<P>, M:  2 11.0 1224.9 2540\n",
    "100%|██████████| 1869/1869 [05:16<00:00,  5.91it/s]\n",
    "document no., pct. complete, 1/<P>, M:  3 17.0 869.15 1869\n",
    "100%|██████████| 2596/2596 [07:07<00:00,  6.07it/s]\n",
    "document no., pct. complete, 1/<P>, M:  4 22.0 198.22 2596\n",
    "100%|██████████| 2014/2014 [05:30<00:00,  6.09it/s]\n",
    "document no., pct. complete, 1/<P>, M:  5 28.000000000000004 59.24 2014\n",
    "100%|██████████| 2610/2610 [07:21<00:00,  5.92it/s]\n",
    "document no., pct. complete, 1/<P>, M:  6 33.0 218.71 2610\n",
    "100%|██████████| 2894/2894 [07:56<00:00,  6.08it/s]\n",
    "document no., pct. complete, 1/<P>, M:  7 39.0 82.13 2894\n",
    "100%|██████████| 2099/2099 [05:48<00:00,  6.03it/s]\n",
    "document no., pct. complete, 1/<P>, M:  8 44.0 99.2 2099\n",
    "100%|██████████| 2299/2299 [06:30<00:00,  5.89it/s]\n",
    "document no., pct. complete, 1/<P>, M:  9 50.0 280.71 2299\n",
    "100%|██████████| 3463/3463 [09:31<00:00,  6.06it/s]\n",
    "document no., pct. complete, 1/<P>, M:  10 56.00000000000001 164.21 3463\n",
    "100%|██████████| 1656/1656 [04:49<00:00,  5.72it/s]\n",
    "document no., pct. complete, 1/<P>, M:  11 61.0 194.34 1656\n",
    "100%|██████████| 2576/2576 [08:26<00:00,  5.09it/s]\n",
    "document no., pct. complete, 1/<P>, M:  12 67.0 89.3 2576\n",
    "100%|██████████| 2726/2726 [07:50<00:00,  5.80it/s]\n",
    "document no., pct. complete, 1/<P>, M:  13 72.0 147.42 2726\n",
    "100%|██████████| 2154/2154 [06:19<00:00,  5.67it/s]\n",
    "document no., pct. complete, 1/<P>, M:  14 78.0 38.17 2154\n",
    "100%|██████████| 2945/2945 [10:30<00:00,  4.67it/s]\n",
    "document no., pct. complete, 1/<P>, M:  15 83.0 117.08 2945\n",
    "100%|██████████| 1804/1804 [12:28<00:00,  2.41it/s]\n",
    "document no., pct. complete, 1/<P>, M:  16 89.0 142.28 1804\n",
    "100%|██████████| 1683/1683 [06:10<00:00,  4.54it/s]\n",
    "document no., pct. complete, 1/<P>, M:  17 94.0 100.67 1683\n",
    "100%|██████████| 2749/2749 [08:16<00:00,  5.54it/s]\n",
    "document no., pct. complete, 1/<P>, M:  18 100.0 78.67 2749\n",
    "```\n",
    "\n",
    "- With IFE, trained on 132 GUM train documents, then 10,000 NewsTweet Pre/Post-training articles, then stenciled on all 18 GUM test documents:\n",
    "\n",
    "```\n",
    "Tokenizing documents..\n",
    "100%|██████████| 18/18 [00:02<00:00,  8.85it/s]\n",
    "Evaluating language model..\n",
    "100%|██████████| 2664/2664 [05:43<00:00,  7.75it/s]\n",
    "document no., pct. complete, 1/<P>, M:  1 6.0 153.41 2664\n",
    "100%|██████████| 2540/2540 [05:41<00:00,  7.44it/s]\n",
    "document no., pct. complete, 1/<P>, M:  2 11.0 1324.68 2540\n",
    "100%|██████████| 1869/1869 [04:02<00:00,  7.70it/s]\n",
    "document no., pct. complete, 1/<P>, M:  3 17.0 977.94 1869\n",
    "100%|██████████| 2596/2596 [05:36<00:00,  7.70it/s]\n",
    "document no., pct. complete, 1/<P>, M:  4 22.0 224.7 2596\n",
    "100%|██████████| 2014/2014 [04:34<00:00,  7.34it/s]\n",
    "document no., pct. complete, 1/<P>, M:  5 28.000000000000004 60.83 2014\n",
    "100%|██████████| 2610/2610 [05:38<00:00,  7.71it/s]\n",
    "document no., pct. complete, 1/<P>, M:  6 33.0 270.45 2610\n",
    "100%|██████████| 2894/2894 [06:15<00:00,  7.70it/s]\n",
    "document no., pct. complete, 1/<P>, M:  7 39.0 98.84 2894\n",
    "100%|██████████| 2099/2099 [04:33<00:00,  7.67it/s]\n",
    "document no., pct. complete, 1/<P>, M:  8 44.0 121.84 2099\n",
    "100%|██████████| 2299/2299 [05:13<00:00,  7.33it/s]\n",
    "document no., pct. complete, 1/<P>, M:  9 50.0 343.57 2299\n",
    "100%|██████████| 3463/3463 [07:31<00:00,  7.67it/s]\n",
    "document no., pct. complete, 1/<P>, M:  10 56.00000000000001 239.94 3463\n",
    "100%|██████████| 1656/1656 [03:36<00:00,  7.66it/s]\n",
    "document no., pct. complete, 1/<P>, M:  11 61.0 251.67 1656\n",
    "100%|██████████| 2576/2576 [05:48<00:00,  7.39it/s]\n",
    "document no., pct. complete, 1/<P>, M:  12 67.0 114.0 2576\n",
    "100%|██████████| 2726/2726 [05:56<00:00,  7.64it/s]\n",
    "document no., pct. complete, 1/<P>, M:  13 72.0 171.13 2726\n",
    "100%|██████████| 2154/2154 [04:40<00:00,  7.69it/s]\n",
    "document no., pct. complete, 1/<P>, M:  14 78.0 44.69 2154\n",
    "100%|██████████| 2945/2945 [06:38<00:00,  7.39it/s]\n",
    "document no., pct. complete, 1/<P>, M:  15 83.0 151.02 2945\n",
    "100%|██████████| 1804/1804 [03:55<00:00,  7.65it/s]\n",
    "document no., pct. complete, 1/<P>, M:  16 89.0 160.74 1804\n",
    "100%|██████████| 1683/1683 [03:40<00:00,  7.63it/s]\n",
    "document no., pct. complete, 1/<P>, M:  17 94.0 118.86 1683\n",
    "100%|██████████| 2749/2749 [05:59<00:00,  7.65it/s]\n",
    "document no., pct. complete, 1/<P>, M:  18 100.0 90.08 2749\n",
    "```\n",
    "\n",
    "- With IFE, trained on 132 GUM train documents, then 14,198 (all) NewsTweet Pre/Post-training articles, then stenciled on all 18 GUM test documents:\n",
    "\n",
    "```\n",
    "Tokenizing documents..\n",
    "100%|██████████| 18/18 [00:02<00:00,  8.94it/s]\n",
    "Evaluating language model..\n",
    "100%|██████████| 2664/2664 [07:51<00:00,  5.65it/s]\n",
    "document no., pct. complete, 1/<P>, M:  1 6.0 87.5 2664\n",
    "100%|██████████| 2540/2540 [08:46<00:00,  4.82it/s]\n",
    "document no., pct. complete, 1/<P>, M:  2 11.0 1246.43 2540\n",
    "100%|██████████| 1869/1869 [06:26<00:00,  4.84it/s]\n",
    "document no., pct. complete, 1/<P>, M:  3 17.0 907.98 1869\n",
    "100%|██████████| 2596/2596 [09:09<00:00,  4.72it/s]\n",
    "document no., pct. complete, 1/<P>, M:  4 22.0 185.52 2596\n",
    "100%|██████████| 2014/2014 [06:57<00:00,  4.82it/s]\n",
    "document no., pct. complete, 1/<P>, M:  5 28.000000000000004 60.31 2014\n",
    "100%|██████████| 2610/2610 [09:11<00:00,  4.73it/s]\n",
    "document no., pct. complete, 1/<P>, M:  6 33.0 211.75 2610\n",
    "100%|██████████| 2894/2894 [10:00<00:00,  4.82it/s]\n",
    "document no., pct. complete, 1/<P>, M:  7 39.0 77.63 2894\n",
    "100%|██████████| 2099/2099 [07:28<00:00,  4.68it/s]\n",
    "document no., pct. complete, 1/<P>, M:  8 44.0 94.97 2099\n",
    "100%|██████████| 2299/2299 [07:57<00:00,  4.81it/s]\n",
    "document no., pct. complete, 1/<P>, M:  9 50.0 275.11 2299\n",
    "100%|██████████| 3463/3463 [12:12<00:00,  4.73it/s]\n",
    "document no., pct. complete, 1/<P>, M:  10 56.00000000000001 153.55 3463\n",
    "100%|██████████| 1656/1656 [05:43<00:00,  4.81it/s]\n",
    "document no., pct. complete, 1/<P>, M:  11 61.0 181.12 1656\n",
    "100%|██████████| 2576/2576 [08:58<00:00,  4.78it/s]\n",
    "document no., pct. complete, 1/<P>, M:  12 67.0 79.46 2576\n",
    "100%|██████████| 2726/2726 [09:38<00:00,  4.71it/s]\n",
    "document no., pct. complete, 1/<P>, M:  13 72.0 134.32 2726\n",
    "100%|██████████| 2154/2154 [07:31<00:00,  4.77it/s]\n",
    "document no., pct. complete, 1/<P>, M:  14 78.0 36.48 2154\n",
    "100%|██████████| 2945/2945 [10:24<00:00,  4.71it/s]\n",
    "document no., pct. complete, 1/<P>, M:  15 83.0 105.72 2945\n",
    "100%|██████████| 1804/1804 [06:16<00:00,  4.79it/s]\n",
    "document no., pct. complete, 1/<P>, M:  16 89.0 138.55 1804\n",
    "100%|██████████| 1683/1683 [05:59<00:00,  4.68it/s]\n",
    "document no., pct. complete, 1/<P>, M:  17 94.0 95.61 1683\n",
    "100%|██████████| 2749/2749 [09:35<00:00,  4.78it/s]\n",
    "document no., pct. complete, 1/<P>, M:  18 100.0 76.1 2749\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hey, what are you thinking? I thought y'ar know aboutYouwhatou'rewantthinking?weepingrankings600?theoriesAM\n"
     ]
    }
   ],
   "source": [
    "model.generate(prompt = \" Hey, what are you thinking? \", seed = seed, m = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hey, what are you thinking? I have fon cou, who has L f-c it \n"
     ]
    }
   ],
   "source": [
    "model.generate(prompt = \" Hey, what are you thinking? I have beon waiting since,”2019 More L f-c it \", seed = seed, \n",
    "               revise = [35, 65], prose = 0.75, chunk = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A portable surgical visualization kit comprising: h Jerryllesein her e f,\"It’s\n"
     ]
    }
   ],
   "source": [
    "model.generate(prompt = \" A portable surgical visualization kit comprising: \", seed = seed, m = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A portable ton visualization kit comprising: The Yankee Press \n"
     ]
    }
   ],
   "source": [
    "model.generate(prompt = \" A portable surgical visualization kit comprising: The Yankee Press \", seed = seed, \n",
    "               revise = [12, 19], prose = 0.75, chunk = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
