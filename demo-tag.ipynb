{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's a Machine and Natural Language Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:18<00:00, 273.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gold-tagged UDs data...\n",
      "Avail. pre-train, total pre-train, Avail. gold, total gold-train, total test-gold:  14198 5000 150 132 18\n"
     ]
    }
   ],
   "source": [
    "from src.IaMaN.base import LM\n",
    "from src.utils.data import load_ud\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "seed = 691\n",
    "\n",
    "print(\"Loading pre-training data...\")\n",
    "pretrain_path = '/data/newstweet/week_2019-40_article_texts/'\n",
    "total_pretrain = len([pretrain_file for pretrain_file in os.listdir(pretrain_path) if re.search(\"^\\d+.txt$\", pretrain_file)])\n",
    "num_pretrain = 5000 # total_pretrain\n",
    "\n",
    "all_pretrain_files = [pretrain_file for pretrain_file in os.listdir(pretrain_path) if re.search(\"^\\d+.txt$\", pretrain_file)]\n",
    "if num_pretrain:\n",
    "    np.random.seed(seed)\n",
    "    pretrain_files = np.random.choice(all_pretrain_files, size=num_pretrain, replace=False)\n",
    "else:\n",
    "    pretrain_files = np.array([])\n",
    "\n",
    "ptdocs = [[[open(pretrain_path+pretrain_file).read()]] for pretrain_file in tqdm(pretrain_files)]\n",
    "\n",
    "print(\"Loading gold-tagged UDs data...\")\n",
    "max_char = 200_000_000\n",
    "load_set = 'GUM'; fine_tune = True; do_ife = False; update_ife = False; runners = 10\n",
    "all_docs = load_ud(\"English\", num_articles = 0, seed = 691, load_set = load_set, rebuild = True)\n",
    "test_docs = [doc for doc in all_docs if 'test' in doc['id'] and len(doc['text']) <= max_char]# [:1]\n",
    "train_docs = [doc for doc in all_docs if 'test' not in doc['id'] and len(doc['text']) <= max_char]# [:4]\n",
    "nsamp = len(test_docs)\n",
    "print('Avail. pre-train, total pre-train, Avail. gold, total gold-train, total test-gold: ', \n",
    "      total_pretrain, len(ptdocs), len(all_docs), len(train_docs), len(test_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing: 100%|██████████| 6503/6503 [00:01<00:00, 3738.49it/s]\n",
      "Fitting:  18%|█▊        | 18/100 [00:41<03:07,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built a vocabulary of 10703 types\n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [00:17<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting documents and aggregating counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4296952it [03:00, 23824.65it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [00:11<00:00, 11.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [00:00<00:00, 460.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4296952/4296952 [00:20<00:00, 205131.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing marginal statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5137899/5137899 [00:21<00:00, 239584.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dense output heads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:51<00:00, 11.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Model params, types, encoding size, contexts, vec dim, max sent, and % capacity used: 4296952 10704 10704 224784 11000 177 0.179\n",
      "Processing pre-training documents...\n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [23:41<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting documents and aggregating counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8958836it [30:30, 4893.06it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:45<00:00, 109.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:02<00:00, 2070.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11951248/11951248 [00:56<00:00, 211578.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-computing marginal statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17135672/17135672 [00:57<00:00, 296228.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-building dense output heads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:20<00:00, 14.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Model params, types, encoding size, contexts, vec dim, max sent, and % capacity used: 11951248 10704 10704 224784 11000 177 0.497\n",
      "Fine-tuning dense output heads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 39/132 [22:38:06<50:19:25, 1948.02s/it]"
     ]
    }
   ],
   "source": [
    "docs = [[\"\".join([row[1] for row in s]) for s in d['conllu']] for d in train_docs]\n",
    "tdocs = [[\"\".join([row[1] for row in s]) for s in d['conllu']] for d in test_docs]\n",
    "covering = [[[row[1] for row in s] for s in d['conllu']] for d in train_docs]\n",
    "covering_vocab = set([t for d in covering for s in d for t in s])\n",
    "\n",
    "all_layers = {d_i: {'pos': [[row[3] for row in s] for s in d['conllu']], \n",
    "                    'sup': [[(str(int(row[6]) - int(row[0])) if int(row[6]) else row[6]) for row in s] for s in d['conllu']], \n",
    "                    'dep': [[row[7] for row in s] for s in d['conllu']],\n",
    "                    'sty': [[d['s_type'][s_i] for row in s] for s_i, s in enumerate(d['conllu'])]}\n",
    "              for d_i, d in enumerate(train_docs)}\n",
    "\n",
    "model = LM(covering_vocab = covering_vocab)\n",
    "model.init(m = 10, noise = 0.001, positional = True, seed = seed, do_ife = do_ife, runners = runners)\n",
    "model.fit(docs, f'{load_set}-{nsamp}', covering = covering, all_layers = all_layers)\n",
    "model.pre_train(ptdocs, update_ife = update_ife)\n",
    "if fine_tune:\n",
    "    model.fine_tune(docs, covering = covering, all_layers = all_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Currently__: ordering for the current fine tuning process:\n",
    "1. train tokenizer and fit model to GUM\n",
    "2. process NewsTweet documents to integrate sparse post-training statistics (requires mr implementation and updates to the vocabularies/indices)\n",
    "3. update the ife and dense model, i.e., produce new statistics and dimensionalities\n",
    "4. fine tune output heads to GUM, and _combine_ them with the dense model from (3), i.e., don't just replace as is current.\n",
    "\n",
    "__Preliminarily__: this does seem to present performance benefits, but as is usual will require 'big data' statistics to become competitive. In particular, the (tokenization, least of all), counting, sorting, and aggregation of co-occurrence counts must all be distributed for the statistical resolution required to approach performance gains aking to more-advanced systems. Currently, a spark-based MR system is implemented for these (all but tokenization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_docs = list([docs[3][0:1]])\n",
    "print(interpret_docs)\n",
    "model.interpret(interpret_docs, seed = 691)\n",
    "for doc in model._documents:\n",
    "    print('opening next doc:')\n",
    "    for s in doc._sentences:\n",
    "        print('opening next sent:')\n",
    "        for t in s._tokens:\n",
    "            print('opening next token:')\n",
    "            print((str(t), t._pos, t._sep, t._sup, t._dep, s._sty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_docs = list([docs[3][0:1]])\n",
    "print(interpret_docs)\n",
    "model.interpret(interpret_docs, seed = 691, dense_predict = True)\n",
    "for doc in model._documents:\n",
    "    print('opening next doc:')\n",
    "    for s in doc._sentences:\n",
    "        print('opening next sent:')\n",
    "        for t in s._tokens:\n",
    "            print('opening next token:')\n",
    "            print((str(t), t._pos, t._sep, t._sup, t._dep, s._sty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[[(int(row[6])-int(row[0]) if int(row[6]) else int(row[6]), row[7]) for row in s] for s in d['conllu']] for d in train_docs][3][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_docs = list([tdocs[0][1:2]])\n",
    "print(interpret_docs)\n",
    "model.interpret(interpret_docs, seed = 691)\n",
    "for doc in model._documents:\n",
    "    print('opening next doc:')\n",
    "    for s in doc._sentences:\n",
    "        print('opening next sent:')\n",
    "        for t in s._tokens:\n",
    "            print('opening next token:')\n",
    "            print((str(t), t._pos, t._sep, t._sup, t._dep, s._sty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_docs = list([tdocs[0][1:2]])\n",
    "print(interpret_docs)\n",
    "interpret_covering =  [[[row[1] for row in s] for s in d['conllu']][1:2] for d in test_docs[0:1]]\n",
    "print(interpret_covering)\n",
    "model.interpret(interpret_docs, seed = 691, covering = interpret_covering)\n",
    "\n",
    "accuracy = defaultdict(list)\n",
    "accuracy_nsp = defaultdict(list)\n",
    "accuracy_all, accuracy_all_nsp, = [], []\n",
    "sup_accuracy, sup_accuracy_nsp, = 0, 0\n",
    "accuracy_sty = defaultdict(list)\n",
    "accuracy_all_sty = []\n",
    "\n",
    "for d_i, doc in enumerate(model._documents):\n",
    "    for s_i, s in enumerate(doc._sentences):\n",
    "        result = s._sty == test_docs[0:1][d_i]['s_type'][s_i+1]\n",
    "        accuracy_sty[test_docs[0:1][d_i]['s_type'][s_i+1]].append(result)\n",
    "        accuracy_all_sty.append(result)\n",
    "\n",
    "pred_toks = [t._form for doc in model._documents for s in doc._sentences for t in s._tokens]\n",
    "pred_arcs = set([(ix, str(t._sup), t._dep) for doc in model._documents for s in doc._sentences for ix, t in enumerate(s._tokens)])\n",
    "pred_spans = list(np.cumsum([len(t) for t in pred_toks]))\n",
    "pred_stream = [t._pos for doc in model._documents for s in doc._sentences for t in s._tokens]\n",
    "pred_spans = {(sh-len(gt), sh): (gl, gt)\n",
    "              for sh, gt, gl in zip(pred_spans, pred_toks, pred_stream)}\n",
    "\n",
    "gold_toks = [row[1] for d in test_docs[:1] for s in d['conllu'][1:2] for row in s]\n",
    "gold_arcs = set([(ix, (str(int(row[6]) - int(row[0])) if int(row[6]) else row[6]),\n",
    "                  row[7]) for d in test_docs[:1] for s in d['conllu'][1:2] for ix, row in enumerate(s)])\n",
    "gold_spans = list(np.cumsum([len(t) for t in gold_toks]))\n",
    "gold_stream = [row[3] for d in test_docs[:1] for s in d['conllu'][1:2] for row in s]\n",
    "gold_spans = {(sh-len(gt), sh): (gl, gt)\n",
    "              for sh, gt, gl in zip(gold_spans, gold_toks, gold_stream)}\n",
    "\n",
    "for gold_span in gold_spans:\n",
    "    if gold_span in pred_spans:\n",
    "        result = gold_spans[gold_span] == pred_spans[gold_span]\n",
    "    else:\n",
    "        result = False\n",
    "    accuracy[gold_spans[gold_span][0]].append(result)\n",
    "    accuracy_all.append(result)\n",
    "    if gold_spans[gold_span][1] != ' ':\n",
    "        accuracy_nsp[gold_spans[gold_span][0]].append(result)\n",
    "        accuracy_all_nsp.append(result)\n",
    "        \n",
    "for ptok, parc in zip(pred_toks, pred_arcs):\n",
    "    if parc in gold_arcs:\n",
    "        sup_accuracy += 1\n",
    "        if ptok != ' ':\n",
    "            sup_accuracy_nsp += 1\n",
    "sup_accuracy /= len(pred_toks)\n",
    "sup_accuracy_nsp /= len([x for x in pred_toks if x != ' '])\n",
    "\n",
    "print(\"Tag-wise POS accuracy with/out space\", {tag: sum(accuracy[tag])/len(accuracy[tag]) for tag in accuracy}, \n",
    "                                          {tag: sum(accuracy_nsp[tag])/len(accuracy_nsp[tag]) for tag in accuracy_nsp})\n",
    "print(\"Overall POS accuracy with/out space\", sum(accuracy_all)/len(accuracy_all), sum(accuracy_all_nsp)/len(accuracy_all_nsp))\n",
    "print(\"Overall SUP:DEP accuracy with/out space\", sup_accuracy, sup_accuracy_nsp)\n",
    "print(\"Overall s_type accuracy: \", sum(accuracy_all_sty)/len(accuracy_all_sty))\n",
    "print(\"Tag-wise accuracy\", list(Counter({tag: (sum(accuracy_sty[tag])/len(accuracy_sty[tag]), len(accuracy_sty[tag])) \n",
    "                                         for tag in accuracy_sty}).most_common()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in model._documents:\n",
    "    print('opening next doc:')\n",
    "    for s in doc._sentences:\n",
    "        print('opening next sent:')\n",
    "        for t in s._tokens:\n",
    "            print('opening next token:')\n",
    "            print((str(t), t._pos, t._sep, t._sup, t._dep, s._sty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_docs = list([tdocs[0][:2]])\n",
    "print(interpret_docs)\n",
    "interpret_covering =  [[[row[1] for row in s] for s in d['conllu']][:2] for d in test_docs[0:1]]\n",
    "print(interpret_covering)\n",
    "model.interpret(interpret_docs, seed = 691, covering = interpret_covering)\n",
    "\n",
    "accuracy = defaultdict(list)\n",
    "accuracy_nsp = defaultdict(list)\n",
    "accuracy_all, accuracy_all_nsp, = [], []\n",
    "sup_accuracy, sup_accuracy_nsp, = 0, 0\n",
    "accuracy_sty = defaultdict(list)\n",
    "accuracy_all_sty = []\n",
    "\n",
    "for d_i, doc in enumerate(model._documents):\n",
    "    for s_i, s in enumerate(doc._sentences):\n",
    "        result = s._sty == test_docs[0:1][d_i]['s_type'][s_i]\n",
    "        accuracy_sty[test_docs[0:1][d_i]['s_type'][s_i]].append(result)\n",
    "        accuracy_all_sty.append(result)\n",
    "\n",
    "pred_toks = [t._form for doc in model._documents for s in doc._sentences for t in s._tokens]\n",
    "pred_arcs = set([(ix, str(t._sup), t._dep, s_i, d_i) for d_i, doc in enumerate(model._documents) for s_i, s in enumerate(doc._sentences) for ix, t in enumerate(s._tokens)])\n",
    "pred_spans = list(np.cumsum([len(t) for t in pred_toks]))\n",
    "pred_stream = [t._pos for doc in model._documents for s in doc._sentences for t in s._tokens]\n",
    "pred_spans = {(sh-len(gt), sh): (gl, gt)\n",
    "              for sh, gt, gl in zip(pred_spans, pred_toks, pred_stream)}\n",
    "\n",
    "gold_toks = [row[1] for d in test_docs[:1] for s in d['conllu'][:2] for row in s]\n",
    "gold_arcs = set([(ix, (str(int(row[6]) - int(row[0])) if int(row[6]) else row[6]), row[7], s_i, d_i) \n",
    "                 for d_i, d in enumerate(test_docs[:1]) for s_i, s in enumerate(d['conllu'][:2]) for ix, row in enumerate(s)])\n",
    "gold_spans = list(np.cumsum([len(t) for t in gold_toks]))\n",
    "gold_stream = [row[3] for d in test_docs[:1] for s in d['conllu'][:2] for row in s]\n",
    "gold_spans = {(sh-len(gt), sh): (gl, gt)\n",
    "              for sh, gt, gl in zip(gold_spans, gold_toks, gold_stream)}\n",
    "\n",
    "for gold_span in gold_spans:\n",
    "    if gold_span in pred_spans:\n",
    "        result = gold_spans[gold_span] == pred_spans[gold_span]\n",
    "    else:\n",
    "        result = False\n",
    "    accuracy[gold_spans[gold_span][0]].append(result)\n",
    "    accuracy_all.append(result)\n",
    "    if gold_spans[gold_span][1] != ' ':\n",
    "        accuracy_nsp[gold_spans[gold_span][0]].append(result)\n",
    "        accuracy_all_nsp.append(result)\n",
    "        \n",
    "for ptok, parc in zip(pred_toks, pred_arcs):\n",
    "    if parc in gold_arcs:\n",
    "        sup_accuracy += 1\n",
    "        if ptok != ' ':\n",
    "            sup_accuracy_nsp += 1\n",
    "sup_accuracy /= len(pred_toks)\n",
    "sup_accuracy_nsp /= len([x for x in pred_toks if x != ' '])\n",
    "\n",
    "print(\"Tag-wise POS accuracy with/out space\", {tag: sum(accuracy[tag])/len(accuracy[tag]) for tag in accuracy}, \n",
    "                                          {tag: sum(accuracy_nsp[tag])/len(accuracy_nsp[tag]) for tag in accuracy_nsp})\n",
    "print(\"Overall POS accuracy with/out space\", sum(accuracy_all)/len(accuracy_all), sum(accuracy_all_nsp)/len(accuracy_all_nsp))\n",
    "print(\"Overall SUP:DEP accuracy with/out space\", sup_accuracy, sup_accuracy_nsp)\n",
    "print(\"Overall s_type accuracy: \", sum(accuracy_all_sty)/len(accuracy_all_sty))\n",
    "print(\"Tag-wise accuracy\", list(Counter({tag: (sum(accuracy_sty[tag])/len(accuracy_sty[tag]), len(accuracy_sty[tag])) \n",
    "                                         for tag in accuracy_sty}).most_common()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in model._documents:\n",
    "    print('opening next doc:')\n",
    "    for s in doc._sentences:\n",
    "        print('opening next sent:')\n",
    "        for t in s._tokens:\n",
    "            print('opening next token:')\n",
    "            print((str(t), t._pos, t._sep, t._sup, t._dep, s._sty))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._documents[0]._sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model._documents[0]._sentences[0], \n",
    "      [t_i for  t_i in model._documents[0]._sentences[0].yield_branch(13)],\n",
    "      [model._documents[0]._sentences[0]._tokens[t_i] for  t_i in sorted(model._documents[0]._sentences[0].yield_branch(13))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(sorted(pred_arcs, key = lambda x: (x[3],x[4],x[0])), sorted(gold_arcs, key = lambda x: (x[3],x[4],x[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.interpret(interpret_docs, seed = 691, covering = interpret_covering, predict_tags = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(\"model vector dimension: \", model._documents[0]._sentences[0]._tokens[0]._whatevers[0]._vec.shape[0])\n",
    "model._documents[0]._sentences[0]._tokens[0]._whatevers[0]._vec.shape[0]\n",
    "nspv = model._documents[0]._sentences[0]._tokens[1]._whatevers[-1]._vec; nspvn = np.linalg.norm(nspv)\n",
    "spv1 = model._documents[0]._sentences[0]._tokens[2]._whatevers[-1]._vec; spv1n = np.linalg.norm(spv1)\n",
    "spv2 = model._documents[0]._sentences[0]._tokens[4]._whatevers[-1]._vec; spv2n = np.linalg.norm(spv2)\n",
    "\n",
    "print(\"vector similarity of non-space to space whatevers (first two), \\nvs. similarity of separate space tokens (last one): \\n\\n\",\n",
    "      nspv.dot(spv1)/(nspvn*spv1n), nspv.dot(spv2)/(nspvn*spv2n), spv1.dot(spv2)/(spv1n*spv2n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.interpret(interpret_docs, seed = 691, covering = interpret_covering, predict_tags = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(\"model vector dimension: \", model._documents[0]._sentences[0]._tokens[0]._whatevers[0]._vec.shape[0])\n",
    "model._documents[0]._sentences[0]._tokens[0]._whatevers[0]._vec.shape[0]\n",
    "nspv = model._documents[0]._sentences[0]._tokens[1]._whatevers[-1]._vec; nspvn = np.linalg.norm(nspv)\n",
    "spv1 = model._documents[0]._sentences[0]._tokens[2]._whatevers[-1]._vec; spv1n = np.linalg.norm(spv1)\n",
    "spv2 = model._documents[0]._sentences[0]._tokens[4]._whatevers[-1]._vec; spv2n = np.linalg.norm(spv2)\n",
    "\n",
    "print(\"vector similarity of non-space to space whatevers (first two), \\nvs. similarity of separate space tokens (last one): \\n\\n\",\n",
    "      nspv.dot(spv1)/(nspvn*spv1n), nspv.dot(spv2)/(nspvn*spv2n), spv1.dot(spv2)/(spv1n*spv2n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(doc._vec.shape, [doc._atn, s._atn, t._atn, t._whatevers[0]._atn], \n",
    " [doc._nrm, s._nrm, t._nrm, t._whatevers[0]._nrm], \n",
    " [doc._vec[:10], s._vec[:10], t._vec[:10], t._whatevers[0]._vec[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "accuracy = defaultdict(list)\n",
    "accuracy_nsp = defaultdict(list)\n",
    "accuracy_all, accuracy_all_nsp, = [], []\n",
    "sup_accuracy, sup_accuracy_nsp, = 0, 0\n",
    "accuracy_sty = defaultdict(list)\n",
    "accuracy_all_sty = []\n",
    "\n",
    "model.interpret(tdocs, seed = 691, covering = [[[row[1] for row in s] for s in d['conllu']] for d in test_docs]) \n",
    "                # ltypes = ['lem'],  layers = [[[[row[2] for row in s] for s in d['conllu']] for d in test_docs]])\n",
    "\n",
    "for d_i, doc in enumerate(model._documents):\n",
    "    for s_i, s in enumerate(doc._sentences):\n",
    "        result = s._sty == test_docs[d_i]['s_type'][s_i]\n",
    "        accuracy_sty[test_docs[d_i]['s_type'][s_i]].append(result)\n",
    "        accuracy_all_sty.append(result)\n",
    "\n",
    "pred_toks = [t._form for doc in model._documents for s in doc._sentences for t in s._tokens]\n",
    "pred_arcs = set([(ix, str(t._sup), t._dep, d_i, s_i) for d_i, doc in enumerate(model._documents) \n",
    "                 for s_i, s in enumerate(doc._sentences) for ix, t in enumerate(s._tokens)])\n",
    "pred_spans = list(np.cumsum([len(t) for t in pred_toks]))\n",
    "pred_stream = [t._pos for doc in model._documents for s in doc._sentences for t in s._tokens]\n",
    "pred_spans = {(sh-len(gt), sh): (gl, gt)\n",
    "              for sh, gt, gl in zip(pred_spans, pred_toks, pred_stream)}\n",
    "\n",
    "gold_toks = [row[1] for d in test_docs for s in d['conllu'] for row in s]\n",
    "gold_arcs = set([(ix, (str(int(row[6]) - int(row[0])) if int(row[6]) else row[6]), row[7], d_i, s_i) \n",
    "                 for d_i, d in enumerate(test_docs) for s_i, s in enumerate(d['conllu']) for ix, row in enumerate(s)])\n",
    "gold_spans = list(np.cumsum([len(t) for t in gold_toks]))\n",
    "gold_stream = [row[3] for d in test_docs for s in d['conllu'] for row in s]\n",
    "gold_spans = {(sh-len(gt), sh): (gl, gt)\n",
    "              for sh, gt, gl in zip(gold_spans, gold_toks, gold_stream)}\n",
    "\n",
    "for gold_span in gold_spans:\n",
    "    if gold_span in pred_spans:\n",
    "        result = gold_spans[gold_span] == pred_spans[gold_span]\n",
    "    else:\n",
    "        result = False\n",
    "    accuracy[gold_spans[gold_span][0]].append(result)\n",
    "    accuracy_all.append(result)\n",
    "    if gold_spans[gold_span][1] != ' ':\n",
    "        accuracy_nsp[gold_spans[gold_span][0]].append(result)\n",
    "        accuracy_all_nsp.append(result)\n",
    "        \n",
    "for ptok, parc in zip(pred_toks, pred_arcs):\n",
    "    if parc in gold_arcs:\n",
    "        sup_accuracy += 1\n",
    "        if ptok != ' ':\n",
    "            sup_accuracy_nsp += 1\n",
    "sup_accuracy /= len(pred_toks)\n",
    "sup_accuracy_nsp /= len([x for x in pred_toks if x != ' '])\n",
    "\n",
    "print(\"Overall POS accuracy with/out space\", sum(accuracy_all)/len(accuracy_all), sum(accuracy_all_nsp)/len(accuracy_all_nsp))\n",
    "print(\"Overall SUP:DEP accuracy with/out space\", sup_accuracy, sup_accuracy_nsp)\n",
    "print(\"Overall s_type accuracy: \", sum(accuracy_all_sty)/len(accuracy_all_sty))\n",
    "print(\"Tag-wise accuracy\", list(Counter({tag: (sum(accuracy_sty[tag])/len(accuracy_sty[tag]), len(accuracy_sty[tag])) \n",
    "                                         for tag in accuracy_sty}).most_common()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Tag-wise accuracy with space\", list(Counter({tag: (sum(accuracy[tag])/len(accuracy[tag]), len(accuracy[tag])) for tag in accuracy}).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Tag-wise accuracy without space\", list(Counter({tag: (sum(accuracy_nsp[tag])/len(accuracy_nsp[tag]), len(accuracy_nsp[tag])) for tag in accuracy_nsp}).most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model params, types, encoding size, contexts, vec dim, max sent, and % capacity used: 4296952 10704 10704 224784 11000 177 0.179\n",
    "- Train = 132; post-train = 0; fine-tune = 132; test = 18; IFE = False\n",
    "\n",
    "```\n",
    "Overall POS accuracy with/out space 0.8729921677950352 0.7639985199802664\n",
    "('Tag-wise accuracy with space',\n",
    " [('PUNCT', (0.9917376064096144, 15976)),\n",
    "  ('PRON', (0.861652739090065, 1077)),\n",
    "  ('DET', (0.8605150214592274, 1398)),\n",
    "  ('ADP', (0.8532002348796242, 1703)),\n",
    "  ('NOUN', (0.852911477616964, 2971)),\n",
    "  ('CCONJ', (0.7448979591836735, 588)),\n",
    "  ('VERB', (0.7441441441441441, 1665)),\n",
    "  ('AUX', (0.7142857142857143, 721)),\n",
    "  ('PART', (0.6507462686567164, 335)),\n",
    "  ('SCONJ', (0.6147540983606558, 244)),\n",
    "  ('ADJ', (0.566003616636528, 1106)),\n",
    "  ('NUM', (0.565597667638484, 343)),\n",
    "  ('ADV', (0.564437194127243, 613)),\n",
    "  ('INTJ', (0.5113636363636364, 88)),\n",
    "  ('PROPN', (0.4561544650040225, 1243)),\n",
    "  ('SYM', (0.11428571428571428, 35)),\n",
    "  ('X', (0.038461538461538464, 26))])\n",
    "Overall SUP:DEP accuracy with/out space 0.6265944724953495 0.6232394366197183\n",
    "Overall s_type accuracy:  0.7259507829977628\n",
    "Tag-wise accuracy [('decl', (0.9830220713073005, 589)), \n",
    "                   ('intj', (0.5, 26)), \n",
    "                   ('q', (0.5, 16)), \n",
    "                   ('imp', (0.3877551020408163, 49)), \n",
    "                   ('frag', (0.25806451612903225, 93)), \n",
    "                   ('wh', (0.19047619047619047, 21)), \n",
    "                   ('multiple', (0.03125, 32)), \n",
    "                   ('sub', (0.024390243902439025, 41)), \n",
    "                   ('other', (0.0, 14)), \n",
    "                   ('ger', (0.0, 8)), \n",
    "                   ('inf', (0.0, 5))]\n",
    "```\n",
    "\n",
    "- Model params, types, encoding size, contexts, vec dim, max sent, and % capacity used: 11951248 10704 10704 224784 11000 177 0.497\n",
    "- Train = 132; post-train = 5000; fine-tune = 132; test = 18; IFE = False\n",
    "\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
