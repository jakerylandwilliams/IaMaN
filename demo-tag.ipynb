{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's a Machine and Natural Language Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading post-training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gold-tagged UDs data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avail. post-train, total post-train, Avail. gold, total gold-train, total test-gold:  14198 0 150 132 18\n"
     ]
    }
   ],
   "source": [
    "from src.IaMaN.base import LM\n",
    "from src.utils.data import load_ud\n",
    "from src.utils.munge import stick_spaces\n",
    "from pprint import pprint as pprint\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "seed = 691; max_char = 200_000_000\n",
    "m = 10; space = True; fine_tune = False; num_posttrain = 0; noise = 0.001\n",
    "positional = 'dependent'; positionally_encode = True; bits = 50; update = True; btype = 'f'; ms_init = 'waiting_time'\n",
    "runners = 10; gpu = False; tokenizer = 'hr-bpe'; decode_method = 'argmax'\n",
    "\n",
    "print(\"Loading post-training data...\")\n",
    "posttrain_path = '/data/newstweet/week_2019-40_article_texts/'\n",
    "total_posttrain = len([posttrain_file for posttrain_file in os.listdir(posttrain_path) if re.search(\"^\\d+.txt$\", posttrain_file)])\n",
    "all_posttrain_files = [posttrain_file for posttrain_file in os.listdir(posttrain_path) if re.search(\"^\\d+.txt$\", posttrain_file)]\n",
    "if num_posttrain:\n",
    "    np.random.seed(seed)\n",
    "    posttrain_files = np.random.choice(all_posttrain_files, size=num_posttrain, replace=False)\n",
    "else:\n",
    "    posttrain_files = np.array([])\n",
    "ptdocs = [[[open(posttrain_path+posttrain_file).read()]] for posttrain_file in tqdm(posttrain_files)]\n",
    "print(\"Loading gold-tagged UDs data...\")\n",
    "load_set = \"GUM\"\n",
    "all_docs = load_ud(\"English\", num_articles = 0, seed = seed, load_set = load_set, rebuild = True, space = space)\n",
    "test_docs = [doc for doc in all_docs if 'test' in doc['id'] and len(doc['text']) <= max_char] # [:2]\n",
    "train_docs = [doc for doc in all_docs if 'test' not in doc['id'] and len(doc['text']) <= max_char] # [:4]\n",
    "nsamp = len(test_docs)\n",
    "print('Avail. post-train, total post-train, Avail. gold, total gold-train, total test-gold: ', \n",
    "      total_posttrain, len(ptdocs), len(all_docs), len(train_docs), len(test_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing: 100%|██████████| 6503/6503 [00:01<00:00, 4466.59it/s]\n",
      "Fitting:  20%|██        | 20/100 [00:42<02:49,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built a vocabulary of 15327 types\n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [00:14<00:00,  8.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [00:05<00:00, 24.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting documents and aggregating counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5082533it [03:39, 23171.26it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting tag-tag transition frequencies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [00:00<00:00, 157.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5082533/5082533 [00:55<00:00, 91682.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building target vocabularies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 1015.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-computing BOW probabilities... done.\n",
      "Pre-computing wave amplitudes... done.\n",
      "Stacking output vocabularies for decoders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 83886.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dense output heads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 32.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building transition matrices for tag-sequence decoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]/code/IaMaN/src/IaMaN/base.py:613: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self._trXs[ltype] = (lambda M: M/M.sum(axis = 1)[:,None])(self._trXs[ltype])\n",
      "100%|██████████| 7/7 [00:00<00:00, 2147.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Model params, types, encoding size, contexts, vec dim, max sent, and % capacity used: 886984 10568 50 1071 353 258 9.681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = [[\"\".join([row[1] for row in s]) for s in d['conllu']] for d in train_docs]\n",
    "tdocs = [[\"\".join([row[1] for row in s]) for s in d['conllu']] for d in test_docs]\n",
    "covering = [[[row[1] for row in s] for s in d['conllu']] for d in train_docs]\n",
    "tcovering = [[[row[1] for row in s] for s in d['conllu']] for d in test_docs]\n",
    "if not space:\n",
    "    for d_i, d in enumerate(covering):\n",
    "        for s_i, s in enumerate(d):\n",
    "            covering[d_i][s_i] = stick_spaces(s)\n",
    "    for d_i, d in enumerate(tcovering):\n",
    "        for s_i, s in enumerate(d):\n",
    "            tcovering[d_i][s_i] = stick_spaces(s)\n",
    "covering_vocab = set([t for d in covering for s in d for t in s])\n",
    "\n",
    "# 'lem': [[row[2] for row in s] for s in d['conllu']], # note: for speed, remove lemma layer\n",
    "train_layers = {d_i: {'sty': [[d['s_type'][s_i] for row in s] for s_i, s in enumerate(d['conllu'])], \n",
    "                      'pos': [[row[3] for row in s] for s in d['conllu']], \n",
    "                      'sup': [[(str(int(row[6]) - int(row[0])) if int(row[6]) else row[6]) for row in s] for s in d['conllu']], \n",
    "                      'dep': [[row[7] for row in s] for s in d['conllu']]}\n",
    "                for d_i, d in enumerate(train_docs)}\n",
    "# 'lem': [[row[2] for row in s] for s in d['conllu']], # note: for speed, remove lemma layer\n",
    "test_layers = {d_i: {'sty': [[d['s_type'][s_i] for row in s] for s_i, s in enumerate(d['conllu'])], \n",
    "                     'pos': [[row[3] for row in s] for s in d['conllu']], \n",
    "                     'sup': [[(str(int(row[6]) - int(row[0])) if int(row[6]) else row[6]) for row in s] for s in d['conllu']], \n",
    "                     'dep': [[row[7] for row in s] for s in d['conllu']]}\n",
    "               for d_i, d in enumerate(test_docs)}\n",
    "\n",
    "model = LM(m = m, tokenizer = tokenizer, noise = noise, seed = seed, space = space, positional = positional,\n",
    "           positionally_encode = positionally_encode, runners = runners, gpu = gpu, bits = bits, \n",
    "           btype = btype, ms_init = ms_init)\n",
    "data_streams = model.fit(docs, f'{load_set}-{nsamp}', covering = covering, all_layers = train_layers, fine_tune = fine_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sty': 2,\n",
       " 'pos': 6,\n",
       " 'sup': 7,\n",
       " 'dep': 7,\n",
       " 'nov': 2,\n",
       " 'iat': 2,\n",
       " 'bot': 2,\n",
       " 'eot': 2,\n",
       " 'eos': 2,\n",
       " 'eod': 2,\n",
       " 'form': 8,\n",
       " 'bits': 10}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2425.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13025.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1569.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 114.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "opening next doc:\n",
      "opening next sent: decl\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "['\" \"']\n",
      "opening next token: Gone, None, False, PROPN, 0, root\n",
      "['\"Gon\"', '\"e\"']\n",
      "opening next token:  , None, False, SPACE, -1, space\n",
      "['\" \"']\n",
      "opening next token: again, None, False, X, -1, dep\n",
      "['\"again\"']\n",
      "opening next token: !, None, True, PUNCT, -1, punct\n",
      "['\"!\"']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.interpret([[' Gone again!']], seed = seed)\n",
    "for doc in model._documents:\n",
    "    print('opening next doc:')\n",
    "    for s in doc._sentences:\n",
    "        print(f'opening next sent: {s._sty}')\n",
    "        for t in s._tokens:\n",
    "            print(f'opening next token: {t._form}, {t._lem}, {t._sep}, {t._pos}, {t._sup}, {t._dep}')\n",
    "            print([\"\\\"\"+w._form+\"\\\"\" for w in t._whatevers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' Emperor Norton ', ' Joshua Abraham Norton (c. 1818 – January 8, 1880), known as Emperor Norton, was a citizen of San Francisco, California, who in 1859 proclaimed himself \"Norton I, Emperor of the United States\". ']]\n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 124.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5882.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 181.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0's Sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 0's STY accuracy:  0.5\n",
      "Document 0's Token segmentation performance without space:  {('P', 0.9782608695652174), ('R', 0.9782608695652174), ('F', 0.9782608695652174)}\n",
      "Document 0's POS accuracy with/out space: 0.7654320987654321 0.5777777777777777\n",
      "Document 0's SUP:DEP accuracy with/out space:  0.5555555555555556 0.2\n",
      "\n",
      "Overall sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Overall STY accuracy:  0.5\n",
      "Overall Token segmentation performance without space:  {('P', 0.9782608695652174), ('R', 0.9782608695652174), ('F', 0.9782608695652174)}\n",
      "Overall POS accuracy with/out space: 0.7654320987654321 0.5777777777777777\n",
      "Overall SUP:DEP accuracy with/out space:  0.5555555555555556 0.2\n",
      "opening next doc:\n",
      "opening next sent: decl\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: Emperor, None, False, PROPN, 0, root\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: Norton, None, False, PROPN, -2, flat\n",
      "opening next token:  , None, True, SPACE, -1, space\n",
      "opening next sent: decl\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: Joshua, None, False, PRON, 4, case\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: Abraham, None, False, VERB, 2, det\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: Norton, None, False, ADP, 4, case\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: (, None, False, NOUN, -6, nmod\n",
      "opening next token: c, None, False, NUM, -3, nmod:tmod\n",
      "opening next token: ., None, False, PUNCT, 2, punct\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: 1818, None, False, PROPN, 2, compound\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: –, None, False, SYM, 4, case\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: January, None, False, PROPN, 2, flat\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: 8, None, False, PROPN, 5, nmod\n",
      "opening next token: ,, None, False, PUNCT, -3, punct\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: 1880, None, False, NUM, -3, nmod:tmod\n",
      "opening next token: ), None, False, PUNCT, -3, punct\n",
      "opening next token: ,, None, False, PUNCT, 2, punct\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: known, None, False, CCONJ, 6, cc\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: as, None, False, AUX, 2, conj\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: Emperor, None, False, ADJ, 2, advcl\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: Norton, None, False, NOUN, 2, nmod\n",
      "opening next token: ,, None, False, PUNCT, 8, punct\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: was, None, False, AUX, 6, case\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: a, None, False, DET, 4, det\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: citizen, None, False, NOUN, 2, conj\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: of, None, False, ADP, 4, case\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: San, None, False, PROPN, 2, det\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: Francisco, None, False, PROPN, 5, flat\n",
      "opening next token: ,, None, False, PUNCT, 2, punct\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: California, None, False, PROPN, 9, appos\n",
      "opening next token: ,, None, False, PUNCT, 6, punct\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: who, None, False, PROPN, 6, nsubj\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: in, None, False, ADP, 4, case\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: 1859, None, False, PRON, 2, nsubj\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: proclaimed, None, False, VERB, 0, root\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: himself, None, False, VERB, 2, obj\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: \", None, False, NOUN, -4, obj\n",
      "opening next token: Norton, None, False, ADP, 2, case\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: I, None, False, PROPN, -4, obl\n",
      "opening next token: ,, None, False, PUNCT, 8, punct\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: Emperor, None, False, PROPN, 6, appos\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: of, None, False, ADP, 6, case\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: the, None, False, DET, 4, det\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: United, None, False, PROPN, 2, nmod\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: States, None, False, NOUN, -16, nmod\n",
      "opening next token: \", None, False, PUNCT, -35, punct\n",
      "opening next token: ., None, False, PUNCT, -1, punct\n",
      "opening next token:  , None, True, SPACE, -1, space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "d_i = 3; s_max = 2\n",
    "interpret_docs = list([docs[d_i][:s_max]])\n",
    "print(interpret_docs)\n",
    "model.interpret(interpret_docs, eval_layers = {0: {tag: train_layers[d_i][tag][:s_max] for tag in train_layers[d_i]}},\n",
    "                eval_covering = [covering[d_i][:s_max]], seed = seed)\n",
    "\n",
    "for doc in model._documents:\n",
    "    print('opening next doc:')\n",
    "    for s in doc._sentences:\n",
    "        print(f'opening next sent: {s._sty}')\n",
    "        for t in s._tokens:\n",
    "            print(f'opening next token: {t._form}, {t._lem}, {t._sep}, {t._pos}, {t._sup}, {t._dep}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' The prevalence of discrimination across racial groups in contemporary America: ']]\n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 325.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12192.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 575.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0's Sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 0's STY accuracy:  0.0\n",
      "Document 0's Token segmentation performance without space:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 0's POS accuracy with/out space: 0.7272727272727273 0.45454545454545453\n",
      "Document 0's SUP:DEP accuracy with/out space:  0.5 0.09090909090909091\n",
      "\n",
      "Overall sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Overall STY accuracy:  0.0\n",
      "Overall Token segmentation performance without space:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Overall POS accuracy with/out space: 0.7272727272727273 0.45454545454545453\n",
      "Overall SUP:DEP accuracy with/out space:  0.5 0.09090909090909091\n",
      "opening next doc:\n",
      "opening next sent: decl\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: The, None, False, PROPN, 0, root\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: prevalence, None, False, PROPN, -2, flat\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: of, None, False, ADP, 2, case\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: discrimination, None, False, PROPN, 2, nmod\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: across, None, False, NOUN, 2, amod\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: racial, None, False, ADJ, 4, amod\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: groups, None, False, NOUN, 2, nmod\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: in, None, False, ADP, 2, case\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: contemporary, None, False, NOUN, -16, nmod\n",
      "opening next token:  , None, False, SPACE, -1, space\n",
      "opening next token: America, None, False, NOUN, -15, dep\n",
      "opening next token: :, None, False, PUNCT, -1, punct\n",
      "opening next token:  , None, True, SPACE, -1, space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "d_i = 0; s_max = 1\n",
    "interpret_docs = list([tdocs[d_i][:s_max]])\n",
    "print(interpret_docs)\n",
    "model.interpret(interpret_docs, eval_layers = {0: {tag: test_layers[d_i][tag][:s_max] for tag in test_layers[d_i]}},\n",
    "                eval_covering = [tcovering[d_i][:s_max]], seed = seed)\n",
    "\n",
    "for doc in model._documents:\n",
    "    print('opening next doc:')\n",
    "    for s in doc._sentences:\n",
    "        print(f'opening next sent: {s._sty}')\n",
    "        for t in s._tokens:\n",
    "            print(f'opening next token: {t._form}, {t._lem}, {t._sep}, {t._pos}, {t._sup}, {t._dep}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' 2. GUJJOLAAY EEGIMAA, ITS SPEAKERS AND THEIR NEIGHBOURS ', ' This section briefly presents the Gújjolaay Eegimaa (Eegimaa for short; Ethnologue code: ISO 639-3: bqj), its speakers and its varieties. ']]\n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 194.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5159.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 95.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0's Sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 0's STY accuracy:  0.5\n",
      "Document 0's Token segmentation performance without space:  {('P', 0.5507246376811594), ('R', 0.76), ('F', 0.638655462184874)}\n",
      "Document 0's POS accuracy with/out space: 0.6666666666666666 0.3888888888888889\n",
      "Document 0's SUP:DEP accuracy with/out space:  0.3484848484848485 0.027777777777777776\n",
      "\n",
      "Overall sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Overall STY accuracy:  0.5\n",
      "Overall Token segmentation performance without space:  {('P', 0.5507246376811594), ('R', 0.76), ('F', 0.638655462184874)}\n",
      "Overall POS accuracy with/out space: 0.6666666666666666 0.3888888888888889\n",
      "Overall SUP:DEP accuracy with/out space:  0.3484848484848485 0.027777777777777776\n",
      "opening next doc:\n",
      "opening next sent: decl\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: 2, None, False, PROPN, 5, vocative\n",
      "opening next token: ., None, False, PROPN, 0, root\n",
      "opening next token:  , None, False, SPACE, -1, space\n",
      "opening next token: G, None, False, PROPN, 5, flat\n",
      "opening next token: U, None, False, PROPN, 7, dep\n",
      "opening next token: J, None, False, PUNCT, -1, nsubj\n",
      "opening next token: JO, None, False, VERB, 5, nsubj\n",
      "opening next token: LA, None, False, VERB, 5, dep\n",
      "opening next token: AY, None, False, VERB, 2, nsubj\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: EE, None, False, VERB, 2, amod\n",
      "opening next token: GI, None, False, VERB, 2, conj\n",
      "opening next token: MA, None, False, NOUN, -6, advmod\n",
      "opening next token: A, None, False, PUNCT, -11, punct\n",
      "opening next token: ,, None, False, PUNCT, -1, punct\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: ITS, None, False, VERB, -3, nsubj\n",
      "opening next token:  , None, False, SPACE, -1, space\n",
      "opening next token: S, None, False, VERB, -2, nsubj\n",
      "opening next token: P, None, False, VERB, -2, nsubj\n",
      "opening next token: EA, None, False, VERB, -7, conj\n",
      "opening next token: K, None, False, VERB, 2, conj\n",
      "opening next token: ER, None, False, NOUN, -2, obj\n",
      "opening next token: S, None, False, VERB, 2, nsubj\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: AND, None, False, VERB, 6, nsubj\n",
      "opening next token:  , None, False, SPACE, -1, space\n",
      "opening next token: THE, None, False, VERB, -4, amod\n",
      "opening next token: IR, None, False, VERB, 2, nsubj\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: NE, None, False, VERB, 2, amod\n",
      "opening next token: IG, None, False, VERB, -14, conj\n",
      "opening next token: HB, None, False, VERB, -6, conj\n",
      "opening next token: OU, None, False, VERB, -2, conj\n",
      "opening next token: RS, None, False, NOUN, -2, obj\n",
      "opening next token:  , None, True, SPACE, -1, space\n",
      "opening next sent: decl\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: This, None, False, ADP, 4, case\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: section, None, False, NOUN, 2, obj\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: briefly, None, False, NOUN, 4, compound\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: presents, None, False, NOUN, -2, case\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: the, None, False, DET, 4, det\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: G, None, False, ADJ, 2, amod\n",
      "opening next token: ú, None, False, NOUN, 2, amod\n",
      "opening next token: j, None, False, NOUN, 3, amod\n",
      "opening next token: jo, None, False, NOUN, 2, nsubj\n",
      "opening next token: la, None, False, NOUN, -6, obl\n",
      "opening next token: ay, None, False, VERB, 2, nsubj\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: Eegimaa, None, False, VERB, 2, amod\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: (, None, False, ADJ, 2, amod\n",
      "opening next token: Eegimaa, None, False, VERB, 2, amod\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: for, None, False, ADP, 6, case\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: short, None, False, NOUN, -6, obl\n",
      "opening next token: ;, None, False, PUNCT, -4, punct\n",
      "opening next token:  , None, False, SPACE, -1, space\n",
      "opening next token: E, None, False, NOUN, 2, obl\n",
      "opening next token: th, None, False, VERB, 2, advmod\n",
      "opening next token: no, None, False, VERB, -4, conj\n",
      "opening next token: logue, None, False, VERB, 0, root\n",
      "opening next token:  , None, False, SPACE, -1, space\n",
      "opening next token: code, None, False, NOUN, -2, obl\n",
      "opening next token: :, None, False, PUNCT, 4, punct\n",
      "opening next token:  , None, False, SPACE, -1, space\n",
      "opening next token: IS, None, False, PROPN, 5, nsubj\n",
      "opening next token: O, None, False, PROPN, 5, nsubj\n",
      "opening next token:  , None, False, SPACE, -1, space\n",
      "opening next token: 6, None, False, PART, 2, advmod\n",
      "opening next token: 39, None, False, PUNCT, 2, punct\n",
      "opening next token: -3, None, False, PROPN, 3, flat\n",
      "opening next token: :, None, False, PUNCT, -28, punct\n",
      "opening next token:  , None, False, SPACE, -1, space\n",
      "opening next token: b, None, False, PROPN, 2, amod\n",
      "opening next token: q, None, False, VERB, 2, amod\n",
      "opening next token: j, None, False, NOUN, -4, conj\n",
      "opening next token: ), None, False, PUNCT, -5, punct\n",
      "opening next token: ,, None, False, PUNCT, 6, punct\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: its, None, False, PRON, 2, nsubj\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: sp, None, False, ADJ, 2, amod\n",
      "opening next token: eakers, None, False, NOUN, 2, obj\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: and, None, False, CCONJ, 6, cc\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: its, None, False, PRON, -43, conj\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: varieties, None, False, PUNCT, -39, nmod\n",
      "opening next token: ., None, False, PUNCT, -42, punct\n",
      "opening next token:  , None, True, SPACE, -1, space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "d_i = 1; s_max = 2\n",
    "interpret_docs = list([tdocs[d_i][:s_max]])\n",
    "print(interpret_docs)\n",
    "model.interpret(interpret_docs, eval_layers = {0: {tag: test_layers[d_i][tag][:s_max] for tag in test_layers[d_i]}},\n",
    "                eval_covering = [tcovering[d_i][:s_max]], seed = seed)\n",
    "for doc in model._documents:\n",
    "    print('opening next doc:')\n",
    "    for s in doc._sentences:\n",
    "        print(f'opening next sent: {s._sty}')\n",
    "        for t in s._tokens:\n",
    "            print(f'opening next token: {t._form}, {t._lem}, {t._sep}, {t._pos}, {t._sup}, {t._dep}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' 2. GUJJOLAAY EEGIMAA, ITS SPEAKERS AND THEIR NEIGHBOURS ', ' This section briefly presents the Gújjolaay Eegimaa (Eegimaa for short; Ethnologue code: ISO 639-3: bqj), its speakers and its varieties. ']]\n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 218.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 762.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 115.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0's STY accuracy:  0.5\n",
      "Document 0's POS accuracy with/out space: 0.7121212121212122 0.4722222222222222\n",
      "Document 0's SUP:DEP accuracy with/out space:  0.36363636363636365 0.027777777777777776\n",
      "\n",
      "Overall STY accuracy:  0.5\n",
      "Overall POS accuracy with/out space: 0.7121212121212122 0.4722222222222222\n",
      "Overall SUP:DEP accuracy with/out space:  0.36363636363636365 0.027777777777777776\n",
      "opening next doc:\n",
      "opening next sent: decl\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: 2., None, False, PROPN, 0, root\n",
      "opening next token:  , None, False, SPACE, -1, space\n",
      "opening next token: GUJJOLAAY, None, False, PROPN, 2, nsubj\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: EEGIMAA, None, False, VERB, -4, punct\n",
      "opening next token: ,, None, False, PUNCT, -1, punct\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: ITS, None, False, VERB, -3, nsubj\n",
      "opening next token:  , None, False, SPACE, -1, space\n",
      "opening next token: SPEAKERS, None, False, VERB, -2, nsubj\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: AND, None, False, VERB, 2, nsubj\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: THEIR, None, False, VERB, -4, nsubj\n",
      "opening next token:  , None, False, SPACE, -1, space\n",
      "opening next token: NEIGHBOURS, None, False, VERB, -2, conj\n",
      "opening next token:  , None, True, SPACE, -1, space\n",
      "opening next sent: decl\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: This, None, False, ADP, 4, case\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: section, None, False, NOUN, 2, obj\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: briefly, None, False, NOUN, 4, compound\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: presents, None, False, NOUN, -2, case\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: the, None, False, DET, 6, det\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: Gújjolaay, None, False, NOUN, 2, amod\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: Eegimaa, None, False, VERB, 2, amod\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: (, None, False, ADJ, 2, amod\n",
      "opening next token: Eegimaa, None, False, VERB, 2, amod\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: for, None, False, ADP, 4, case\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: short, None, False, NOUN, -6, obl\n",
      "opening next token: ;, None, False, PUNCT, 2, punct\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: Ethnologue, None, False, VERB, 0, root\n",
      "opening next token:  , None, False, SPACE, -1, space\n",
      "opening next token: code, None, False, NOUN, -2, obl\n",
      "opening next token: :, None, False, PUNCT, 5, punct\n",
      "opening next token:  , None, False, SPACE, -1, space\n",
      "opening next token: ISO, None, False, PROPN, 5, nsubj\n",
      "opening next token:  , None, False, SPACE, -1, space\n",
      "opening next token: 639-3, None, False, PROPN, 2, punct\n",
      "opening next token: :, None, False, PUNCT, 4, punct\n",
      "opening next token:  , None, False, SPACE, -1, space\n",
      "opening next token: bqj, None, False, NOUN, 2, amod\n",
      "opening next token: ), None, False, PUNCT, -2, punct\n",
      "opening next token: ,, None, False, PUNCT, 6, punct\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: its, None, False, PRON, 4, nsubj\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: speakers, None, False, NOUN, 2, amod\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: and, None, False, CCONJ, 4, cc\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: its, None, False, PRON, -4, conj\n",
      "opening next token:  , None, False, SPACE, 1, space\n",
      "opening next token: varieties, None, False, PUNCT, -23, nmod\n",
      "opening next token: ., None, False, PUNCT, -42, punct\n",
      "opening next token:  , None, True, SPACE, -1, space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "d_i = 1; s_max = 2\n",
    "interpret_docs = list([tdocs[d_i][:s_max]])\n",
    "print(interpret_docs)\n",
    "model.interpret(interpret_docs, eval_layers = {0: {tag: test_layers[d_i][tag][:s_max] for tag in test_layers[d_i]}},\n",
    "                covering = [tcovering[d_i][:s_max]], seed = seed)\n",
    "for doc in model._documents:\n",
    "    print('opening next doc:')\n",
    "    for s in doc._sentences:\n",
    "        print(f'opening next sent: {s._sty}')\n",
    "        for t in s._tokens:\n",
    "            print(f'opening next token: {t._form}, {t._lem}, {t._sep}, {t._pos}, {t._sup}, {t._dep}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:01<00:00,  9.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 558.81it/s]\n",
      "100%|██████████| 18/18 [00:02<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1/18 [00:14<04:03, 14.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0's Sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 0's STY accuracy:  0.6851851851851852\n",
      "Document 0's Token segmentation performance without space:  {('P', 0.9112688553682342), ('R', 0.9227313566936208), ('F', 0.9169642857142857)}\n",
      "Document 0's POS accuracy with/out space: 0.7686116700201208 0.5642245480494766\n",
      "Document 0's SUP:DEP accuracy with/out space:  0.56841046277666 0.21027592768791628\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2/18 [00:32<04:26, 16.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1's Sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 1's STY accuracy:  0.8611111111111112\n",
      "Document 1's Token segmentation performance without space:  {('P', 0.7702371218315618), ('R', 0.8618481244281794), ('F', 0.8134715025906736)}\n",
      "Document 1's POS accuracy with/out space: 0.7435597189695551 0.5150166852057843\n",
      "Document 1's SUP:DEP accuracy with/out space:  0.5392271662763466 0.1557285873192436\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 3/18 [00:43<03:30, 14.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 2's Sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 2's STY accuracy:  0.9310344827586207\n",
      "Document 2's Token segmentation performance without space:  {('P', 0.8073394495412844), ('R', 0.8585365853658536), ('F', 0.8321513002364067)}\n",
      "Document 2's POS accuracy with/out space: 0.7543726235741445 0.5373563218390804\n",
      "Document 2's SUP:DEP accuracy with/out space:  0.5422053231939163 0.16810344827586207\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 4/18 [00:59<03:29, 14.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 3's Sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 3's STY accuracy:  0.9\n",
      "Document 3's Token segmentation performance without space:  {('P', 0.8631950573698146), ('R', 0.8874773139745916), ('F', 0.8751677852348994)}\n",
      "Document 3's POS accuracy with/out space: 0.7841880341880342 0.590030518819939\n",
      "Document 3's SUP:DEP accuracy with/out space:  0.5710470085470085 0.20142421159715157\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 5/18 [01:05<02:32, 11.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 4's Sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 4's STY accuracy:  0.46078431372549017\n",
      "Document 4's Token segmentation performance without space:  {('P', 0.9296875), ('R', 0.9133771929824561), ('F', 0.9214601769911503)}\n",
      "Document 4's POS accuracy with/out space: 0.7850985221674877 0.6\n",
      "Document 4's SUP:DEP accuracy with/out space:  0.6059113300492611 0.2755813953488372\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 6/18 [01:17<02:19, 11.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 5's Sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 5's STY accuracy:  0.7894736842105263\n",
      "Document 5's Token segmentation performance without space:  {('P', 0.857872340425532), ('R', 0.8865435356200527), ('F', 0.8719723183391004)}\n",
      "Document 5's POS accuracy with/out space: 0.7490755414685684 0.5335305719921104\n",
      "Document 5's SUP:DEP accuracy with/out space:  0.5467511885895404 0.19230769230769232\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 7/18 [01:32<02:19, 12.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 6's Sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 6's STY accuracy:  0.9148936170212766\n",
      "Document 6's Token segmentation performance without space:  {('P', 0.8862642169728784), ('R', 0.8964601769911504), ('F', 0.8913330400351956)}\n",
      "Document 6's POS accuracy with/out space: 0.75 0.5241379310344828\n",
      "Document 6's SUP:DEP accuracy with/out space:  0.5447530864197531 0.15369458128078817\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 8/18 [01:39<01:50, 11.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 7's Sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 7's STY accuracy:  0.5344827586206896\n",
      "Document 7's Token segmentation performance without space:  {('P', 0.8984547461368654), ('R', 0.9105145413870246), ('F', 0.9044444444444445)}\n",
      "Document 7's POS accuracy with/out space: 0.7786211258697027 0.5687732342007435\n",
      "Document 7's SUP:DEP accuracy with/out space:  0.592662871600253 0.22057001239157373\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 9/18 [01:52<01:43, 11.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 8's Sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 8's STY accuracy:  0.5952380952380952\n",
      "Document 8's Token segmentation performance without space:  {('P', 0.8648373983739838), ('R', 0.8986272439281943), ('F', 0.8814085965820818)}\n",
      "Document 8's POS accuracy with/out space: 0.7422934648581998 0.508274231678487\n",
      "Document 8's SUP:DEP accuracy with/out space:  0.5591861898890259 0.1773049645390071\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 10/18 [02:12<01:53, 14.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 9's Sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 9's STY accuracy:  0.72\n",
      "Document 9's Token segmentation performance without space:  {('P', 0.8479890933878664), ('R', 0.8693221523410203), ('F', 0.8585231193926847)}\n",
      "Document 9's POS accuracy with/out space: 0.7546699875466999 0.5347551342812006\n",
      "Document 9's SUP:DEP accuracy with/out space:  0.5466998754669987 0.15955766192733017\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 11/18 [02:20<01:27, 12.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 10's Sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 10's STY accuracy:  0.5428571428571428\n",
      "Document 10's Token segmentation performance without space:  {('P', 0.8435754189944135), ('R', 0.8640915593705293), ('F', 0.8537102473498234)}\n",
      "Document 10's POS accuracy with/out space: 0.7390939597315436 0.5056\n",
      "Document 10's SUP:DEP accuracy with/out space:  0.5385906040268457 0.1488\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 12/18 [02:34<01:15, 12.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 11's Sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 11's STY accuracy:  0.8181818181818182\n",
      "Document 11's Token segmentation performance without space:  {('P', 0.8804554079696395), ('R', 0.9106967615309126), ('F', 0.8953207911239749)}\n",
      "Document 11's POS accuracy with/out space: 0.7720670391061453 0.5562841530054645\n",
      "Document 11's SUP:DEP accuracy with/out space:  0.5754189944134078 0.1825136612021858\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 13/18 [02:46<01:02, 12.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 12's Sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 12's STY accuracy:  0.6545454545454545\n",
      "Document 12's Token segmentation performance without space:  {('P', 0.8933209647495362), ('R', 0.9084905660377358), ('F', 0.9008419083255378)}\n",
      "Document 12's POS accuracy with/out space: 0.7588075880758808 0.5462012320328542\n",
      "Document 12's SUP:DEP accuracy with/out space:  0.5382113821138211 0.15503080082135523\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 14/18 [02:56<00:47, 11.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 13's Sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 13's STY accuracy:  0.4418604651162791\n",
      "Document 13's Token segmentation performance without space:  {('P', 0.9328358208955224), ('R', 0.9171907756813418), ('F', 0.9249471458773786)}\n",
      "Document 13's POS accuracy with/out space: 0.751015670342426 0.5244444444444445\n",
      "Document 13's SUP:DEP accuracy with/out space:  0.546720835751596 0.15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 15/18 [03:17<00:43, 14.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 14's Sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 14's STY accuracy:  0.8918918918918919\n",
      "Document 14's Token segmentation performance without space:  {('P', 0.8849630238290879), ('R', 0.8952618453865336), ('F', 0.8900826446280992)}\n",
      "Document 14's POS accuracy with/out space: 0.7688995215311005 0.5624430264357339\n",
      "Document 14's SUP:DEP accuracy with/out space:  0.5464114832535886 0.15679124886052873\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 16/18 [03:24<00:24, 12.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 15's Sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 15's STY accuracy:  0.8421052631578947\n",
      "Document 15's Token segmentation performance without space:  {('P', 0.8428184281842819), ('R', 0.8735955056179775), ('F', 0.8579310344827586)}\n",
      "Document 15's POS accuracy with/out space: 0.7687296416938111 0.5456\n",
      "Document 15's SUP:DEP accuracy with/out space:  0.5749185667752443 0.1936\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 17/18 [03:32<00:10, 10.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 16's Sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 16's STY accuracy:  0.32558139534883723\n",
      "Document 16's Token segmentation performance without space:  {('P', 0.9100968188105117), ('R', 0.9228611500701263), ('F', 0.9164345403899722)}\n",
      "Document 16's POS accuracy with/out space: 0.774781919111816 0.5672333848531684\n",
      "Document 16's SUP:DEP accuracy with/out space:  0.5900079302141158 0.2241112828438949\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [03:41<00:00, 12.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 17's Sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Document 17's STY accuracy:  0.4153846153846154\n",
      "Document 17's Token segmentation performance without space:  {('P', 0.9175068744271311), ('R', 0.9242843951985226), ('F', 0.9208831646734131)}\n",
      "Document 17's POS accuracy with/out space: 0.7970118495620814 0.6044176706827309\n",
      "Document 17's SUP:DEP accuracy with/out space:  0.5744461617722824 0.1897590361445783\n",
      "\n",
      "Overall sentence segmentation performance:  {('F', 1.0), ('R', 1.0), ('P', 1.0)}\n",
      "Overall STY accuracy:  0.6588366890380313\n",
      "Overall Token segmentation performance without space:  {('P', 0.8734375845462908), ('R', 0.895683054045056), ('F', 0.8844204585924444)}\n",
      "Overall POS accuracy with/out space: 0.7636820730999807 0.5501356684755797\n",
      "Overall SUP:DEP accuracy with/out space:  0.5603687230065106 0.18296743956586087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.interpret(tdocs, eval_layers = test_layers, eval_covering = tcovering, seed = seed, verbose_result = False, decode_method = decode_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:01<00:00,  9.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 46.37it/s]\n",
      "100%|██████████| 18/18 [00:02<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1/18 [00:13<03:48, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0's STY accuracy:  0.6851851851851852\n",
      "Document 0's POS accuracy with/out space: 0.7892354124748491 0.6032350142721218\n",
      "Document 0's SUP:DEP accuracy with/out space:  0.5714285714285714 0.2131303520456708\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2/18 [00:25<03:26, 12.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1's STY accuracy:  0.8611111111111112\n",
      "Document 1's POS accuracy with/out space: 0.7640515222482436 0.5539488320355951\n",
      "Document 1's SUP:DEP accuracy with/out space:  0.5345433255269321 0.14015572858731926\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 3/18 [00:34<02:44, 10.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 2's STY accuracy:  0.9310344827586207\n",
      "Document 2's POS accuracy with/out space: 0.7749049429657795 0.5761494252873564\n",
      "Document 2's SUP:DEP accuracy with/out space:  0.5444866920152092 0.17672413793103448\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 4/18 [00:48<02:50, 12.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 3's STY accuracy:  0.9\n",
      "Document 3's POS accuracy with/out space: 0.8018162393162394 0.6236012207527976\n",
      "Document 3's SUP:DEP accuracy with/out space:  0.5737179487179487 0.20549338758901323\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 5/18 [00:54<02:08,  9.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 4's STY accuracy:  0.46078431372549017\n",
      "Document 4's POS accuracy with/out space: 0.8146551724137931 0.6558139534883721\n",
      "Document 4's SUP:DEP accuracy with/out space:  0.625615763546798 0.3127906976744186\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 6/18 [01:04<01:57,  9.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 5's STY accuracy:  0.7894736842105263\n",
      "Document 5's POS accuracy with/out space: 0.7844690966719493 0.5996055226824457\n",
      "Document 5's SUP:DEP accuracy with/out space:  0.5541468568409932 0.21005917159763313\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 7/18 [01:16<01:58, 10.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 6's STY accuracy:  0.9148936170212766\n",
      "Document 6's POS accuracy with/out space: 0.7844650205761317 0.5901477832512315\n",
      "Document 6's SUP:DEP accuracy with/out space:  0.5509259259259259 0.17142857142857143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 8/18 [01:23<01:34,  9.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 7's STY accuracy:  0.5344827586206896\n",
      "Document 7's POS accuracy with/out space: 0.7988614800759013 0.6084262701363073\n",
      "Document 7's SUP:DEP accuracy with/out space:  0.5989879822896901 0.23915737298636927\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 9/18 [01:34<01:28,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 8's STY accuracy:  0.5952380952380952\n",
      "Document 8's POS accuracy with/out space: 0.7651048088779285 0.5520094562647754\n",
      "Document 8's SUP:DEP accuracy with/out space:  0.5635018495684341 0.18557919621749408\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 10/18 [01:51<01:36, 12.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 9's STY accuracy:  0.72\n",
      "Document 9's POS accuracy with/out space: 0.7804068078040681 0.5837282780410743\n",
      "Document 9's SUP:DEP accuracy with/out space:  0.5541718555417185 0.17377567140600317\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 11/18 [01:58<01:14, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 10's STY accuracy:  0.5428571428571428\n",
      "Document 10's POS accuracy with/out space: 0.7709731543624161 0.5664\n",
      "Document 10's SUP:DEP accuracy with/out space:  0.5444630872483222 0.16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 12/18 [02:09<01:05, 10.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 11's STY accuracy:  0.8181818181818182\n",
      "Document 11's POS accuracy with/out space: 0.7905027932960894 0.5923497267759563\n",
      "Document 11's SUP:DEP accuracy with/out space:  0.5849162011173185 0.1989071038251366\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 13/18 [02:20<00:54, 10.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 12's STY accuracy:  0.6545454545454545\n",
      "Document 12's POS accuracy with/out space: 0.775609756097561 0.5780287474332649\n",
      "Document 12's SUP:DEP accuracy with/out space:  0.5398373983739837 0.15092402464065707\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 14/18 [02:30<00:42, 10.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 13's STY accuracy:  0.4418604651162791\n",
      "Document 13's POS accuracy with/out space: 0.7782936738247244 0.5766666666666667\n",
      "Document 13's SUP:DEP accuracy with/out space:  0.5536854323853744 0.16555555555555557\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 15/18 [02:48<00:38, 12.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 14's STY accuracy:  0.8918918918918919\n",
      "Document 14's POS accuracy with/out space: 0.7904306220095694 0.6034639927073838\n",
      "Document 14's SUP:DEP accuracy with/out space:  0.5550239234449761 0.1731996353691887\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 16/18 [02:55<00:21, 10.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 15's STY accuracy:  0.8421052631578947\n",
      "Document 15's POS accuracy with/out space: 0.8127035830618893 0.632\n",
      "Document 15's SUP:DEP accuracy with/out space:  0.5724755700325733 0.1888\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 17/18 [03:02<00:09,  9.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 16's STY accuracy:  0.32558139534883723\n",
      "Document 16's POS accuracy with/out space: 0.7930214115781126 0.6027820710973725\n",
      "Document 16's SUP:DEP accuracy with/out space:  0.591593973037272 0.2302936630602782\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [03:10<00:00, 10.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 17's STY accuracy:  0.4153846153846154\n",
      "Document 17's POS accuracy with/out space: 0.824317362184441 0.6576305220883534\n",
      "Document 17's SUP:DEP accuracy with/out space:  0.583204533745492 0.20983935742971888\n",
      "\n",
      "Overall STY accuracy:  0.6588366890380313\n",
      "Overall POS accuracy with/out space: 0.7885966608650808 0.5978046373951653\n",
      "Overall SUP:DEP accuracy with/out space:  0.5659124605169857 0.19394425259003453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.interpret(tdocs, eval_layers = test_layers, covering = tcovering, seed = seed, verbose_result = False, decode_method = decode_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.post_train(ptdocs, update = update, fine_tune = fine_tune, bits = bits)\n",
    "if fine_tune and ptdocs:\n",
    "    model.fine_tune(docs, covering = covering, all_layers = train_layers, streams = data_streams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if ptdocs:\n",
    "    model.interpret(tdocs, eval_layers = test_layers, eval_covering = tcovering, seed = seed, verbose_result = False, decode_method = decode_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if ptdocs:\n",
    "    model.interpret(tdocs, eval_layers = test_layers, covering = tcovering, seed = seed, verbose_result = False, decode_method = decode_method)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
