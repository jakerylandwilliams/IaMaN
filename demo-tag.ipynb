{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's a Machine and Natural: Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36/36 [00:00<00:00, 135421.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6515/6515 [00:01<00:00, 3765.12it/s]\n",
      "Fitting:   7%|████████▋                                                                                                                   | 7/100 [00:15<03:19,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built a vocabulary of 10984 types\n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [03:43<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lem-tags for whatever-layer prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:00<00:00, 192.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pos-tags for whatever-layer prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:00<00:00, 199.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting language model...\n",
      "Absorbing form-layer co-occurrences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:36<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:00<00:00, 305.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting all tag layers...\n",
      "Absorbing eot-layer co-occurrences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:14<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absorbing nov-layer co-occurrences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:14<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absorbing oov-layer co-occurrences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:15<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absorbing eos-layer co-occurrences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:15<00:00,  8.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absorbing eod-layer co-occurrences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:15<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absorbing lem-layer co-occurrences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:16<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absorbing pos-layer co-occurrences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:15<00:00,  8.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing marginal statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5689877/5689877 [00:28<00:00, 202820.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Model params, tokens, contexts, and % capacity used: 5689877 21922 442650 0.059\n"
     ]
    }
   ],
   "source": [
    "from src.IaMaN.base import LM\n",
    "from src.utils.data import load_gum\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "seed = 691\n",
    "all_docs = load_gum(num_articles = 0, seed = seed)\n",
    "nsamp = min(25, len(all_docs)-1) ## this is the size of the test document base..., so nsamp=125 => test == 125 & train == 25!\n",
    "test_docs = all_docs[:nsamp]\n",
    "train_docs = all_docs[nsamp:]\n",
    "docs = [[\"\".join([row[1] for row in s]) for s in d['conllu']] for d in train_docs]\n",
    "tdocs = [[\"\".join([row[1] for row in s]) for s in d['conllu']] for d in test_docs]\n",
    "covering = [[[row[1] for row in s] for s in d['conllu']] for d in train_docs]\n",
    "covering_vocab = set([t for d in covering for s in d for t in s])\n",
    "\n",
    "layers = [[[[row[2] for row in s] for s in d['conllu']] for d in train_docs],\n",
    "          [[[row[3] for row in s] for s in d['conllu']] for d in train_docs]]\n",
    "ltypes = ['lem','pos']\n",
    "\n",
    "model = LM(form2ind={\"<sos>\": 0, \"<pad>\": 1}, covering_vocab = covering_vocab)\n",
    "model.init(m = 10, zeta = 0.01, positional = True, seed = seed)\n",
    "model.fit(docs, f'GUM-{nsamp}', covering = covering, layers = layers, ltypes = ltypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['How to Prepare Quinoa ']]\n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 16710.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening next doc:\n",
      "opening next sent:\n",
      "opening next token:\n",
      "('How', 'SCONJ', 'how', False)\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False)\n",
      "opening next token:\n",
      "('to', 'PART', 'to', False)\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False)\n",
      "opening next token:\n",
      "('Prepare', 'VERB', 'prepare', False)\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False)\n",
      "opening next token:\n",
      "('Quinoa', 'DET', 'quinoa', False)\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "interpret_docs = list([docs[-2][:1]])\n",
    "print(interpret_docs)\n",
    "model.interpret(interpret_docs, seed = 691)\n",
    "for doc in model._documents:\n",
    "    print('opening next doc:')\n",
    "    for s in doc._sentences:\n",
    "        print('opening next sent:')\n",
    "        for t in s._tokens:\n",
    "            print('opening next token:')\n",
    "            print((str(t), t._pos, t._lem, t._sep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Insights from Eye-Tracking ']]\n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 33.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15768.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening next doc:\n",
      "opening next sent:\n",
      "opening next token:\n",
      "('Insights', 'NOUN', 'degree', False)\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False)\n",
      "opening next token:\n",
      "('from', 'ADP', 'from', False)\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False)\n",
      "opening next token:\n",
      "('Eye', 'PROPN', 'the', False)\n",
      "opening next token:\n",
      "('-', 'PUNCT', ' ', False)\n",
      "opening next token:\n",
      "('Tracking', 'PROPN', 'year', False)\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "interpret_docs = list([tdocs[0][1:2]])\n",
    "print(interpret_docs)\n",
    "model.interpret(interpret_docs, seed = 691)\n",
    "for doc in model._documents:\n",
    "    print('opening next doc:')\n",
    "    for s in doc._sentences:\n",
    "        print('opening next sent:')\n",
    "        for t in s._tokens:\n",
    "            print('opening next token:')\n",
    "            print((str(t), t._pos, t._lem, t._sep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Insights from Eye-Tracking ']]\n",
      "[[['Insights', ' ', 'from', ' ', 'Eye-Tracking', ' ']]]\n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 33.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14665.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Insights', ' ', 'from', ' ', 'Eye-Tracking', ' ']]]\n",
      "opening next doc:\n",
      "opening next sent:\n",
      "opening next token:\n",
      "('Insights', 'NOUN', 'degree', False, 'NOUN')\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False, 'PUNCT')\n",
      "opening next token:\n",
      "('from', 'ADP', 'from', False, 'ADP')\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False, 'PUNCT')\n",
      "opening next token:\n",
      "('Eye-Tracking', 'PUNCT', ' ', False, 'NOUN')\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', True, 'PUNCT')\n",
      "Tag-wise accuracy with/out space {'NOUN': 0.5, 'PUNCT': 1.0, 'ADP': 1.0} {'NOUN': 0.5, 'ADP': 1.0}\n",
      "Overall accuracy with/out space 0.8333333333333334 0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "interpret_docs = list([tdocs[0][1:2]])\n",
    "print(interpret_docs)\n",
    "interpret_covering =  [[[row[1] for row in s] for s in d['conllu']][1:2] for d in test_docs[0:1]]\n",
    "print(interpret_covering)\n",
    "model.interpret(interpret_docs, seed = 691, covering = interpret_covering)\n",
    "interpret_gold =  [[[row[3] for row in s] for s in d['conllu']][1:2] for d in test_docs[0:1]]\n",
    "print(interpret_covering)\n",
    "accuracy = defaultdict(list)\n",
    "accuracy_nsp = defaultdict(list)\n",
    "accuracy_all, accuracy_all_nsp, = [], []\n",
    "for doc_i, doc in enumerate(model._documents):\n",
    "    print('opening next doc:')\n",
    "    for sent_i, s in enumerate(doc._sentences):\n",
    "        print('opening next sent:')\n",
    "        for tok_i, t in enumerate(s._tokens):\n",
    "            accuracy[interpret_gold[doc_i][sent_i][tok_i]].append(interpret_gold[doc_i][sent_i][tok_i]==t._pos)\n",
    "            accuracy_all.append(interpret_gold[doc_i][sent_i][tok_i]==t._pos)\n",
    "            if str(t) != ' ':\n",
    "                accuracy_nsp[interpret_gold[doc_i][sent_i][tok_i]].append(interpret_gold[doc_i][sent_i][tok_i]==t._pos)\n",
    "                accuracy_all_nsp.append(interpret_gold[doc_i][sent_i][tok_i]==t._pos)\n",
    "            print('opening next token:')\n",
    "            print((str(t), t._pos, t._lem, t._sep, interpret_gold[doc_i][sent_i][tok_i]))\n",
    "print(\"Tag-wise accuracy with/out space\", {tag: sum(accuracy[tag])/len(accuracy[tag]) for tag in accuracy}, \n",
    "                                          {tag: sum(accuracy_nsp[tag])/len(accuracy_nsp[tag]) for tag in accuracy_nsp})\n",
    "print(\"Overall accuracy with/out space\", sum(accuracy_all)/len(accuracy_all), sum(accuracy_all_nsp)/len(accuracy_all_nsp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Distant Rhythm: Automatic Enjambment Detection on Four Centuries of Spanish Sonnets ']]\n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 28.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 9597.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:36<00:00, 36.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Distant', ' ', 'Rhythm', ':', ' ', 'Automatic', ' ', 'Enjambment', ' ', 'Detection', ' ', 'on', ' ', 'Four', ' ', 'Centuries', ' ', 'of', ' ', 'Spanish', ' ', 'Sonnets', ' ']]]\n",
      "opening next doc:\n",
      "opening next sent:\n",
      "opening next token:\n",
      "('Distant', 'NOUN', 'mean', False, 'ADJ')\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False, 'PUNCT')\n",
      "opening next token:\n",
      "('Rhythm', 'NOUN', ' ', False, 'NOUN')\n",
      "opening next token:\n",
      "(':', 'PUNCT', 'seem', False, 'PUNCT')\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False, 'PUNCT')\n",
      "opening next token:\n",
      "('Automatic', 'NOUN', ' ', False, 'ADJ')\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False, 'PUNCT')\n",
      "opening next token:\n",
      "('Enjambment', 'NOUN', ' ', False, 'NOUN')\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False, 'PUNCT')\n",
      "opening next token:\n",
      "('Detection', 'NOUN', ' ', False, 'NOUN')\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False, 'PUNCT')\n",
      "opening next token:\n",
      "('on', 'ADP', 'on', False, 'ADP')\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False, 'PUNCT')\n",
      "opening next token:\n",
      "('Four', 'PROPN', 'the', False, 'NUM')\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False, 'PUNCT')\n",
      "opening next token:\n",
      "('Centuries', 'PROPN', 'one', False, 'NOUN')\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False, 'PUNCT')\n",
      "opening next token:\n",
      "('of', 'ADP', 'of', False, 'ADP')\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False, 'PUNCT')\n",
      "opening next token:\n",
      "('Spanish', 'PROPN', 'the', False, 'ADJ')\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False, 'PUNCT')\n",
      "opening next token:\n",
      "('Sonnets', 'NOUN', ' ', False, 'NOUN')\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', True, 'PUNCT')\n",
      "Tag-wise accuracy with/out space {'ADJ': 0.0, 'PUNCT': 1.0, 'NOUN': 0.8, 'ADP': 1.0, 'NUM': 0.0} {'ADJ': 0.0, 'NOUN': 0.8, 'PUNCT': 1.0, 'ADP': 1.0, 'NUM': 0.0}\n",
      "Overall accuracy with/out space 0.782608695652174 0.5833333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "interpret_docs = list([tdocs[4][:1]])\n",
    "print(interpret_docs)\n",
    "interpret_covering =  [[[row[1] for row in s] for s in d['conllu']][:1] for d in test_docs[4:5]]\n",
    "model.interpret(interpret_docs, seed = 691, covering = interpret_covering)\n",
    "interpret_gold =  [[[row[3] for row in s] for s in d['conllu']][:1] for d in test_docs[4:5]]\n",
    "print(interpret_covering)\n",
    "accuracy = defaultdict(list)\n",
    "accuracy_nsp = defaultdict(list)\n",
    "accuracy_all, accuracy_all_nsp, = [], []\n",
    "for doc_i, doc in enumerate(model._documents):\n",
    "    print('opening next doc:')\n",
    "    for sent_i, s in enumerate(doc._sentences):\n",
    "        print('opening next sent:')\n",
    "        for tok_i, t in enumerate(s._tokens):\n",
    "            accuracy[interpret_gold[doc_i][sent_i][tok_i]].append(interpret_gold[doc_i][sent_i][tok_i]==t._pos)\n",
    "            accuracy_all.append(interpret_gold[doc_i][sent_i][tok_i]==t._pos)\n",
    "            if str(t) != ' ':\n",
    "                accuracy_nsp[interpret_gold[doc_i][sent_i][tok_i]].append(interpret_gold[doc_i][sent_i][tok_i]==t._pos)\n",
    "                accuracy_all_nsp.append(interpret_gold[doc_i][sent_i][tok_i]==t._pos)\n",
    "            print('opening next token:')\n",
    "            print((str(t), t._pos, t._lem, t._sep, interpret_gold[doc_i][sent_i][tok_i]))\n",
    "print(\"Tag-wise accuracy with/out space\", {tag: sum(accuracy[tag])/len(accuracy[tag]) for tag in accuracy}, \n",
    "                                          {tag: sum(accuracy_nsp[tag])/len(accuracy_nsp[tag]) for tag in accuracy_nsp})\n",
    "print(\"Overall accuracy with/out space\", sum(accuracy_all)/len(accuracy_all), sum(accuracy_all_nsp)/len(accuracy_all_nsp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vector dimension:  10984\n",
      "vector similarity of non-space to space whatevers (first two), \n",
      "vs. similarity of separate space tokens (last one): \n",
      "\n",
      " 0.996383687675028 0.9945622841576942 0.9997247105385825\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"model vector dimension: \", model._documents[0]._sentences[0]._tokens[0]._whatevers[0]._vec.shape[0])\n",
    "model._documents[0]._sentences[0]._tokens[0]._whatevers[0]._vec.shape[0]\n",
    "nspv = model._documents[0]._sentences[0]._tokens[0]._whatevers[-1]._vec; nspvn = np.linalg.norm(nspv)\n",
    "spv1 = model._documents[0]._sentences[0]._tokens[1]._whatevers[-1]._vec; spv1n = np.linalg.norm(spv1)\n",
    "spv2 = model._documents[0]._sentences[0]._tokens[4]._whatevers[-1]._vec; spv2n = np.linalg.norm(spv2)\n",
    "\n",
    "print(\"vector similarity of non-space to space whatevers (first two), \\nvs. similarity of separate space tokens (last one): \\n\\n\",\n",
    "      nspv.dot(spv1)/(nspvn*spv1n), nspv.dot(spv2)/(nspvn*spv2n), spv1.dot(spv2)/(spv1n*spv2n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:51<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 541.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lem-tags for whatever-layer prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 162.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:53<00:00,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy with/out space 0.8559518186112424 0.73962860529435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = defaultdict(list)\n",
    "accuracy_nsp = defaultdict(list)\n",
    "accuracy_all, accuracy_all_nsp, = [], []\n",
    "model.interpret(tdocs, seed = 691, covering = [[[row[1] for row in s] for s in d['conllu']] for d in test_docs], \n",
    "                ltypes = ['lem'],  layers = [[[[row[2] for row in s] for s in d['conllu']] for d in test_docs]], vecs = False)\n",
    "interpret_gold =  [[[row[3] for row in s] for s in d['conllu']] for d in test_docs]\n",
    "for doc_i, doc in enumerate(model._documents):\n",
    "    for sent_i, s in enumerate(doc._sentences):\n",
    "        for tok_i, t in enumerate(s._tokens):\n",
    "            accuracy[interpret_gold[doc_i][sent_i][tok_i]].append(interpret_gold[doc_i][sent_i][tok_i]==t._pos)\n",
    "            accuracy_all.append(interpret_gold[doc_i][sent_i][tok_i]==t._pos)\n",
    "            if str(t) != ' ':\n",
    "                accuracy_nsp[interpret_gold[doc_i][sent_i][tok_i]].append(interpret_gold[doc_i][sent_i][tok_i]==t._pos)\n",
    "                accuracy_all_nsp.append(interpret_gold[doc_i][sent_i][tok_i]==t._pos)\n",
    "\n",
    "print(\"Overall accuracy with/out space\", sum(accuracy_all)/len(accuracy_all), sum(accuracy_all_nsp)/len(accuracy_all_nsp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Tag-wise accuracy with space',\n",
       " [('PUNCT', (0.9667164310178273, 22774)),\n",
       "  ('ADP', (0.9113409678611009, 2707)),\n",
       "  ('NOUN', (0.8836094158674804, 4588)),\n",
       "  ('DET', (0.8683417085427135, 1990)),\n",
       "  ('CCONJ', (0.7891061452513967, 716)),\n",
       "  ('AUX', (0.7050632911392405, 790)),\n",
       "  ('VERB', (0.674937965260546, 2015)),\n",
       "  ('PRON', (0.6707589285714286, 896)),\n",
       "  ('NUM', (0.6593707250341997, 731)),\n",
       "  ('ADJ', (0.6036550542547116, 1751)),\n",
       "  ('PART', (0.5606796116504854, 412)),\n",
       "  ('ADV', (0.4393939393939394, 726)),\n",
       "  ('PROPN', (0.37850467289719625, 1712)),\n",
       "  ('SCONJ', (0.3643410852713178, 387)),\n",
       "  ('SYM', (0.18032786885245902, 61)),\n",
       "  ('INTJ', (0.09090909090909091, 11)),\n",
       "  ('X', (0.0410958904109589, 73))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Tag-wise accuracy with space\", list(Counter({tag: (sum(accuracy[tag])/len(accuracy[tag]), len(accuracy[tag])) for tag in accuracy}).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Tag-wise accuracy without space',\n",
       " [('ADP', (0.921209858103062, 2678)),\n",
       "  ('NOUN', (0.8898156277436348, 4556)),\n",
       "  ('DET', (0.874051593323217, 1977)),\n",
       "  ('CCONJ', (0.7980225988700564, 708)),\n",
       "  ('PUNCT', (0.7788004750593824, 3368)),\n",
       "  ('AUX', (0.7077509529860229, 787)),\n",
       "  ('VERB', (0.6783042394014963, 2005)),\n",
       "  ('PRON', (0.6745230078563412, 891)),\n",
       "  ('NUM', (0.6666666666666666, 723)),\n",
       "  ('ADJ', (0.6088709677419355, 1736)),\n",
       "  ('PART', (0.5661764705882353, 408)),\n",
       "  ('ADV', (0.44367176634214184, 719)),\n",
       "  ('PROPN', (0.38095238095238093, 1701)),\n",
       "  ('SCONJ', (0.36910994764397903, 382)),\n",
       "  ('SYM', (0.1896551724137931, 58)),\n",
       "  ('INTJ', (0.09090909090909091, 11)),\n",
       "  ('X', (0.04225352112676056, 71))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Tag-wise accuracy without space\", list(Counter({tag: (sum(accuracy_nsp[tag])/len(accuracy_nsp[tag]), len(accuracy_nsp[tag])) for tag in accuracy_nsp}).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
