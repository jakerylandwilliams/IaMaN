{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's a Machine and Natural Language Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gold-tagged UDs data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avail. pre-train, total pre-train, Avail. gold, total gold-train, total test-gold:  14198 0 150 132 18\n"
     ]
    }
   ],
   "source": [
    "from src.IaMaN.base import LM\n",
    "from src.utils.data import load_ud\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os, re\n",
    "\n",
    "seed = 691\n",
    "\n",
    "print(\"Loading pre-training data...\")\n",
    "pretrain_path = '/data/newstweet/week_2019-40_article_texts/'\n",
    "total_pretrain = len([pretrain_file for pretrain_file in os.listdir(pretrain_path) if re.search(\"^\\d+.txt$\", pretrain_file)])\n",
    "num_pretrain = 0\n",
    "\n",
    "all_pretrain_files = [pretrain_file for pretrain_file in os.listdir(pretrain_path) if re.search(\"^\\d+.txt$\", pretrain_file)]\n",
    "if num_pretrain:\n",
    "    np.random.seed(seed)\n",
    "    pretrain_files = np.random.choice(all_pretrain_files, size=num_pretrain, replace=False)\n",
    "else:\n",
    "    pretrain_files = np.array([])\n",
    "\n",
    "ptdocs = [[[open(pretrain_path+pretrain_file).read()]] for pretrain_file in tqdm(pretrain_files)]\n",
    "\n",
    "max_char = 200_000_000\n",
    "m = 10; space = True; fine_tune = False; fine_tune_post_pretrain = False\n",
    "positional = 'independent'; positionally_encode = 't'; do_ife = False; update_ife = False; update_bow = False; \n",
    "runners = 10; gpu = False\n",
    "\n",
    "print(\"Loading gold-tagged UDs data...\")\n",
    "load_set = 'GUM'; tokenizer = 'hr-bpe'\n",
    "all_docs = load_ud(\"English\", num_articles = 0, seed = 691, load_set = load_set, rebuild = True, space = space)\n",
    "test_docs = [doc for doc in all_docs if 'test' in doc['id'] and len(doc['text']) <= max_char]# [:1]\n",
    "train_docs = [doc for doc in all_docs if 'test' not in doc['id'] and len(doc['text']) <= max_char]# [:4]\n",
    "nsamp = len(test_docs)\n",
    "print('Avail. pre-train, total pre-train, Avail. gold, total gold-train, total test-gold: ', \n",
    "      total_pretrain, len(ptdocs), len(all_docs), len(train_docs), len(test_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing: 100%|██████████| 6503/6503 [00:01<00:00, 4471.64it/s]\n",
      "Fitting:  20%|██        | 20/100 [00:33<02:14,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built a vocabulary of 10606 types\n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [00:15<00:00,  8.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting documents and aggregating counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4758045it [05:29, 14435.65it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [00:06<00:00, 19.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [00:00<00:00, 160.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4758045/4758045 [00:43<00:00, 109826.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing marginal statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:06<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dense output heads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22 [00:00<?, ?it/s]/code/IaMaN/src/IaMaN/base.py:328: RuntimeWarning: invalid value encountered in log10\n",
      "  X[X==0] = self._noise; X /= X.sum(axis = 1)[:,None]; X = np.nan_to_num(-np.log10(X))\n",
      "100%|██████████| 22/22 [00:14<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting for transition matrices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [00:06<00:00, 19.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building transition matrices for Viterbi tag decoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 49.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Model params, types, encoding size, contexts, vec dim, max sent, and % capacity used: 507580 10607 10607 10607 10898 178 0.451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "docs = [[\"\".join([row[1] for row in s]) for s in d['conllu']] for d in train_docs]\n",
    "tdocs = [[\"\".join([row[1] for row in s]) for s in d['conllu']] for d in test_docs]\n",
    "covering = [[[row[1] for row in s] for s in d['conllu']] for d in train_docs]\n",
    "tcovering = [[[row[1] for row in s] for s in d['conllu']] for d in test_docs]\n",
    "covering_vocab = set([t for d in covering for s in d for t in s])\n",
    "\n",
    "all_layers = {d_i: {# 'lem': [[row[2] for row in s] for s in d['conllu']], \n",
    "                    # 'sty': [[d['s_type'][s_i] for row in s] for s_i, s in enumerate(d['conllu'])], \n",
    "                    'pos': [[row[3] for row in s] for s in d['conllu']], \n",
    "                    'sup': [[(str(int(row[6]) - int(row[0])) if int(row[6]) else row[6]) for row in s] for s in d['conllu']], \n",
    "                    'dep': [[row[7] for row in s] for s in d['conllu']]}\n",
    "              for d_i, d in enumerate(train_docs)}\n",
    "\n",
    "model = LM(m = m, tokenizer = tokenizer, noise = 0.001, seed = seed, space = space, positional = positional,\n",
    "           positionally_encode = positionally_encode, do_ife = do_ife, runners = runners, gpu = gpu)\n",
    "model.fit(docs, f'{load_set}-{nsamp}', covering = covering, all_layers = all_layers, \n",
    "          fine_tune = fine_tune)\n",
    "model.pre_train(ptdocs, update_ife = update_ife)\n",
    "if fine_tune_post_pretrain:\n",
    "    model.fine_tune(docs, covering = covering, all_layers = all_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Currently__: ordering for the current fine tuning process:\n",
    "1. train tokenizer and fit model to GUM\n",
    "2. process NewsTweet documents to integrate sparse post-training statistics (requires mr implementation and updates to the vocabularies/indices)\n",
    "3. update the ife and dense model, i.e., produce new statistics and dimensionalities\n",
    "4. fine tune output heads to GUM, and _combine_ them with the dense model from (3), i.e., don't just replace as is current.\n",
    "\n",
    "__Preliminarily__: this does seem to present performance benefits, but as is usual will require 'big data' statistics to become competitive. In particular, the (tokenization, least of all), counting, sorting, and aggregation of co-occurrence counts must all be distributed for the statistical resolution required to approach performance gains aking to more-advanced systems. Currently, a spark-based MR system is implemented for these (all but tokenization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' Emperor Norton ']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening next doc:\n",
      "opening next sent: None\n",
      "opening next token:  , False, SPACE, 0, root\n",
      "opening next token: Emperor, False, PROPN, 1, space\n",
      "opening next token:  , False, NOUN, 1, space\n",
      "opening next token: Nort, False, PUNCT, 1, space\n",
      "opening next token: o, False, SPACE, -4, space\n",
      "opening next token: n, False, PROPN, -2, space\n",
      "opening next token:  , True, SPACE, -2, space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "interpret_docs = list([docs[3][0:1]])\n",
    "print(interpret_docs)\n",
    "model.interpret(interpret_docs, seed = 691)\n",
    "for doc in model._documents:\n",
    "    print('opening next doc:')\n",
    "    for s in doc._sentences:\n",
    "        print(f'opening next sent: {s._sty}')\n",
    "        for t in s._tokens:\n",
    "            print(f'opening next token: {t._form}, {t._sep}, {t._pos}, {t._sup}, {t._dep}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' Results from a nationally representative sample of adults ']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening next doc:\n",
      "opening next sent: None\n",
      "opening next token:  Results from, False, SPACE, 1, space\n",
      "opening next token:  , False, NOUN, 0, root\n",
      "opening next token: a, False, NOUN, 1, space\n",
      "opening next token:  , False, SPACE, 1, space\n",
      "opening next token: n, False, NOUN, 1, space\n",
      "opening next token: a, False, NOUN, -4, space\n",
      "opening next token: t, False, NOUN, 1, space\n",
      "opening next token: io, False, NOUN, 1, space\n",
      "opening next token: n, False, NOUN, 1, space\n",
      "opening next token: ally, False, SPACE, 1, space\n",
      "opening next token:  , False, NOUN, 1, space\n",
      "opening next token: representative, False, NOUN, -7, space\n",
      "opening next token:  , False, SPACE, 1, space\n",
      "opening next token: sample, False, NOUN, 1, space\n",
      "opening next token:  , False, SPACE, 1, space\n",
      "opening next token: of, False, NOUN, -7, space\n",
      "opening next token:  , False, SPACE, 1, space\n",
      "opening next token: adults, False, NOUN, -7, space\n",
      "opening next token:  , True, SPACE, -4, space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "interpret_docs = list([tdocs[0][1:2]])\n",
    "print(interpret_docs)\n",
    "model.interpret(interpret_docs, seed = 691)\n",
    "for doc in model._documents:\n",
    "    print('opening next doc:')\n",
    "    for s in doc._sentences:\n",
    "        print(f'opening next sent: {s._sty}')\n",
    "        for t in s._tokens:\n",
    "            print(f'opening next token: {t._form}, {t._sep}, {t._pos}, {t._sup}, {t._dep}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' Results from a nationally representative sample of adults ']]\n",
      "[[[' ', 'Results', ' ', 'from', ' ', 'a', ' ', 'nationally', ' ', 'representative', ' ', 'sample', ' ', 'of', ' ', 'adults', ' ']]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag-wise POS accuracy with/out space {'SPACE': 0.7777777777777778, 'NOUN': 1.0, 'ADP': 0.0, 'DET': 0.0, 'ADV': 0.0, 'ADJ': 0.0} {'NOUN': 1.0, 'ADP': 0.0, 'DET': 0.0, 'ADV': 0.0, 'ADJ': 0.0}\n",
      "Overall POS accuracy with/out space 0.5882352941176471 0.375\n",
      "Overall SUP:DEP accuracy with/out space 0.35294117647058826 0.375\n",
      "Tag-wise accuracy []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "interpret_docs = list([tdocs[0][1:2]])\n",
    "print(interpret_docs)\n",
    "interpret_covering =  [[[row[1] for row in s] for s in d['conllu']][1:2] for d in test_docs[0:1]]\n",
    "print(interpret_covering)\n",
    "model.interpret(interpret_docs, seed = 691, covering = interpret_covering)\n",
    "\n",
    "accuracy = defaultdict(list)\n",
    "accuracy_nsp = defaultdict(list)\n",
    "accuracy_all, accuracy_all_nsp, = [], []\n",
    "sup_accuracy, sup_accuracy_nsp, = 0, 0\n",
    "accuracy_sty = defaultdict(list)\n",
    "accuracy_all_sty = []\n",
    "\n",
    "for d_i, doc in enumerate(model._documents):\n",
    "    for s_i, s in enumerate(doc._sentences):\n",
    "        if s._sty is not None:\n",
    "            result = s._sty == test_docs[0:1][d_i]['s_type'][s_i+1]\n",
    "            accuracy_sty[test_docs[0:1][d_i]['s_type'][s_i+1]].append(result)\n",
    "            accuracy_all_sty.append(result)\n",
    "\n",
    "pred_toks = [t._form for doc in model._documents for s in doc._sentences for t in s._tokens]\n",
    "pred_arcs = set([(ix, str(t._sup), t._dep) for doc in model._documents for s in doc._sentences for ix, t in enumerate(s._tokens)])\n",
    "pred_spans = list(np.cumsum([len(t) for t in pred_toks]))\n",
    "pred_stream = [t._pos for doc in model._documents for s in doc._sentences for t in s._tokens]\n",
    "pred_spans = {(sh-len(gt), sh): (gl, gt)\n",
    "              for sh, gt, gl in zip(pred_spans, pred_toks, pred_stream)}\n",
    "\n",
    "gold_toks = [row[1] for d in test_docs[:1] for s in d['conllu'][1:2] for row in s]\n",
    "gold_arcs = set([(ix, (str(int(row[6]) - int(row[0])) if int(row[6]) else row[6]),\n",
    "                  row[7]) for d in test_docs[:1] for s in d['conllu'][1:2] for ix, row in enumerate(s)])\n",
    "gold_spans = list(np.cumsum([len(t) for t in gold_toks]))\n",
    "gold_stream = [row[3] for d in test_docs[:1] for s in d['conllu'][1:2] for row in s]\n",
    "gold_spans = {(sh-len(gt), sh): (gl, gt)\n",
    "              for sh, gt, gl in zip(gold_spans, gold_toks, gold_stream)}\n",
    "\n",
    "for gold_span in gold_spans:\n",
    "    if gold_span in pred_spans:\n",
    "        result = gold_spans[gold_span] == pred_spans[gold_span]\n",
    "    else:\n",
    "        result = False\n",
    "    accuracy[gold_spans[gold_span][0]].append(result)\n",
    "    accuracy_all.append(result)\n",
    "    if gold_spans[gold_span][1] != ' ':\n",
    "        accuracy_nsp[gold_spans[gold_span][0]].append(result)\n",
    "        accuracy_all_nsp.append(result)\n",
    "        \n",
    "for ptok, parc in zip(pred_toks, pred_arcs):\n",
    "    if parc in gold_arcs:\n",
    "        sup_accuracy += 1\n",
    "        if ptok != ' ':\n",
    "            sup_accuracy_nsp += 1\n",
    "sup_accuracy /= len(pred_toks)\n",
    "sup_accuracy_nsp /= len([x for x in pred_toks if x != ' '])\n",
    "\n",
    "print(\"Tag-wise POS accuracy with/out space\", {tag: sum(accuracy[tag])/len(accuracy[tag]) for tag in accuracy}, \n",
    "                                          {tag: sum(accuracy_nsp[tag])/len(accuracy_nsp[tag]) for tag in accuracy_nsp})\n",
    "print(\"Overall POS accuracy with/out space\", sum(accuracy_all)/len(accuracy_all), sum(accuracy_all_nsp)/len(accuracy_all_nsp))\n",
    "print(\"Overall SUP:DEP accuracy with/out space\", sup_accuracy, sup_accuracy_nsp)\n",
    "if len(accuracy_all_sty):\n",
    "    print(\"Overall s_type accuracy: \", sum(accuracy_all_sty)/len(accuracy_all_sty))\n",
    "print(\"Tag-wise accuracy\", list(Counter({tag: (sum(accuracy_sty[tag])/len(accuracy_sty[tag]), len(accuracy_sty[tag])) \n",
    "                                         for tag in accuracy_sty}).most_common()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' The prevalence of discrimination across racial groups in contemporary America: ', ' Results from a nationally representative sample of adults ']]\n",
      "[[[' ', 'The', ' ', 'prevalence', ' ', 'of', ' ', 'discrimination', ' ', 'across', ' ', 'racial', ' ', 'groups', ' ', 'in', ' ', 'contemporary', ' ', 'America', ':', ' '], [' ', 'Results', ' ', 'from', ' ', 'a', ' ', 'nationally', ' ', 'representative', ' ', 'sample', ' ', 'of', ' ', 'adults', ' ']]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag-wise POS accuracy with/out space {'SPACE': 0.7, 'DET': 0.0, 'NOUN': 0.8333333333333334, 'ADP': 0.0, 'ADJ': 0.0, 'PROPN': 0.0, 'PUNCT': 0.0, 'ADV': 0.0} {'DET': 0.0, 'NOUN': 0.8333333333333334, 'ADP': 0.0, 'ADJ': 0.0, 'PROPN': 0.0, 'PUNCT': 0.0, 'ADV': 0.0}\n",
      "Overall POS accuracy with/out space 0.48717948717948717 0.2631578947368421\n",
      "Overall SUP:DEP accuracy with/out space 0.38461538461538464 0.47368421052631576\n",
      "Tag-wise accuracy []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "interpret_docs = list([tdocs[0][:2]])\n",
    "print(interpret_docs)\n",
    "interpret_covering =  [[[row[1] for row in s] for s in d['conllu']][:2] for d in test_docs[0:1]]\n",
    "print(interpret_covering)\n",
    "model.interpret(interpret_docs, seed = 691, covering = interpret_covering)\n",
    "\n",
    "accuracy = defaultdict(list)\n",
    "accuracy_nsp = defaultdict(list)\n",
    "accuracy_all, accuracy_all_nsp, = [], []\n",
    "sup_accuracy, sup_accuracy_nsp, = 0, 0\n",
    "accuracy_sty = defaultdict(list)\n",
    "accuracy_all_sty = []\n",
    "\n",
    "for d_i, doc in enumerate(model._documents):\n",
    "    for s_i, s in enumerate(doc._sentences):\n",
    "        if s._sty is not None:\n",
    "            result = s._sty == test_docs[0:1][d_i]['s_type'][s_i]\n",
    "            accuracy_sty[test_docs[0:1][d_i]['s_type'][s_i]].append(result)\n",
    "            accuracy_all_sty.append(result)\n",
    "\n",
    "pred_toks = [t._form for doc in model._documents for s in doc._sentences for t in s._tokens]\n",
    "pred_arcs = set([(ix, str(t._sup), t._dep, s_i, d_i) for d_i, doc in enumerate(model._documents) for s_i, s in enumerate(doc._sentences) for ix, t in enumerate(s._tokens)])\n",
    "pred_spans = list(np.cumsum([len(t) for t in pred_toks]))\n",
    "pred_stream = [t._pos for doc in model._documents for s in doc._sentences for t in s._tokens]\n",
    "pred_spans = {(sh-len(gt), sh): (gl, gt)\n",
    "              for sh, gt, gl in zip(pred_spans, pred_toks, pred_stream)}\n",
    "\n",
    "gold_toks = [row[1] for d in test_docs[:1] for s in d['conllu'][:2] for row in s]\n",
    "gold_arcs = set([(ix, (str(int(row[6]) - int(row[0])) if int(row[6]) else row[6]), row[7], s_i, d_i) \n",
    "                 for d_i, d in enumerate(test_docs[:1]) for s_i, s in enumerate(d['conllu'][:2]) for ix, row in enumerate(s)])\n",
    "gold_spans = list(np.cumsum([len(t) for t in gold_toks]))\n",
    "gold_stream = [row[3] for d in test_docs[:1] for s in d['conllu'][:2] for row in s]\n",
    "gold_spans = {(sh-len(gt), sh): (gl, gt)\n",
    "              for sh, gt, gl in zip(gold_spans, gold_toks, gold_stream)}\n",
    "\n",
    "for gold_span in gold_spans:\n",
    "    if gold_span in pred_spans:\n",
    "        result = gold_spans[gold_span] == pred_spans[gold_span]\n",
    "    else:\n",
    "        result = False\n",
    "    accuracy[gold_spans[gold_span][0]].append(result)\n",
    "    accuracy_all.append(result)\n",
    "    if gold_spans[gold_span][1] != ' ':\n",
    "        accuracy_nsp[gold_spans[gold_span][0]].append(result)\n",
    "        accuracy_all_nsp.append(result)\n",
    "        \n",
    "for ptok, parc in zip(pred_toks, pred_arcs):\n",
    "    if parc in gold_arcs:\n",
    "        sup_accuracy += 1\n",
    "        if ptok != ' ':\n",
    "            sup_accuracy_nsp += 1\n",
    "sup_accuracy /= len(pred_toks)\n",
    "sup_accuracy_nsp /= len([x for x in pred_toks if x != ' '])\n",
    "\n",
    "print(\"Tag-wise POS accuracy with/out space\", {tag: sum(accuracy[tag])/len(accuracy[tag]) for tag in accuracy}, \n",
    "                                          {tag: sum(accuracy_nsp[tag])/len(accuracy_nsp[tag]) for tag in accuracy_nsp})\n",
    "print(\"Overall POS accuracy with/out space\", sum(accuracy_all)/len(accuracy_all), sum(accuracy_all_nsp)/len(accuracy_all_nsp))\n",
    "print(\"Overall SUP:DEP accuracy with/out space\", sup_accuracy, sup_accuracy_nsp)\n",
    "if len(accuracy_all_sty):\n",
    "    print(\"Overall s_type accuracy: \", sum(accuracy_all_sty)/len(accuracy_all_sty))\n",
    "print(\"Tag-wise accuracy\", list(Counter({tag: (sum(accuracy_sty[tag])/len(accuracy_sty[tag]), len(accuracy_sty[tag])) \n",
    "                                         for tag in accuracy_sty}).most_common()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [32:58<00:00, 109.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token segmentation performance with/out space Counter({'TP': 20687, 'FP': 12487, 'FN': 10339, 'R': 0.667, 'F': 0.645, 'P': 0.624}) Counter({'TP': 13418, 'FP': 8065, 'FN': 2798, 'R': 0.827, 'F': 0.712, 'P': 0.625})\n",
      "Overall POS accuracy with/out space 0.1950944369238703 0.09176122348297977\n",
      "Overall SUP:DEP accuracy with/out space 0.18776752878760475 0.1894521249359959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Tag-wise accuracy',\n",
       " [],\n",
       " [('SPACE', (0.30823767724510465, 14810)),\n",
       "  ('INTJ', (0.29545454545454547, 88)),\n",
       "  ('NOUN', (0.18882531134298217, 2971)),\n",
       "  ('PUNCT', (0.1878640776699029, 2060)),\n",
       "  ('NUM', (0.13994169096209913, 343)),\n",
       "  ('PROPN', (0.1335478680611424, 1243)),\n",
       "  ('PRON', (0.07613741875580315, 1077)),\n",
       "  ('AUX', (0.036061026352288486, 721)),\n",
       "  ('ADV', (0.03588907014681892, 613)),\n",
       "  ('VERB', (0.032432432432432434, 1665)),\n",
       "  ('ADP', (0.029359953024075163, 1703)),\n",
       "  ('DET', (0.02145922746781116, 1398)),\n",
       "  ('CCONJ', (0.01870748299319728, 588)),\n",
       "  ('ADJ', (0.015370705244122965, 1106)),\n",
       "  ('PART', (0.014925373134328358, 335)),\n",
       "  ('SCONJ', (0.012295081967213115, 244)),\n",
       "  ('SYM', (0.0, 35)),\n",
       "  ('X', (0.0, 26))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "confusion = Counter()\n",
    "confusion_nsp = Counter()\n",
    "accuracy = defaultdict(list)\n",
    "accuracy_nsp = defaultdict(list)\n",
    "accuracy_all, accuracy_all_nsp, = [], []\n",
    "sup_accuracy, sup_accuracy_nsp, = 0, 0\n",
    "accuracy_sty = defaultdict(list)\n",
    "accuracy_all_sty = []\n",
    "\n",
    "model.interpret(tdocs, seed = 691) \n",
    "\n",
    "pred_toks = [t._form for doc in model._documents for s in doc._sentences for t in s._tokens]\n",
    "pred_spans = list(np.cumsum([len(t) for t in pred_toks]))\n",
    "pred_spans = set([(sh-len(gt), sh) for sh, gt in zip(pred_spans, pred_toks)])\n",
    "\n",
    "gold_toks = [row[1] for d in test_docs for s in d['conllu'] for row in s]\n",
    "gold_spans = list(np.cumsum([len(t) for t in gold_toks]))\n",
    "gold_spans = set([(sh-len(gt), sh) for sh, gt in zip(gold_spans, gold_toks)])\n",
    "\n",
    "for pred_span, pred_tok in zip(pred_spans, pred_toks):\n",
    "    if pred_span in gold_spans:\n",
    "        confusion['TP'] += 1\n",
    "        if pred_tok != ' ':\n",
    "            confusion_nsp['TP'] += 1\n",
    "    else:\n",
    "        confusion['FP'] += 1\n",
    "        if pred_tok != ' ':\n",
    "            confusion_nsp['FP'] += 1\n",
    "confusion['FN'] = len(gold_spans) - confusion['TP']\n",
    "confusion_nsp['FN'] = len([t for t in gold_toks if t != ' ']) - confusion_nsp['TP']\n",
    "\n",
    "confusion['P'] = round(confusion['TP']/(confusion['TP'] + confusion['FP']), 3)\n",
    "confusion['R'] = round(confusion['TP']/(confusion['TP'] + confusion['FN']), 3)\n",
    "confusion['F'] = round(2*confusion['P']*confusion['R']/(confusion['P']+confusion[\"R\"]), 3)\n",
    "confusion_nsp['P'] = round(confusion_nsp['TP']/(confusion_nsp['TP'] + confusion_nsp['FP']), 3)\n",
    "confusion_nsp['R'] = round(confusion_nsp['TP']/(confusion_nsp['TP'] + confusion_nsp['FN']), 3)\n",
    "confusion_nsp['F'] = round(2*confusion_nsp['P']*confusion_nsp['R']/(confusion_nsp['P']+confusion_nsp[\"R\"]), 3)\n",
    "\n",
    "for d_i, doc in enumerate(model._documents):\n",
    "    for s_i, s in enumerate(doc._sentences):\n",
    "        if s._sty is not None:\n",
    "            result = s._sty == test_docs[d_i]['s_type'][s_i]\n",
    "            accuracy_sty[test_docs[d_i]['s_type'][s_i]].append(result)\n",
    "            accuracy_all_sty.append(result)\n",
    "\n",
    "pred_toks = [t._form for doc in model._documents for s in doc._sentences for t in s._tokens]\n",
    "pred_arcs = set([(ix, str(t._sup), t._dep, d_i, s_i) for d_i, doc in enumerate(model._documents) \n",
    "                 for s_i, s in enumerate(doc._sentences) for ix, t in enumerate(s._tokens)])\n",
    "pred_spans = list(np.cumsum([len(t) for t in pred_toks]))\n",
    "pred_stream = [t._pos for doc in model._documents for s in doc._sentences for t in s._tokens]\n",
    "pred_spans = {(sh-len(gt), sh): (gl, gt)\n",
    "              for sh, gt, gl in zip(pred_spans, pred_toks, pred_stream)}\n",
    "\n",
    "gold_toks = [row[1] for d in test_docs for s in d['conllu'] for row in s]\n",
    "gold_arcs = set([(ix, (str(int(row[6]) - int(row[0])) if int(row[6]) else row[6]), row[7], d_i, s_i) \n",
    "                 for d_i, d in enumerate(test_docs) for s_i, s in enumerate(d['conllu']) for ix, row in enumerate(s)])\n",
    "gold_spans = list(np.cumsum([len(t) for t in gold_toks]))\n",
    "gold_stream = [row[3] for d in test_docs for s in d['conllu'] for row in s]\n",
    "gold_spans = {(sh-len(gt), sh): (gl, gt)\n",
    "              for sh, gt, gl in zip(gold_spans, gold_toks, gold_stream)}\n",
    "\n",
    "for gold_span in gold_spans:\n",
    "    if gold_span in pred_spans:\n",
    "        result = gold_spans[gold_span] == pred_spans[gold_span]\n",
    "    else:\n",
    "        result = False\n",
    "    accuracy[gold_spans[gold_span][0]].append(result)\n",
    "    accuracy_all.append(result)\n",
    "    if gold_spans[gold_span][1] != ' ':\n",
    "        accuracy_nsp[gold_spans[gold_span][0]].append(result)\n",
    "        accuracy_all_nsp.append(result)\n",
    "        \n",
    "for ptok, parc in zip(pred_toks, pred_arcs):\n",
    "    if parc in gold_arcs:\n",
    "        sup_accuracy += 1\n",
    "        if ptok != ' ':\n",
    "            sup_accuracy_nsp += 1\n",
    "sup_accuracy /= len(pred_toks)\n",
    "sup_accuracy_nsp /= len([x for x in pred_toks if x != ' '])\n",
    "\n",
    "print(\"Token segmentation performance with/out space\", confusion, confusion_nsp)\n",
    "print(\"Overall POS accuracy with/out space\", sum(accuracy_all)/len(accuracy_all), sum(accuracy_all_nsp)/len(accuracy_all_nsp))\n",
    "print(\"Overall SUP:DEP accuracy with/out space\", sup_accuracy, sup_accuracy_nsp)\n",
    "if len(accuracy_all_sty):\n",
    "    print(\"Overall s_type accuracy: \", sum(accuracy_all_sty)/len(accuracy_all_sty))\n",
    "\"Tag-wise accuracy\", list(Counter({tag: (sum(accuracy_sty[tag])/len(accuracy_sty[tag]), len(accuracy_sty[tag])) \n",
    "                                   for tag in accuracy_sty}).most_common()), list(Counter({tag: (sum(accuracy[tag])/len(accuracy[tag]), len(accuracy[tag])) \n",
    "                                                                                           for tag in accuracy}).most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "--- start small training/test\n",
    "--- ife, sparse, uniform-positional encoding\n",
    "100%|██████████| 1/1 [00:18<00:00, 18.28s/it]\n",
    "\n",
    "Token segmentation performance with/out space \n",
    "Counter({'TP': 1660, 'FP': 874, 'FN': 328, 'R': 0.835, 'F': 0.734, 'P': 0.655}) \n",
    "Counter({'TP': 1024, 'FP': 573, 'FN': 27, 'R': 0.974, 'F': 0.773, 'P': 0.641})\n",
    "Overall POS accuracy with/out space 0.6403420523138833 0.3824928639391056\n",
    "Overall SUP:DEP accuracy with/out space 0.19179163378058406 0.1959924859110833\n",
    "Overall s_type accuracy:  0.7037037037037037\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (1.0, 37)), ('frag', (0.0625, 16)), ('wh', (0.0, 1))],\n",
    " [('SPACE', (0.9295624332977588, 937)),\n",
    "  ('PUNCT', (0.68125, 160)),\n",
    "  ('DET', (0.5876288659793815, 97)),\n",
    "  ('ADP', (0.5765765765765766, 111)),\n",
    "  ('CCONJ', (0.56, 25)),\n",
    "  ('NOUN', (0.3884297520661157, 242)),\n",
    "  ('NUM', (0.32558139534883723, 43)),\n",
    "  ('AUX', (0.2682926829268293, 41)),\n",
    "  ('VERB', (0.26548672566371684, 113)),\n",
    "  ('PRON', (0.22727272727272727, 22)),\n",
    "  ('PROPN', (0.09375, 32)),\n",
    "  ('ADV', (0.02564102564102564, 39)),\n",
    "  ('ADJ', (0.0, 83)),\n",
    "  ('SCONJ', (0.0, 16)),\n",
    "  ('PART', (0.0, 13)),\n",
    "  ('SYM', (0.0, 12)),\n",
    "  ('X', (0.0, 2))])\n",
    "--- ife, sparse, token-positional encoding\n",
    "100%|██████████| 1/1 [00:20<00:00, 20.01s/it]\n",
    "\n",
    "Token segmentation performance with/out space \n",
    "Counter({'TP': 1658, 'FP': 876, 'FN': 330, 'R': 0.834, 'F': 0.733, 'P': 0.654}) \n",
    "Counter({'TP': 1047, 'FP': 550, 'FN': 4, 'R': 0.996, 'F': 0.791, 'P': 0.656})\n",
    "Overall POS accuracy with/out space 0.6740442655935613 0.3939105613701237\n",
    "Overall SUP:DEP accuracy with/out space 0.1910023677979479 0.19724483406386975\n",
    "Overall s_type accuracy:  0.7037037037037037\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (1.0, 37)), ('frag', (0.0625, 16)), ('wh', (0.0, 1))],\n",
    " [('SPACE', (0.9882604055496265, 937)),\n",
    "  ('PUNCT', (0.70625, 160)),\n",
    "  ('DET', (0.6288659793814433, 97)),\n",
    "  ('ADP', (0.6126126126126126, 111)),\n",
    "  ('CCONJ', (0.6, 25)),\n",
    "  ('NOUN', (0.384297520661157, 242)),\n",
    "  ('NUM', (0.32558139534883723, 43)),\n",
    "  ('AUX', (0.2682926829268293, 41)),\n",
    "  ('VERB', (0.26548672566371684, 113)),\n",
    "  ('PRON', (0.22727272727272727, 22)),\n",
    "  ('PROPN', (0.09375, 32)),\n",
    "  ('ADV', (0.02564102564102564, 39)),\n",
    "  ('ADJ', (0.0, 83)),\n",
    "  ('SCONJ', (0.0, 16)),\n",
    "  ('PART', (0.0, 13)),\n",
    "  ('SYM', (0.0, 12)),\n",
    "  ('X', (0.0, 2))])\n",
    "--- ife, fine tuned, uniform-positional encoding\n",
    "100%|██████████| 1/1 [00:19<00:00, 19.83s/it]\n",
    "\n",
    "Token segmentation performance with/out space \n",
    "Counter({'TP': 1611, 'FP': 1089, 'FN': 377, 'R': 0.81, 'F': 0.687, 'P': 0.597}) \n",
    "Counter({'TP': 1055, 'FP': 716, 'R': 1.004, 'F': 0.748, 'P': 0.596, 'FN': -4})\n",
    "Overall POS accuracy with/out space 0.619215291750503 0.3368220742150333\n",
    "Overall SUP:DEP accuracy with/out space 0.1925925925925926 0.19875776397515527\n",
    "Overall s_type accuracy:  0.6851851851851852\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (1.0, 37)), ('frag', (0.0, 16)), ('wh', (0.0, 1))],\n",
    " [('SPACE', (0.935965848452508, 937)),\n",
    "  ('DET', (0.6082474226804123, 97)),\n",
    "  ('PUNCT', (0.6, 160)),\n",
    "  ('ADP', (0.5945945945945946, 111)),\n",
    "  ('CCONJ', (0.52, 25)),\n",
    "  ('AUX', (0.36585365853658536, 41)),\n",
    "  ('NUM', (0.27906976744186046, 43)),\n",
    "  ('NOUN', (0.256198347107438, 242)),\n",
    "  ('PART', (0.23076923076923078, 13)),\n",
    "  ('VERB', (0.168141592920354, 113)),\n",
    "  ('PRON', (0.09090909090909091, 22)),\n",
    "  ('PROPN', (0.0625, 32)),\n",
    "  ('ADJ', (0.04819277108433735, 83)),\n",
    "  ('ADV', (0.02564102564102564, 39)),\n",
    "  ('SCONJ', (0.0, 16)),\n",
    "  ('SYM', (0.0, 12)),\n",
    "  ('X', (0.0, 2))])\n",
    "--- ife, fine tuned, token-positional encoding\n",
    "100%|██████████| 1/1 [00:22<00:00, 22.50s/it]\n",
    "\n",
    "Token segmentation performance with/out space \n",
    "Counter({'TP': 1666, 'FP': 870, 'FN': 322, 'R': 0.838, 'F': 0.737, 'P': 0.657}) \n",
    "Counter({'TP': 1043, 'FP': 556, 'FN': 8, 'R': 0.992, 'F': 0.787, 'P': 0.652})\n",
    "Overall POS accuracy with/out space 0.6745472837022133 0.384395813510942\n",
    "Overall SUP:DEP accuracy with/out space 0.16876971608832808 0.17886178861788618\n",
    "Overall s_type accuracy:  0.6851851851851852\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (1.0, 37)), ('frag', (0.0, 16)), ('wh', (0.0, 1))],\n",
    " [('SPACE', (1.0, 937)),\n",
    "  ('PUNCT', (0.74375, 160)),\n",
    "  ('DET', (0.6597938144329897, 97)),\n",
    "  ('CCONJ', (0.6, 25)),\n",
    "  ('ADP', (0.5945945945945946, 111)),\n",
    "  ('AUX', (0.34146341463414637, 41)),\n",
    "  ('NOUN', (0.28512396694214875, 242)),\n",
    "  ('VERB', (0.2831858407079646, 113)),\n",
    "  ('NUM', (0.27906976744186046, 43)),\n",
    "  ('PRON', (0.18181818181818182, 22)),\n",
    "  ('PART', (0.15384615384615385, 13)),\n",
    "  ('PROPN', (0.0625, 32)),\n",
    "  ('ADJ', (0.04819277108433735, 83)),\n",
    "  ('ADV', (0.02564102564102564, 39)),\n",
    "  ('SCONJ', (0.0, 16)),\n",
    "  ('SYM', (0.0, 12)),\n",
    "  ('X', (0.0, 2))])\n",
    "--- form, sparse, uniform-positional encoding\n",
    "Token segmentation performance with/out space: \n",
    "Counter({'TP': 1567, 'FP': 1035, 'FN': 367, 'R': 0.81, 'F': 0.691, 'P': 0.602}) \n",
    "Counter({'TP': 1013, 'FP': 706, 'FN': 38, 'R': 0.964, 'F': 0.731, 'P': 0.589})\n",
    "Overall POS accuracy with/out space 0.655635987590486 0.36631779257849667\n",
    "Overall SUP:DEP accuracy with/out space 0.1848578016910069 0.1878999418266434\n",
    "Overall s_type accuracy:  0.6851851851851852\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (1.0, 37)), ('frag', (0.0, 16)), ('wh', (0.0, 1))],\n",
    " [('PUNCT', (0.9674017257909875, 1043)),\n",
    "  ('ADP', (0.6846846846846847, 111)),\n",
    "  ('DET', (0.5463917525773195, 97)),\n",
    "  ('NUM', (0.37209302325581395, 43)),\n",
    "  ('NOUN', (0.28512396694214875, 242)),\n",
    "  ('PART', (0.23076923076923078, 13)),\n",
    "  ('VERB', (0.23008849557522124, 113)),\n",
    "  ('CCONJ', (0.2, 25)),\n",
    "  ('PRON', (0.13636363636363635, 22)),\n",
    "  ('PROPN', (0.0625, 32)),\n",
    "  ('ADV', (0.05128205128205128, 39)),\n",
    "  ('AUX', (0.04878048780487805, 41)),\n",
    "  ('ADJ', (0.024096385542168676, 83)),\n",
    "  ('SCONJ', (0.0, 16)),\n",
    "  ('SYM', (0.0, 12)),\n",
    "  ('X', (0.0, 2))])\n",
    "--- end small training/test\n",
    "--- start full training/test\n",
    "--- ife, sparse, uniform-positional encoding\n",
    "100%|██████████| 18/18 [43:41<00:00, 145.63s/it]\n",
    "\n",
    "Token segmentation performance with/out space \n",
    "Counter({'TP': 28408, 'FP': 3461, 'FN': 1724, 'R': 0.943, 'F': 0.916, 'P': 0.891}) \n",
    "Counter({'TP': 15983, 'FP': 1974, 'FN': 233, 'R': 0.986, 'F': 0.936, 'P': 0.89})\n",
    "Overall POS accuracy with/out space 0.7840501792114696 0.6013196842624569\n",
    "Overall SUP:DEP accuracy with/out space 0.34704571840973986 0.3442111711310353\n",
    "Overall s_type accuracy:  0.7237136465324385\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (0.966044142614601, 589)),\n",
    "  ('intj', (0.5769230769230769, 26)),\n",
    "  ('q', (0.5, 16)),\n",
    "  ('inf', (0.4, 5)),\n",
    "  ('wh', (0.38095238095238093, 21)),\n",
    "  ('frag', (0.3225806451612903, 93)),\n",
    "  ('imp', (0.20408163265306123, 49)),\n",
    "  ('sub', (0.12195121951219512, 41)),\n",
    "  ('multiple', (0.0, 32)),\n",
    "  ('other', (0.0, 14)),\n",
    "  ('ger', (0.0, 8))],\n",
    " [('SPACE', (0.9969818913480886, 13916)),\n",
    "  ('PUNCT', (0.8689320388349514, 2060)),\n",
    "  ('DET', (0.8190271816881259, 1398)),\n",
    "  ('ADP', (0.7428068115091015, 1703)),\n",
    "  ('NOUN', (0.7381353079771121, 2971)),\n",
    "  ('CCONJ', (0.7380952380952381, 588)),\n",
    "  ('PRON', (0.6666666666666666, 1077)),\n",
    "  ('SCONJ', (0.6024590163934426, 244)),\n",
    "  ('AUX', (0.5242718446601942, 721)),\n",
    "  ('VERB', (0.48768768768768767, 1665)),\n",
    "  ('INTJ', (0.375, 88)),\n",
    "  ('PART', (0.3074626865671642, 335)),\n",
    "  ('NUM', (0.22448979591836735, 343)),\n",
    "  ('PROPN', (0.22123893805309736, 1243)),\n",
    "  ('ADJ', (0.2206148282097649, 1106)),\n",
    "  ('ADV', (0.2137030995106036, 613)),\n",
    "  ('SYM', (0.17142857142857143, 35)),\n",
    "  ('X', (0.0, 26))])\n",
    "--- ife, sparse, token-positional encoding\n",
    "100%|██████████| 18/18 [07:05<00:00, 23.61s/it]\n",
    "\n",
    "Token segmentation performance with/out space \n",
    "Counter({'TP': 29317, 'FP': 3473, 'FN': 1709, 'R': 0.945, 'F': 0.919, 'P': 0.894}) \n",
    "Counter({'TP': 16083, 'FP': 1897, 'FN': 133, 'R': 0.992, 'F': 0.94, 'P': 0.894})\n",
    "Overall POS accuracy with/out space 0.795687487913363 0.6090897878638382\n",
    "Overall SUP:DEP accuracy with/out space 0.3339737724916133 0.33403781979977754\n",
    "Overall s_type accuracy:  0.7136465324384788\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (0.9864176570458404, 589)),\n",
    "  ('intj', (0.46153846153846156, 26)),\n",
    "  ('q', (0.375, 16)),\n",
    "  ('wh', (0.2857142857142857, 21)),\n",
    "  ('frag', (0.26881720430107525, 93)),\n",
    "  ('inf', (0.2, 5)),\n",
    "  ('imp', (0.12244897959183673, 49)),\n",
    "  ('sub', (0.024390243902439025, 41)),\n",
    "  ('multiple', (0.0, 32)),\n",
    "  ('other', (0.0, 14)),\n",
    "  ('ger', (0.0, 8))],\n",
    " [('SPACE', (1.0, 14810)),\n",
    "  ('PUNCT', (0.8941747572815534, 2060)),\n",
    "  ('DET', (0.8183118741058655, 1398)),\n",
    "  ('ADP', (0.7680563711098062, 1703)),\n",
    "  ('CCONJ', (0.7517006802721088, 588)),\n",
    "  ('NOUN', (0.7485695052170986, 2971)),\n",
    "  ('PRON', (0.7056638811513464, 1077)),\n",
    "  ('AUX', (0.5312066574202496, 721)),\n",
    "  ('VERB', (0.4924924924924925, 1665)),\n",
    "  ('SCONJ', (0.3770491803278688, 244)),\n",
    "  ('INTJ', (0.3068181818181818, 88)),\n",
    "  ('PART', (0.29850746268656714, 335)),\n",
    "  ('PROPN', (0.2333065164923572, 1243)),\n",
    "  ('ADJ', (0.22423146473779385, 1106)),\n",
    "  ('NUM', (0.21865889212827988, 343)),\n",
    "  ('ADV', (0.19086460032626426, 613)),\n",
    "  ('SYM', (0.14285714285714285, 35)),\n",
    "  ('X', (0.0, 26))])\n",
    "--- ife, fine tuned, uniform-positional encoding\n",
    "100%|██████████| 18/18 [2:34:46<00:00, 515.94s/it]\n",
    "\n",
    "Token segmentation performance with/out space:\n",
    "Counter({'TP': 27967, 'FP': 4668, 'FN': 2165, 'R': 0.928, 'F': 0.891, 'P': 0.857}) \n",
    "Counter({'TP': 16028, 'FP': 2718, 'FN': 188, 'R': 0.988, 'F': 0.917, 'P': 0.855})\n",
    "Overall POS accuracy with/out space 0.7831541218637993 0.5993463246176616\n",
    "Overall SUP:DEP accuracy with/out space 0.30378428068025126 0.30390483303104665\n",
    "Overall s_type accuracy:  0.7125279642058165\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (0.9643463497453311, 589)),\n",
    "  ('intj', (0.6153846153846154, 26)),\n",
    "  ('q', (0.5, 16)),\n",
    "  ('frag', (0.3010752688172043, 93)),\n",
    "  ('wh', (0.23809523809523808, 21)),\n",
    "  ('imp', (0.16326530612244897, 49)),\n",
    "  ('sub', (0.07317073170731707, 41)),\n",
    "  ('multiple', (0.03125, 32)),\n",
    "  ('other', (0.0, 14)),\n",
    "  ('ger', (0.0, 8)),\n",
    "  ('inf', (0.0, 5))],\n",
    " [('SPACE', (0.9973411899971256, 13916)),\n",
    "  ('PUNCT', (0.9242718446601942, 2060)),\n",
    "  ('DET', (0.9062947067238912, 1398)),\n",
    "  ('CCONJ', (0.8690476190476191, 588)),\n",
    "  ('ADP', (0.7862595419847328, 1703)),\n",
    "  ('PRON', (0.7520891364902507, 1077)),\n",
    "  ('AUX', (0.7128987517337032, 721)),\n",
    "  ('PART', (0.6746268656716418, 335)),\n",
    "  ('SCONJ', (0.5245901639344263, 244)),\n",
    "  ('NOUN', (0.49814877145742176, 2971)),\n",
    "  ('INTJ', (0.36363636363636365, 88)),\n",
    "  ('VERB', (0.35315315315315315, 1665)),\n",
    "  ('PROPN', (0.30973451327433627, 1243)),\n",
    "  ('ADJ', (0.26763110307414106, 1106)),\n",
    "  ('NUM', (0.2478134110787172, 343)),\n",
    "  ('ADV', (0.2463295269168026, 613)),\n",
    "  ('SYM', (0.08571428571428572, 35)),\n",
    "  ('X', (0.0, 26))])\n",
    "--- ife, fine tuned, token-positional encoding\n",
    "100%|██████████| 18/18 [18:42<00:00, 62.39s/it]\n",
    "\n",
    "Token segmentation performance with/out space \n",
    "Counter({'TP': 29101, 'FP': 3984, 'FN': 1925, 'R': 0.938, 'F': 0.908, 'P': 0.88}) \n",
    "Counter({'TP': 16028, 'FP': 2247, 'FN': 188, 'R': 0.988, 'F': 0.929, 'P': 0.877})\n",
    "Overall POS accuracy with/out space 0.7975568877715464 0.6126665022200296\n",
    "Overall SUP:DEP accuracy with/out space 0.28958742632612966 0.28891928864569083\n",
    "Overall s_type accuracy:  0.6778523489932886\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (0.99830220713073, 589)),\n",
    "  ('intj', (0.34615384615384615, 26)),\n",
    "  ('frag', (0.08602150537634409, 93)),\n",
    "  ('q', (0.0625, 16)),\n",
    "  ('imp', (0.0, 49)),\n",
    "  ('sub', (0.0, 41)),\n",
    "  ('multiple', (0.0, 32)),\n",
    "  ('wh', (0.0, 21)),\n",
    "  ('other', (0.0, 14)),\n",
    "  ('ger', (0.0, 8)),\n",
    "  ('inf', (0.0, 5))],\n",
    " [('SPACE', (1.0, 14810)),\n",
    "  ('PUNCT', (0.9529126213592233, 2060)),\n",
    "  ('DET', (0.9213161659513591, 1398)),\n",
    "  ('CCONJ', (0.8401360544217688, 588)),\n",
    "  ('ADP', (0.806224310041104, 1703)),\n",
    "  ('PRON', (0.7325905292479109, 1077)),\n",
    "  ('PART', (0.6985074626865672, 335)),\n",
    "  ('AUX', (0.6463245492371706, 721)),\n",
    "  ('NOUN', (0.5412319084483339, 2971)),\n",
    "  ('SCONJ', (0.45491803278688525, 244)),\n",
    "  ('VERB', (0.4096096096096096, 1665)),\n",
    "  ('PROPN', (0.31053901850362026, 1243)),\n",
    "  ('INTJ', (0.3068181818181818, 88)),\n",
    "  ('ADJ', (0.2585895117540687, 1106)),\n",
    "  ('NUM', (0.24489795918367346, 343)),\n",
    "  ('ADV', (0.22838499184339314, 613)),\n",
    "  ('SYM', (0.11428571428571428, 35)),\n",
    "  ('X', (0.0, 26))])\n",
    "--- ife, fine tuned using all pre-training data, token-positional encoding\n",
    "100%|██████████| 18/18 [12:42<00:00, 42.36s/it]\n",
    "\n",
    "Token segmentation performance with/out space \n",
    "Counter({'TP': 28946, 'FP': 4430, 'FN': 2080, 'R': 0.933, 'F': 0.899, 'P': 0.867}) \n",
    "Counter({'TP': 16129, 'FP': 2437, 'FN': 87, 'R': 0.995, 'F': 0.928, 'P': 0.869})\n",
    "Overall POS accuracy with/out space 0.7999742151743698 0.6172915638875185\n",
    "Overall SUP:DEP accuracy with/out space 0.28577420901246403 0.28374447915544543\n",
    "Overall s_type accuracy:  0.6789709172259508\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (0.99830220713073, 589)),\n",
    "  ('intj', (0.19230769230769232, 26)),\n",
    "  ('frag', (0.15053763440860216, 93)),\n",
    "  ('imp', (0.0, 49)),\n",
    "  ('sub', (0.0, 41)),\n",
    "  ('multiple', (0.0, 32)),\n",
    "  ('wh', (0.0, 21)),\n",
    "  ('q', (0.0, 16)),\n",
    "  ('other', (0.0, 14)),\n",
    "  ('ger', (0.0, 8)),\n",
    "  ('inf', (0.0, 5))],\n",
    " [('SPACE', (1.0, 14810)),\n",
    "  ('PUNCT', (0.9601941747572815, 2060)),\n",
    "  ('DET', (0.9277539341917024, 1398)),\n",
    "  ('CCONJ', (0.8537414965986394, 588)),\n",
    "  ('ADP', (0.8027011156782149, 1703)),\n",
    "  ('PART', (0.7850746268656716, 335)),\n",
    "  ('PRON', (0.7678737233054782, 1077)),\n",
    "  ('AUX', (0.6907073509015257, 721)),\n",
    "  ('SCONJ', (0.6229508196721312, 244)),\n",
    "  ('NOUN', (0.47088522383036013, 2971)),\n",
    "  ('VERB', (0.4114114114114114, 1665)),\n",
    "  ('INTJ', (0.4090909090909091, 88)),\n",
    "  ('NUM', (0.35276967930029157, 343)),\n",
    "  ('PROPN', (0.3153660498793242, 1243)),\n",
    "  ('ADV', (0.2969004893964111, 613)),\n",
    "  ('ADJ', (0.2730560578661845, 1106)),\n",
    "  ('SYM', (0.2571428571428571, 35)),\n",
    "  ('X', (0.0, 26))])\n",
    "--- ife, fine tuned using all pre-training data, token-positional encoding, no spaces\n",
    "Token segmentation performance with/out space \n",
    "Counter({'TP': 12109, 'FP': 4632, 'FN': 4107, 'R': 0.747, 'F': 0.735, 'P': 0.723}) \n",
    "Counter({'TP': 12109, 'FP': 4632, 'FN': 4107, 'R': 0.747, 'F': 0.735, 'P': 0.723})\n",
    "Overall POS accuracy with/out space 0.5549457326097681 0.5549457326097681\n",
    "Overall SUP:DEP accuracy with/out space 0.1356549787945762 0.1356549787945762\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [],\n",
    " [('PUNCT', (0.9097087378640777, 2060)),\n",
    "  ('DET', (0.8497854077253219, 1398)),\n",
    "  ('CCONJ', (0.8180272108843537, 588)),\n",
    "  ('ADP', (0.7146212566059894, 1703)),\n",
    "  ('INTJ', (0.5113636363636364, 88)),\n",
    "  ('AUX', (0.5104022191400832, 721)),\n",
    "  ('SCONJ', (0.5081967213114754, 244)),\n",
    "  ('PRON', (0.5051067780872794, 1077)),\n",
    "  ('NOUN', (0.4867048131942107, 2971)),\n",
    "  ('ADJ', (0.4240506329113924, 1106)),\n",
    "  ('VERB', (0.34654654654654654, 1665)),\n",
    "  ('PART', (0.3283582089552239, 335)),\n",
    "  ('ADV', (0.27569331158238175, 613)),\n",
    "  ('NUM', (0.2478134110787172, 343)),\n",
    "  ('PROPN', (0.24215607401448108, 1243)),\n",
    "  ('SYM', (0.02857142857142857, 35)),\n",
    "  ('X', (0.0, 26))])\n",
    "--- form, sparse, uniform-positional encoding\n",
    "100%|██████████| 18/18 [9:55:08<00:00, 1983.80s/it]\n",
    "\n",
    "Token segmentation performance with/out space \n",
    "Counter({'TP': 29528, 'FP': 3009, 'FN': 1498, 'R': 0.952, 'F': 0.929, 'P': 0.908}) \n",
    "Counter({'TP': 16051, 'FP': 1676, 'FN': 165, 'R': 0.99, 'F': 0.946, 'P': 0.905})\n",
    "Overall POS accuracy with/out space 0.8271127441500676 0.6697705969412926\n",
    "Overall SUP:DEP accuracy with/out space 0.3659218735593325 0.36684154115191514\n",
    "Overall s_type accuracy:  0.7304250559284117\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (0.9881154499151104, 589)),\n",
    "  ('intj', (0.5384615384615384, 26)),\n",
    "  ('q', (0.5, 16)),\n",
    "  ('imp', (0.3673469387755102, 49)),\n",
    "  ('frag', (0.27956989247311825, 93)),\n",
    "  ('wh', (0.19047619047619047, 21)),\n",
    "  ('sub', (0.024390243902439025, 41)),\n",
    "  ('multiple', (0.0, 32)),\n",
    "  ('other', (0.0, 14)),\n",
    "  ('ger', (0.0, 8)),\n",
    "  ('inf', (0.0, 5))],\n",
    " [('SPACE', (0.9993923024983119, 14810)),\n",
    "  ('PUNCT', (0.9271844660194175, 2060)),\n",
    "  ('NOUN', (0.8391114102995625, 2971)),\n",
    "  ('DET', (0.7982832618025751, 1398)),\n",
    "  ('ADP', (0.7639459776864357, 1703)),\n",
    "  ('PRON', (0.754874651810585, 1077)),\n",
    "  ('VERB', (0.6216216216216216, 1665)),\n",
    "  ('CCONJ', (0.5952380952380952, 588)),\n",
    "  ('AUX', (0.5409153952843273, 721)),\n",
    "  ('SCONJ', (0.4344262295081967, 244)),\n",
    "  ('ADJ', (0.4204339963833635, 1106)),\n",
    "  ('ADV', (0.3866231647634584, 613)),\n",
    "  ('NUM', (0.36443148688046645, 343)),\n",
    "  ('PROPN', (0.3153660498793242, 1243)),\n",
    "  ('INTJ', (0.3068181818181818, 88)),\n",
    "  ('PART', (0.29253731343283584, 335)),\n",
    "  ('SYM', (0.08571428571428572, 35)),\n",
    "  ('X', (0.0, 26))])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [33:13<00:00, 110.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall POS accuracy with/out space 0.2919164571649584 0.20072767636901825\n",
      "Overall SUP:DEP accuracy with/out space 0.3028582489192851 0.30219915987150975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Tag-wise accuracy',\n",
       " [],\n",
       " [('NOUN', (0.568158869067654, 2971)),\n",
       "  ('SPACE', (0.39176232275489536, 14810)),\n",
       "  ('INTJ', (0.36363636363636365, 88)),\n",
       "  ('PROPN', (0.25422365245374096, 1243)),\n",
       "  ('PRON', (0.2414113277623027, 1077)),\n",
       "  ('NUM', (0.2303206997084548, 343)),\n",
       "  ('PUNCT', (0.19223300970873786, 2060)),\n",
       "  ('VERB', (0.13753753753753753, 1665)),\n",
       "  ('ADV', (0.09135399673735727, 613)),\n",
       "  ('PART', (0.05373134328358209, 335)),\n",
       "  ('AUX', (0.052704576976421634, 721)),\n",
       "  ('ADP', (0.040516735173223725, 1703)),\n",
       "  ('ADJ', (0.023508137432188065, 1106)),\n",
       "  ('DET', (0.022889842632331903, 1398)),\n",
       "  ('SCONJ', (0.020491803278688523, 244)),\n",
       "  ('CCONJ', (0.01870748299319728, 588)),\n",
       "  ('SYM', (0.0, 35)),\n",
       "  ('X', (0.0, 26))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "accuracy = defaultdict(list)\n",
    "accuracy_nsp = defaultdict(list)\n",
    "accuracy_all, accuracy_all_nsp, = [], []\n",
    "sup_accuracy, sup_accuracy_nsp, = 0, 0\n",
    "accuracy_sty = defaultdict(list)\n",
    "accuracy_all_sty = []\n",
    "\n",
    "model.interpret(tdocs, seed = 691, covering = [[[row[1] for row in s] for s in d['conllu']] for d in test_docs])\n",
    "\n",
    "for d_i, doc in enumerate(model._documents):\n",
    "    for s_i, s in enumerate(doc._sentences):\n",
    "        if s._sty is not None:\n",
    "            result = s._sty == test_docs[d_i]['s_type'][s_i]\n",
    "            accuracy_sty[test_docs[d_i]['s_type'][s_i]].append(result)\n",
    "            accuracy_all_sty.append(result)\n",
    "\n",
    "pred_toks = [t._form for doc in model._documents for s in doc._sentences for t in s._tokens]\n",
    "pred_arcs = set([(ix, str(t._sup), t._dep, d_i, s_i) for d_i, doc in enumerate(model._documents) \n",
    "                 for s_i, s in enumerate(doc._sentences) for ix, t in enumerate(s._tokens)])\n",
    "pred_spans = list(np.cumsum([len(t) for t in pred_toks]))\n",
    "pred_stream = [t._pos for doc in model._documents for s in doc._sentences for t in s._tokens]\n",
    "pred_spans = {(sh-len(gt), sh): (gl, gt)\n",
    "              for sh, gt, gl in zip(pred_spans, pred_toks, pred_stream)}\n",
    "\n",
    "gold_toks = [row[1] for d in test_docs for s in d['conllu'] for row in s]\n",
    "gold_arcs = set([(ix, (str(int(row[6]) - int(row[0])) if int(row[6]) else row[6]), row[7], d_i, s_i) \n",
    "                 for d_i, d in enumerate(test_docs) for s_i, s in enumerate(d['conllu']) for ix, row in enumerate(s)])\n",
    "gold_spans = list(np.cumsum([len(t) for t in gold_toks]))\n",
    "gold_stream = [row[3] for d in test_docs for s in d['conllu'] for row in s]\n",
    "gold_spans = {(sh-len(gt), sh): (gl, gt)\n",
    "              for sh, gt, gl in zip(gold_spans, gold_toks, gold_stream)}\n",
    "\n",
    "for gold_span in gold_spans:\n",
    "    if gold_span in pred_spans:\n",
    "        result = gold_spans[gold_span] == pred_spans[gold_span]\n",
    "    else:\n",
    "        result = False\n",
    "    accuracy[gold_spans[gold_span][0]].append(result)\n",
    "    accuracy_all.append(result)\n",
    "    if gold_spans[gold_span][1] != ' ':\n",
    "        accuracy_nsp[gold_spans[gold_span][0]].append(result)\n",
    "        accuracy_all_nsp.append(result)\n",
    "        \n",
    "for ptok, parc in zip(pred_toks, pred_arcs):\n",
    "    if parc in gold_arcs:\n",
    "        sup_accuracy += 1\n",
    "        if ptok != ' ':\n",
    "            sup_accuracy_nsp += 1\n",
    "sup_accuracy /= len(pred_toks)\n",
    "sup_accuracy_nsp /= len([x for x in pred_toks if x != ' '])\n",
    "\n",
    "print(\"Overall POS accuracy with/out space\", sum(accuracy_all)/len(accuracy_all), sum(accuracy_all_nsp)/len(accuracy_all_nsp))\n",
    "print(\"Overall SUP:DEP accuracy with/out space\", sup_accuracy, sup_accuracy_nsp)\n",
    "if len(accuracy_all_sty):\n",
    "    print(\"Overall s_type accuracy: \", sum(accuracy_all_sty)/len(accuracy_all_sty))\n",
    "\"Tag-wise accuracy\", list(Counter({tag: (sum(accuracy_sty[tag])/len(accuracy_sty[tag]), len(accuracy_sty[tag])) \n",
    "                                   for tag in accuracy_sty}).most_common()), list(Counter({tag: (sum(accuracy[tag])/len(accuracy[tag]), len(accuracy[tag])) \n",
    "                                                                                           for tag in accuracy}).most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "--- start small training/test\n",
    "--- ife, sparse, uniform-positional encoding\n",
    "100%|██████████| 1/1 [00:13<00:00, 13.63s/it]\n",
    "\n",
    "Overall POS accuracy with/out space 0.7208249496981891 0.5242626070409134\n",
    "Overall SUP:DEP accuracy with/out space 0.499496475327291 0.4918970448045758\n",
    "Overall s_type accuracy:  0.7037037037037037\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (1.0, 37)), ('frag', (0.0625, 16)), ('wh', (0.0, 1))],\n",
    " [('SPACE', (0.9413020277481323, 937)),\n",
    "  ('NOUN', (0.871900826446281, 242)),\n",
    "  ('PUNCT', (0.70625, 160)),\n",
    "  ('DET', (0.5876288659793815, 97)),\n",
    "  ('ADP', (0.5855855855855856, 111)),\n",
    "  ('CCONJ', (0.56, 25)),\n",
    "  ('VERB', (0.46017699115044247, 113)),\n",
    "  ('PRON', (0.36363636363636365, 22)),\n",
    "  ('NUM', (0.32558139534883723, 43)),\n",
    "  ('AUX', (0.2682926829268293, 41)),\n",
    "  ('PROPN', (0.15625, 32)),\n",
    "  ('ADV', (0.02564102564102564, 39)),\n",
    "  ('ADJ', (0.0, 83)),\n",
    "  ('SCONJ', (0.0, 16)),\n",
    "  ('PART', (0.0, 13)),\n",
    "  ('SYM', (0.0, 12)),\n",
    "  ('X', (0.0, 2))])\n",
    "--- ife, sparse, token-positional encoding\n",
    "100%|██████████| 1/1 [00:12<00:00, 12.83s/it]\n",
    "\n",
    "Overall POS accuracy with/out space 0.7540241448692153 0.5394862036156042\n",
    "Overall SUP:DEP accuracy with/out space 0.5161127895266868 0.5147759771210677\n",
    "Overall s_type accuracy:  0.7037037037037037\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (1.0, 37)), ('frag', (0.0625, 16)), ('wh', (0.0, 1))],\n",
    " [('SPACE', (0.9946638207043756, 937)),\n",
    "  ('NOUN', (0.8801652892561983, 242)),\n",
    "  ('PUNCT', (0.7375, 160)),\n",
    "  ('ADP', (0.6306306306306306, 111)),\n",
    "  ('DET', (0.6288659793814433, 97)),\n",
    "  ('CCONJ', (0.6, 25)),\n",
    "  ('VERB', (0.4690265486725664, 113)),\n",
    "  ('NUM', (0.32558139534883723, 43)),\n",
    "  ('PRON', (0.2727272727272727, 22)),\n",
    "  ('AUX', (0.2682926829268293, 41)),\n",
    "  ('PROPN', (0.15625, 32)),\n",
    "  ('ADV', (0.02564102564102564, 39)),\n",
    "  ('ADJ', (0.0, 83)),\n",
    "  ('SCONJ', (0.0, 16)),\n",
    "  ('PART', (0.0, 13)),\n",
    "  ('SYM', (0.0, 12)),\n",
    "  ('X', (0.0, 2))])\n",
    "--- ife, fine tuned, uniform-positional encoding \n",
    "100%|██████████| 1/1 [00:12<00:00, 12.06s/it]\n",
    "\n",
    "Overall POS accuracy with/out space 0.6896378269617707 0.4652711703139867\n",
    "Overall SUP:DEP accuracy with/out space 0.5115810674723061 0.5081029551954243\n",
    "Overall s_type accuracy:  0.6851851851851852\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (1.0, 37)), ('frag', (0.0, 16)), ('wh', (0.0, 1))],\n",
    " [('SPACE', (0.9413020277481323, 937)),\n",
    "  ('NOUN', (0.6570247933884298, 242)),\n",
    "  ('ADP', (0.6306306306306306, 111)),\n",
    "  ('DET', (0.6185567010309279, 97)),\n",
    "  ('PUNCT', (0.6125, 160)),\n",
    "  ('CCONJ', (0.52, 25)),\n",
    "  ('AUX', (0.3902439024390244, 41)),\n",
    "  ('VERB', (0.3893805309734513, 113)),\n",
    "  ('NUM', (0.27906976744186046, 43)),\n",
    "  ('PART', (0.23076923076923078, 13)),\n",
    "  ('PRON', (0.18181818181818182, 22)),\n",
    "  ('ADJ', (0.08433734939759036, 83)),\n",
    "  ('PROPN', (0.0625, 32)),\n",
    "  ('ADV', (0.02564102564102564, 39)),\n",
    "  ('SCONJ', (0.0, 16)),\n",
    "  ('SYM', (0.0, 12)),\n",
    "  ('X', (0.0, 2))])\n",
    "--- ife, fine tuned, token-positional encoding\n",
    "100%|██████████| 1/1 [00:15<00:00, 15.76s/it]\n",
    "\n",
    "Overall POS accuracy with/out space 0.7364185110663984 0.5014272121788773\n",
    "Overall SUP:DEP accuracy with/out space 0.5518630412890232 0.5643469971401335\n",
    "Overall s_type accuracy:  0.6851851851851852\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (1.0, 37)), ('frag', (0.0, 16)), ('wh', (0.0, 1))],\n",
    " [('SPACE', (1.0, 937)),\n",
    "  ('PUNCT', (0.75625, 160)),\n",
    "  ('NOUN', (0.6694214876033058, 242)),\n",
    "  ('DET', (0.6597938144329897, 97)),\n",
    "  ('ADP', (0.6396396396396397, 111)),\n",
    "  ('CCONJ', (0.6, 25)),\n",
    "  ('VERB', (0.45132743362831856, 113)),\n",
    "  ('AUX', (0.34146341463414637, 41)),\n",
    "  ('NUM', (0.27906976744186046, 43)),\n",
    "  ('PRON', (0.18181818181818182, 22)),\n",
    "  ('PROPN', (0.15625, 32)),\n",
    "  ('PART', (0.15384615384615385, 13)),\n",
    "  ('ADJ', (0.060240963855421686, 83)),\n",
    "  ('ADV', (0.02564102564102564, 39)),\n",
    "  ('SCONJ', (0.0, 16)),\n",
    "  ('SYM', (0.0, 12)),\n",
    "  ('X', (0.0, 2))])\n",
    "--- form, sparse, uniform-positional encoding\n",
    "Overall POS accuracy with/out space 0.7471561530506722 0.5347288296860133\n",
    "Overall SUP:DEP accuracy with/out space 0.5377846790890269 0.5243088655862727\n",
    "Overall s_type accuracy:  0.6851851851851852\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (1.0, 37)), ('frag', (0.0, 16)), ('wh', (0.0, 1))],\n",
    " [('PUNCT', (0.9731543624161074, 1043)),\n",
    "  ('NOUN', (0.8842975206611571, 242)),\n",
    "  ('ADP', (0.7027027027027027, 111)),\n",
    "  ('DET', (0.5567010309278351, 97)),\n",
    "  ('NUM', (0.3953488372093023, 43)),\n",
    "  ('VERB', (0.3893805309734513, 113)),\n",
    "  ('PART', (0.23076923076923078, 13)),\n",
    "  ('PRON', (0.22727272727272727, 22)),\n",
    "  ('CCONJ', (0.2, 25)),\n",
    "  ('PROPN', (0.09375, 32)),\n",
    "  ('ADV', (0.07692307692307693, 39)),\n",
    "  ('AUX', (0.04878048780487805, 41)),\n",
    "  ('ADJ', (0.024096385542168676, 83)),\n",
    "  ('SCONJ', (0.0, 16)),\n",
    "  ('SYM', (0.0, 12)),\n",
    "  ('X', (0.0, 2))])\n",
    "--- end small training/test\n",
    "--- start full training/test\n",
    "--- ife, sparse, uniform-positional encoding\n",
    "100%|██████████| 18/18 [39:12<00:00, 130.72s/it]\n",
    "\n",
    "Overall POS accuracy with/out space 0.8118279569892473 0.6528737049827331\n",
    "Overall SUP:DEP accuracy with/out space 0.6086566569226681 0.6032863849765259\n",
    "Overall s_type accuracy:  0.7248322147651006\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (0.9643463497453311, 589)),\n",
    "  ('intj', (0.5769230769230769, 26)),\n",
    "  ('q', (0.5625, 16)),\n",
    "  ('inf', (0.4, 5)),\n",
    "  ('wh', (0.38095238095238093, 21)),\n",
    "  ('frag', (0.3225806451612903, 93)),\n",
    "  ('imp', (0.20408163265306123, 49)),\n",
    "  ('sub', (0.12195121951219512, 41)),\n",
    "  ('other', (0.07142857142857142, 14)),\n",
    "  ('multiple', (0.0, 32)),\n",
    "  ('ger', (0.0, 8))],\n",
    " [('SPACE', (0.997053751077896, 13916)),\n",
    "  ('PUNCT', (0.8820388349514563, 2060)),\n",
    "  ('NOUN', (0.8801750252440256, 2971)),\n",
    "  ('DET', (0.8204577968526466, 1398)),\n",
    "  ('ADP', (0.7563123899001761, 1703)),\n",
    "  ('CCONJ', (0.7380952380952381, 588)),\n",
    "  ('PRON', (0.7214484679665738, 1077)),\n",
    "  ('SCONJ', (0.6065573770491803, 244)),\n",
    "  ('AUX', (0.5866851595006934, 721)),\n",
    "  ('VERB', (0.5321321321321322, 1665)),\n",
    "  ('INTJ', (0.38636363636363635, 88)),\n",
    "  ('PART', (0.382089552238806, 335)),\n",
    "  ('NUM', (0.3119533527696793, 343)),\n",
    "  ('PROPN', (0.3049074818986323, 1243)),\n",
    "  ('ADV', (0.24796084828711257, 613)),\n",
    "  ('ADJ', (0.22151898734177214, 1106)),\n",
    "  ('SYM', (0.17142857142857143, 35)),\n",
    "  ('X', (0.038461538461538464, 26))])\n",
    "--- ife, sparse, token-positional encoding\n",
    "100%|██████████| 18/18 [06:18<00:00, 21.02s/it]\n",
    "\n",
    "Overall POS accuracy with/out space 0.8213756204473667 0.6582387765170202\n",
    "Overall SUP:DEP accuracy with/out space 0.5965546164268662 0.5992092908327156\n",
    "Overall s_type accuracy:  0.7125279642058165\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (0.9847198641765704, 589)),\n",
    "  ('intj', (0.46153846153846156, 26)),\n",
    "  ('q', (0.375, 16)),\n",
    "  ('wh', (0.2857142857142857, 21)),\n",
    "  ('frag', (0.26881720430107525, 93)),\n",
    "  ('inf', (0.2, 5)),\n",
    "  ('imp', (0.12244897959183673, 49)),\n",
    "  ('sub', (0.024390243902439025, 41)),\n",
    "  ('multiple', (0.0, 32)),\n",
    "  ('other', (0.0, 14)),\n",
    "  ('ger', (0.0, 8))],\n",
    " [('SPACE', (1.0, 14810)),\n",
    "  ('PUNCT', (0.9053398058252428, 2060)),\n",
    "  ('NOUN', (0.8872433524065971, 2971)),\n",
    "  ('DET', (0.8190271816881259, 1398)),\n",
    "  ('ADP', (0.7821491485613623, 1703)),\n",
    "  ('PRON', (0.7595171773444754, 1077)),\n",
    "  ('CCONJ', (0.7517006802721088, 588)),\n",
    "  ('AUX', (0.5936199722607489, 721)),\n",
    "  ('VERB', (0.5315315315315315, 1665)),\n",
    "  ('SCONJ', (0.38114754098360654, 244)),\n",
    "  ('PART', (0.3611940298507463, 335)),\n",
    "  ('PROPN', (0.31938857602574416, 1243)),\n",
    "  ('INTJ', (0.3181818181818182, 88)),\n",
    "  ('NUM', (0.26239067055393583, 343)),\n",
    "  ('ADJ', (0.22694394213381555, 1106)),\n",
    "  ('ADV', (0.22512234910277323, 613)),\n",
    "  ('SYM', (0.14285714285714285, 35)),\n",
    "  ('X', (0.0, 26))])\n",
    "--- ife, fine tuned, uniform-positional encoding\n",
    "100%|██████████| 18/18 [2:29:47<00:00, 499.28s/it]\n",
    "\n",
    "Overall POS accuracy with/out space 0.816938802601885 0.6610754810064134\n",
    "Overall SUP:DEP accuracy with/out space 0.5741097528567632 0.573943661971831\n",
    "Overall s_type accuracy:  0.70917225950783\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (0.9609507640067911, 589)),\n",
    "  ('intj', (0.6153846153846154, 26)),\n",
    "  ('q', (0.5, 16)),\n",
    "  ('frag', (0.3010752688172043, 93)),\n",
    "  ('wh', (0.23809523809523808, 21)),\n",
    "  ('imp', (0.14285714285714285, 49)),\n",
    "  ('sub', (0.07317073170731707, 41)),\n",
    "  ('multiple', (0.03125, 32)),\n",
    "  ('other', (0.0, 14)),\n",
    "  ('ger', (0.0, 8)),\n",
    "  ('inf', (0.0, 5))],\n",
    " [('SPACE', (0.9985628054038517, 13916)),\n",
    "  ('PUNCT', (0.9266990291262136, 2060)),\n",
    "  ('DET', (0.9084406294706724, 1398)),\n",
    "  ('CCONJ', (0.8707482993197279, 588)),\n",
    "  ('PRON', (0.8142989786443825, 1077)),\n",
    "  ('ADP', (0.8038755137991779, 1703)),\n",
    "  ('AUX', (0.7572815533980582, 721)),\n",
    "  ('PART', (0.7522388059701492, 335)),\n",
    "  ('NOUN', (0.6893301918545944, 2971)),\n",
    "  ('SCONJ', (0.5245901639344263, 244)),\n",
    "  ('VERB', (0.42162162162162165, 1665)),\n",
    "  ('PROPN', (0.4006436041834272, 1243)),\n",
    "  ('INTJ', (0.375, 88)),\n",
    "  ('NUM', (0.2915451895043732, 343)),\n",
    "  ('ADV', (0.27569331158238175, 613)),\n",
    "  ('ADJ', (0.2739602169981917, 1106)),\n",
    "  ('SYM', (0.08571428571428572, 35)),\n",
    "  ('X', (0.038461538461538464, 26))])\n",
    "--- ife, fine tuned, token-positional encoding\n",
    "100%|██████████| 18/18 [10:14<00:00, 34.12s/it]\n",
    "\n",
    "Overall POS accuracy with/out space 0.8244697995229807 0.6641588554514061\n",
    "Overall SUP:DEP accuracy with/out space 0.5768114071875605 0.5762910798122066\n",
    "Overall s_type accuracy:  0.6778523489932886\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (0.99830220713073, 589)),\n",
    "  ('intj', (0.34615384615384615, 26)),\n",
    "  ('frag', (0.08602150537634409, 93)),\n",
    "  ('q', (0.0625, 16)),\n",
    "  ('imp', (0.0, 49)),\n",
    "  ('sub', (0.0, 41)),\n",
    "  ('multiple', (0.0, 32)),\n",
    "  ('wh', (0.0, 21)),\n",
    "  ('other', (0.0, 14)),\n",
    "  ('ger', (0.0, 8)),\n",
    "  ('inf', (0.0, 5))],\n",
    " [('SPACE', (1.0, 14810)),\n",
    "  ('PUNCT', (0.954368932038835, 2060)),\n",
    "  ('DET', (0.9227467811158798, 1398)),\n",
    "  ('CCONJ', (0.8401360544217688, 588)),\n",
    "  ('ADP', (0.8244274809160306, 1703)),\n",
    "  ('PRON', (0.7883008356545961, 1077)),\n",
    "  ('PART', (0.7820895522388059, 335)),\n",
    "  ('NOUN', (0.6886570178391114, 2971)),\n",
    "  ('AUX', (0.6851595006934813, 721)),\n",
    "  ('VERB', (0.48228228228228226, 1665)),\n",
    "  ('SCONJ', (0.45491803278688525, 244)),\n",
    "  ('PROPN', (0.38777152051488334, 1243)),\n",
    "  ('INTJ', (0.32954545454545453, 88)),\n",
    "  ('NUM', (0.29737609329446063, 343)),\n",
    "  ('ADJ', (0.2603978300180832, 1106)),\n",
    "  ('ADV', (0.23817292006525284, 613)),\n",
    "  ('SYM', (0.11428571428571428, 35)),\n",
    "  ('X', (0.0, 26))])\n",
    "--- ife, fine tuned using all pre-training data, token-positional encoding\n",
    "100%|██████████| 18/18 [12:25<00:00, 41.44s/it]\n",
    "\n",
    "Overall POS accuracy with/out space 0.8316895506994134 0.6779723729649728\n",
    "Overall SUP:DEP accuracy with/out space 0.5690367120459384 0.5663454410674573\n",
    "Overall s_type accuracy:  0.6789709172259508\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (0.99830220713073, 589)),\n",
    "  ('intj', (0.19230769230769232, 26)),\n",
    "  ('frag', (0.15053763440860216, 93)),\n",
    "  ('imp', (0.0, 49)),\n",
    "  ('sub', (0.0, 41)),\n",
    "  ('multiple', (0.0, 32)),\n",
    "  ('wh', (0.0, 21)),\n",
    "  ('q', (0.0, 16)),\n",
    "  ('other', (0.0, 14)),\n",
    "  ('ger', (0.0, 8)),\n",
    "  ('inf', (0.0, 5))],\n",
    " [('SPACE', (1.0, 14810)),\n",
    "  ('PUNCT', (0.9616504854368932, 2060)),\n",
    "  ('DET', (0.9284692417739628, 1398)),\n",
    "  ('PART', (0.8776119402985074, 335)),\n",
    "  ('CCONJ', (0.8537414965986394, 588)),\n",
    "  ('ADP', (0.815619495008808, 1703)),\n",
    "  ('PRON', (0.8133704735376045, 1077)),\n",
    "  ('AUX', (0.723994452149792, 721)),\n",
    "  ('NOUN', (0.6560080780881858, 2971)),\n",
    "  ('SCONJ', (0.6229508196721312, 244)),\n",
    "  ('VERB', (0.4864864864864865, 1665)),\n",
    "  ('NUM', (0.4518950437317784, 343)),\n",
    "  ('INTJ', (0.42045454545454547, 88)),\n",
    "  ('PROPN', (0.415124698310539, 1243)),\n",
    "  ('ADV', (0.32300163132137033, 613)),\n",
    "  ('ADJ', (0.2766726943942134, 1106)),\n",
    "  ('SYM', (0.2571428571428571, 35)),\n",
    "  ('X', (0.0, 26))])\n",
    "--- ife, fine tuned using all pre-training data, token-positional encoding, no spaces\n",
    "100%|██████████| 18/18 [02:36<00:00,  8.67s/it]\n",
    "\n",
    "Overall POS accuracy with/out space 0.712074494326591 0.712074494326591\n",
    "Overall SUP:DEP accuracy with/out space 0.2855201383741043 0.2855201383741043\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [],\n",
    " [('PUNCT', (0.9169902912621359, 2060)),\n",
    "  ('DET', (0.9120171673819742, 1398)),\n",
    "  ('ADP', (0.8461538461538461, 1703)),\n",
    "  ('CCONJ', (0.8384353741496599, 588)),\n",
    "  ('AUX', (0.8377253814147018, 721)),\n",
    "  ('PRON', (0.8217270194986073, 1077)),\n",
    "  ('PART', (0.7283582089552239, 335)),\n",
    "  ('NOUN', (0.6974082800403905, 2971)),\n",
    "  ('SCONJ', (0.6311475409836066, 244)),\n",
    "  ('VERB', (0.5897897897897898, 1665)),\n",
    "  ('INTJ', (0.5681818181818182, 88)),\n",
    "  ('ADJ', (0.5289330922242315, 1106)),\n",
    "  ('ADV', (0.41435562805872755, 613)),\n",
    "  ('PROPN', (0.40788415124698313, 1243)),\n",
    "  ('NUM', (0.3119533527696793, 343)),\n",
    "  ('X', (0.15384615384615385, 26)),\n",
    "  ('SYM', (0.02857142857142857, 35))])\n",
    "--- form, sparse, uniform-positional encoding\n",
    "100%|██████████| 18/18 [10:36:22<00:00, 2121.24s/it]\n",
    "\n",
    "Overall POS accuracy with/out space 0.8492232321278927 0.7121361618154909\n",
    "Overall SUP:DEP accuracy with/out space 0.6333957029485773 0.6320731405979738\n",
    "Overall s_type accuracy:  0.727069351230425\n",
    "\n",
    "('Tag-wise accuracy',\n",
    " [('decl', (0.9830220713073005, 589)),\n",
    "  ('intj', (0.5384615384615384, 26)),\n",
    "  ('q', (0.5, 16)),\n",
    "  ('imp', (0.3673469387755102, 49)),\n",
    "  ('frag', (0.27956989247311825, 93)),\n",
    "  ('wh', (0.19047619047619047, 21)),\n",
    "  ('sub', (0.024390243902439025, 41)),\n",
    "  ('multiple', (0.0, 32)),\n",
    "  ('other', (0.0, 14)),\n",
    "  ('ger', (0.0, 8)),\n",
    "  ('inf', (0.0, 5))],\n",
    " [('SPACE', (0.9993247805536799, 14810)),\n",
    "  ('NOUN', (0.9447997307303938, 2971)),\n",
    "  ('PUNCT', (0.9373786407766991, 2060)),\n",
    "  ('PRON', (0.8050139275766016, 1077)),\n",
    "  ('DET', (0.7982832618025751, 1398)),\n",
    "  ('ADP', (0.7780387551379918, 1703)),\n",
    "  ('VERB', (0.6582582582582582, 1665)),\n",
    "  ('CCONJ', (0.5952380952380952, 588)),\n",
    "  ('AUX', (0.5880721220527045, 721)),\n",
    "  ('SCONJ', (0.4385245901639344, 244)),\n",
    "  ('NUM', (0.4314868804664723, 343)),\n",
    "  ('ADJ', (0.42495479204339964, 1106)),\n",
    "  ('ADV', (0.42251223491027734, 613)),\n",
    "  ('PROPN', (0.4022526146419952, 1243)),\n",
    "  ('PART', (0.3492537313432836, 335)),\n",
    "  ('INTJ', (0.3181818181818182, 88)),\n",
    "  ('SYM', (0.08571428571428572, 35)),\n",
    "  ('X', (0.0, 26))])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning dense output heads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [6:31:15<00:00, 177.84s/it]\n"
     ]
    }
   ],
   "source": [
    "model.fine_tune(docs, covering = covering, all_layers = all_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [1:28:36<00:00, 295.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token segmentation performance with/out space Counter({'FN': 17674, 'TP': 13352, 'FP': 6633, 'P': 0.668, 'F': 0.523, 'R': 0.43}) Counter({'TP': 8521, 'FN': 7695, 'FP': 4203, 'P': 0.67, 'F': 0.589, 'R': 0.525})\n",
      "Overall POS accuracy with/out space 0.08824856571907433 0.05346571287617168\n",
      "Overall SUP:DEP accuracy with/out space 0.17062797097823368 0.16842187991197738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Tag-wise accuracy',\n",
       " [],\n",
       " [('INTJ', (0.3068181818181818, 88)),\n",
       "  ('SPACE', (0.1263335584064821, 14810)),\n",
       "  ('PUNCT', (0.12524271844660195, 2060)),\n",
       "  ('NUM', (0.11078717201166181, 343)),\n",
       "  ('DET', (0.0765379113018598, 1398)),\n",
       "  ('PRON', (0.06963788300835655, 1077)),\n",
       "  ('PROPN', (0.061946902654867256, 1243)),\n",
       "  ('NOUN', (0.05856613934702121, 2971)),\n",
       "  ('CCONJ', (0.02891156462585034, 588)),\n",
       "  ('SYM', (0.02857142857142857, 35)),\n",
       "  ('AUX', (0.027739251040221916, 721)),\n",
       "  ('ADP', (0.023487962419260128, 1703)),\n",
       "  ('ADV', (0.022838499184339316, 613)),\n",
       "  ('SCONJ', (0.00819672131147541, 244)),\n",
       "  ('VERB', (0.007207207207207207, 1665)),\n",
       "  ('ADJ', (0.0045207956600361665, 1106)),\n",
       "  ('PART', (0.0, 335)),\n",
       "  ('X', (0.0, 26))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "confusion = Counter()\n",
    "confusion_nsp = Counter()\n",
    "accuracy = defaultdict(list)\n",
    "accuracy_nsp = defaultdict(list)\n",
    "accuracy_all, accuracy_all_nsp, = [], []\n",
    "sup_accuracy, sup_accuracy_nsp, = 0, 0\n",
    "accuracy_sty = defaultdict(list)\n",
    "accuracy_all_sty = []\n",
    "\n",
    "model.interpret(tdocs, seed = 691) \n",
    "\n",
    "pred_toks = [t._form for doc in model._documents for s in doc._sentences for t in s._tokens]\n",
    "pred_spans = list(np.cumsum([len(t) for t in pred_toks]))\n",
    "pred_spans = set([(sh-len(gt), sh) for sh, gt in zip(pred_spans, pred_toks)])\n",
    "\n",
    "gold_toks = [row[1] for d in test_docs for s in d['conllu'] for row in s]\n",
    "gold_spans = list(np.cumsum([len(t) for t in gold_toks]))\n",
    "gold_spans = set([(sh-len(gt), sh) for sh, gt in zip(gold_spans, gold_toks)])\n",
    "\n",
    "for pred_span, pred_tok in zip(pred_spans, pred_toks):\n",
    "    if pred_span in gold_spans:\n",
    "        confusion['TP'] += 1\n",
    "        if pred_tok != ' ':\n",
    "            confusion_nsp['TP'] += 1\n",
    "    else:\n",
    "        confusion['FP'] += 1\n",
    "        if pred_tok != ' ':\n",
    "            confusion_nsp['FP'] += 1\n",
    "confusion['FN'] = len(gold_spans) - confusion['TP']\n",
    "confusion_nsp['FN'] = len([t for t in gold_toks if t != ' ']) - confusion_nsp['TP']\n",
    "\n",
    "confusion['P'] = round(confusion['TP']/(confusion['TP'] + confusion['FP']), 3)\n",
    "confusion['R'] = round(confusion['TP']/(confusion['TP'] + confusion['FN']), 3)\n",
    "confusion['F'] = round(2*confusion['P']*confusion['R']/(confusion['P']+confusion[\"R\"]), 3)\n",
    "confusion_nsp['P'] = round(confusion_nsp['TP']/(confusion_nsp['TP'] + confusion_nsp['FP']), 3)\n",
    "confusion_nsp['R'] = round(confusion_nsp['TP']/(confusion_nsp['TP'] + confusion_nsp['FN']), 3)\n",
    "confusion_nsp['F'] = round(2*confusion_nsp['P']*confusion_nsp['R']/(confusion_nsp['P']+confusion_nsp[\"R\"]), 3)\n",
    "\n",
    "for d_i, doc in enumerate(model._documents):\n",
    "    for s_i, s in enumerate(doc._sentences):\n",
    "        if s._sty is not None:\n",
    "            result = s._sty == test_docs[d_i]['s_type'][s_i]\n",
    "            accuracy_sty[test_docs[d_i]['s_type'][s_i]].append(result)\n",
    "            accuracy_all_sty.append(result)\n",
    "\n",
    "pred_toks = [t._form for doc in model._documents for s in doc._sentences for t in s._tokens]\n",
    "pred_arcs = set([(ix, str(t._sup), t._dep, d_i, s_i) for d_i, doc in enumerate(model._documents) \n",
    "                 for s_i, s in enumerate(doc._sentences) for ix, t in enumerate(s._tokens)])\n",
    "pred_spans = list(np.cumsum([len(t) for t in pred_toks]))\n",
    "pred_stream = [t._pos for doc in model._documents for s in doc._sentences for t in s._tokens]\n",
    "pred_spans = {(sh-len(gt), sh): (gl, gt)\n",
    "              for sh, gt, gl in zip(pred_spans, pred_toks, pred_stream)}\n",
    "\n",
    "gold_toks = [row[1] for d in test_docs for s in d['conllu'] for row in s]\n",
    "gold_arcs = set([(ix, (str(int(row[6]) - int(row[0])) if int(row[6]) else row[6]), row[7], d_i, s_i) \n",
    "                 for d_i, d in enumerate(test_docs) for s_i, s in enumerate(d['conllu']) for ix, row in enumerate(s)])\n",
    "gold_spans = list(np.cumsum([len(t) for t in gold_toks]))\n",
    "gold_stream = [row[3] for d in test_docs for s in d['conllu'] for row in s]\n",
    "gold_spans = {(sh-len(gt), sh): (gl, gt)\n",
    "              for sh, gt, gl in zip(gold_spans, gold_toks, gold_stream)}\n",
    "\n",
    "for gold_span in gold_spans:\n",
    "    if gold_span in pred_spans:\n",
    "        result = gold_spans[gold_span] == pred_spans[gold_span]\n",
    "    else:\n",
    "        result = False\n",
    "    accuracy[gold_spans[gold_span][0]].append(result)\n",
    "    accuracy_all.append(result)\n",
    "    if gold_spans[gold_span][1] != ' ':\n",
    "        accuracy_nsp[gold_spans[gold_span][0]].append(result)\n",
    "        accuracy_all_nsp.append(result)\n",
    "        \n",
    "for ptok, parc in zip(pred_toks, pred_arcs):\n",
    "    if parc in gold_arcs:\n",
    "        sup_accuracy += 1\n",
    "        if ptok != ' ':\n",
    "            sup_accuracy_nsp += 1\n",
    "sup_accuracy /= len(pred_toks)\n",
    "sup_accuracy_nsp /= len([x for x in pred_toks if x != ' '])\n",
    "\n",
    "print(\"Token segmentation performance with/out space\", confusion, confusion_nsp)\n",
    "print(\"Overall POS accuracy with/out space\", sum(accuracy_all)/len(accuracy_all), sum(accuracy_all_nsp)/len(accuracy_all_nsp))\n",
    "print(\"Overall SUP:DEP accuracy with/out space\", sup_accuracy, sup_accuracy_nsp)\n",
    "if len(accuracy_all_sty):\n",
    "    print(\"Overall s_type accuracy: \", sum(accuracy_all_sty)/len(accuracy_all_sty))\n",
    "\"Tag-wise accuracy\", list(Counter({tag: (sum(accuracy_sty[tag])/len(accuracy_sty[tag]), len(accuracy_sty[tag])) \n",
    "                                   for tag in accuracy_sty}).most_common()), list(Counter({tag: (sum(accuracy[tag])/len(accuracy[tag]), len(accuracy[tag])) \n",
    "                                                                                           for tag in accuracy}).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [1:29:42<00:00, 299.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall POS accuracy with/out space 0.22706762070521497 0.17834237789837198\n",
      "Overall SUP:DEP accuracy with/out space 0.28269565778437317 0.28181368915245864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Tag-wise accuracy',\n",
       " [],\n",
       " [('NOUN', (0.46987546280713566, 2971)),\n",
       "  ('INTJ', (0.3977272727272727, 88)),\n",
       "  ('PRON', (0.3045496750232126, 1077)),\n",
       "  ('SPACE', (0.2804186360567184, 14810)),\n",
       "  ('NUM', (0.1749271137026239, 343)),\n",
       "  ('VERB', (0.15915915915915915, 1665)),\n",
       "  ('PUNCT', (0.13349514563106796, 2060)),\n",
       "  ('PROPN', (0.12308930008045052, 1243)),\n",
       "  ('ADV', (0.10440456769983687, 613)),\n",
       "  ('AUX', (0.08876560332871013, 721)),\n",
       "  ('DET', (0.07939914163090128, 1398)),\n",
       "  ('PART', (0.07164179104477612, 335)),\n",
       "  ('SCONJ', (0.04918032786885246, 244)),\n",
       "  ('CCONJ', (0.03571428571428571, 588)),\n",
       "  ('ADP', (0.03523194362889019, 1703)),\n",
       "  ('SYM', (0.02857142857142857, 35)),\n",
       "  ('ADJ', (0.020795660036166366, 1106)),\n",
       "  ('X', (0.0, 26))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "accuracy = defaultdict(list)\n",
    "accuracy_nsp = defaultdict(list)\n",
    "accuracy_all, accuracy_all_nsp, = [], []\n",
    "sup_accuracy, sup_accuracy_nsp, = 0, 0\n",
    "accuracy_sty = defaultdict(list)\n",
    "accuracy_all_sty = []\n",
    "\n",
    "model.interpret(tdocs, seed = 691, covering = [[[row[1] for row in s] for s in d['conllu']] for d in test_docs])\n",
    "\n",
    "for d_i, doc in enumerate(model._documents):\n",
    "    for s_i, s in enumerate(doc._sentences):\n",
    "        if s._sty is not None:\n",
    "            result = s._sty == test_docs[d_i]['s_type'][s_i]\n",
    "            accuracy_sty[test_docs[d_i]['s_type'][s_i]].append(result)\n",
    "            accuracy_all_sty.append(result)\n",
    "\n",
    "pred_toks = [t._form for doc in model._documents for s in doc._sentences for t in s._tokens]\n",
    "pred_arcs = set([(ix, str(t._sup), t._dep, d_i, s_i) for d_i, doc in enumerate(model._documents) \n",
    "                 for s_i, s in enumerate(doc._sentences) for ix, t in enumerate(s._tokens)])\n",
    "pred_spans = list(np.cumsum([len(t) for t in pred_toks]))\n",
    "pred_stream = [t._pos for doc in model._documents for s in doc._sentences for t in s._tokens]\n",
    "pred_spans = {(sh-len(gt), sh): (gl, gt)\n",
    "              for sh, gt, gl in zip(pred_spans, pred_toks, pred_stream)}\n",
    "\n",
    "gold_toks = [row[1] for d in test_docs for s in d['conllu'] for row in s]\n",
    "gold_arcs = set([(ix, (str(int(row[6]) - int(row[0])) if int(row[6]) else row[6]), row[7], d_i, s_i) \n",
    "                 for d_i, d in enumerate(test_docs) for s_i, s in enumerate(d['conllu']) for ix, row in enumerate(s)])\n",
    "gold_spans = list(np.cumsum([len(t) for t in gold_toks]))\n",
    "gold_stream = [row[3] for d in test_docs for s in d['conllu'] for row in s]\n",
    "gold_spans = {(sh-len(gt), sh): (gl, gt)\n",
    "              for sh, gt, gl in zip(gold_spans, gold_toks, gold_stream)}\n",
    "\n",
    "for gold_span in gold_spans:\n",
    "    if gold_span in pred_spans:\n",
    "        result = gold_spans[gold_span] == pred_spans[gold_span]\n",
    "    else:\n",
    "        result = False\n",
    "    accuracy[gold_spans[gold_span][0]].append(result)\n",
    "    accuracy_all.append(result)\n",
    "    if gold_spans[gold_span][1] != ' ':\n",
    "        accuracy_nsp[gold_spans[gold_span][0]].append(result)\n",
    "        accuracy_all_nsp.append(result)\n",
    "        \n",
    "for ptok, parc in zip(pred_toks, pred_arcs):\n",
    "    if parc in gold_arcs:\n",
    "        sup_accuracy += 1\n",
    "        if ptok != ' ':\n",
    "            sup_accuracy_nsp += 1\n",
    "sup_accuracy /= len(pred_toks)\n",
    "sup_accuracy_nsp /= len([x for x in pred_toks if x != ' '])\n",
    "\n",
    "print(\"Overall POS accuracy with/out space\", sum(accuracy_all)/len(accuracy_all), sum(accuracy_all_nsp)/len(accuracy_all_nsp))\n",
    "print(\"Overall SUP:DEP accuracy with/out space\", sup_accuracy, sup_accuracy_nsp)\n",
    "if len(accuracy_all_sty):\n",
    "    print(\"Overall s_type accuracy: \", sum(accuracy_all_sty)/len(accuracy_all_sty))\n",
    "\"Tag-wise accuracy\", list(Counter({tag: (sum(accuracy_sty[tag])/len(accuracy_sty[tag]), len(accuracy_sty[tag])) \n",
    "                                   for tag in accuracy_sty}).most_common()), list(Counter({tag: (sum(accuracy[tag])/len(accuracy[tag]), len(accuracy[tag])) \n",
    "                                                                                           for tag in accuracy}).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
