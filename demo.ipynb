{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's a Machine and Natural&mdash;Language Model (IaMaN-LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:00<00:00, 150428.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1394/1394 [00:00<00:00, 4394.86it/s]\n",
      "Fitting:   8%|█████████▉                                                                                                                  | 8/100 [00:04<00:48,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built a vocabulary of 3597 types\n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:20<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lem-tags for whatever-layer prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 198.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pos-tags for whatever-layer prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 211.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting language model...\n",
      "Absorbing form-layer co-occurrences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:03<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 319.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absorbing nov-layer co-occurrences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:03<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absorbing oov-layer co-occurrences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:03<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absorbing eos-layer co-occurrences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:03<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absorbing eod-layer co-occurrences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:03<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting token boundaries...\n",
      "Absorbing eot-layer co-occurrences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:03<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting additional tag layers...\n",
      "Absorbing lem-layer co-occurrences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:03<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absorbing pos-layer co-occurrences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:03<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing marginal statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1089371/1089371 [00:05<00:00, 204068.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Model params, tokens, contexts, and % capacity used: 1089371 6977 68369 0.228\n"
     ]
    }
   ],
   "source": [
    "from src.IaMaN.base import LM\n",
    "from src.utils.data import load_gum\n",
    "\n",
    "seed = 691\n",
    "all_docs = load_gum(num_articles = 0, seed = seed)\n",
    "nsamp = min(125, len(all_docs)-1) ## this is the size of the test document base..., so nsamp=125 => test == 125 & train == 25!\n",
    "test_docs = all_docs[:nsamp]\n",
    "train_docs = all_docs[nsamp:]\n",
    "docs = [[\"\".join([row[1] for row in s]) for s in d['conllu']] for d in train_docs]\n",
    "tdocs = [[\"\".join([row[1] for row in s]) for s in d['conllu']] for d in test_docs]\n",
    "covering = [[[row[1] for row in s] for s in d['conllu']] for d in train_docs]\n",
    "covering_vocab = set([t for d in covering for s in d for t in s])\n",
    "\n",
    "layers = [[[[row[2] for row in s] for s in d['conllu']] for d in train_docs],\n",
    "          [[[row[3] for row in s] for s in d['conllu']] for d in train_docs]]\n",
    "ltypes = ['lem','pos']\n",
    "\n",
    "model = LM(form2ind={\"<sos>\": 0, \"<pad>\": 1}, covering_vocab = covering_vocab)\n",
    "model.init(m = 10, zeta = 0.01, positional = True, seed = seed)\n",
    "model.fit(docs, f'GUM-{nsamp}', covering = covering, layers = layers, ltypes = ltypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['How to Prepare Quinoa ']]\n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 16448.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening next doc:\n",
      "opening next sent:\n",
      "opening next token:\n",
      "('How', 'SCONJ', 'how', False)\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False)\n",
      "opening next token:\n",
      "('to', 'PART', 'to', False)\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False)\n",
      "opening next token:\n",
      "('Prepare', 'VERB', 'prepare', False)\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False)\n",
      "opening next token:\n",
      "('Quinoa', 'DET', 'a', False)\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "interpret_docs = list([docs[-2][:1]])\n",
    "print(interpret_docs)\n",
    "model.interpret(interpret_docs, seed = 691)\n",
    "for doc in model._documents:\n",
    "    print('opening next doc:')\n",
    "    for s in doc._sentences:\n",
    "        print('opening next sent:')\n",
    "        for t in s._tokens:\n",
    "            print('opening next token:')\n",
    "            print((str(t), t._pos, t._lem, t._sep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Mérida ']]\n",
      "[[['Mérida', ' ']]]\n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 380.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 16008.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening next doc:\n",
      "opening next sent:\n",
      "opening next token:\n",
      "('M', 'PUNCT', ' ', False)\n",
      "opening next token:\n",
      "('éri', 'PUNCT', ' ', False)\n",
      "opening next token:\n",
      "('da', 'PROPN', 'ride', False)\n",
      "opening next token:\n",
      "(' ', 'PUNCT', 'Fereder', False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "interpret_docs = list([tdocs[-2][:1]])\n",
    "print(interpret_docs)\n",
    "interpret_covering =  [[[row[1] for row in s] for s in d['conllu']][:1] for d in test_docs[-2:-1]]\n",
    "print(interpret_covering)\n",
    "model.interpret(interpret_docs, seed = 691)\n",
    "for doc in model._documents:\n",
    "    print('opening next doc:')\n",
    "    for s in doc._sentences:\n",
    "        print('opening next sent:')\n",
    "        for t in s._tokens:\n",
    "            print('opening next token:')\n",
    "            print((str(t), t._pos, t._lem, t._sep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Distant Rhythm: Automatic Enjambment Detection on Four Centuries of Spanish Sonnets ']]\n",
      "[[['Distant', ' ', 'Rhythm', ':', ' ', 'Automatic', ' ', 'Enjambment', ' ', 'Detection', ' ', 'on', ' ', 'Four', ' ', 'Centuries', ' ', 'of', ' ', 'Spanish', ' ', 'Sonnets', ' ']]]\n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 64.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10922.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:35<00:00, 35.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening next doc:\n",
      "opening next sent:\n",
      "opening next token:\n",
      "('Distant', 'VERB', 'translate', False)\n",
      "opening next token:\n",
      "(' ', 'PUNCT', 'translate', False)\n",
      "opening next token:\n",
      "('Rhythm', 'PUNCT', 'every', False)\n",
      "opening next token:\n",
      "(':', 'VERB', 'smartly', False)\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False)\n",
      "opening next token:\n",
      "('Automatic', 'NOUN', 'the', False)\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False)\n",
      "opening next token:\n",
      "('Enjambment', 'PUNCT', ' ', False)\n",
      "opening next token:\n",
      "(' ', 'NOUN', ' ', False)\n",
      "opening next token:\n",
      "('Detection', 'VERB', 'be', False)\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False)\n",
      "opening next token:\n",
      "('on', 'ADP', 'item', False)\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False)\n",
      "opening next token:\n",
      "('Four', 'DET', 'the', False)\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False)\n",
      "opening next token:\n",
      "('Centuries', 'NOUN', 'be', False)\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False)\n",
      "opening next token:\n",
      "('of', 'ADP', 'and', False)\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False)\n",
      "opening next token:\n",
      "('Spanish', 'CCONJ', 'the', False)\n",
      "opening next token:\n",
      "(' ', 'PUNCT', ' ', False)\n",
      "opening next token:\n",
      "('Sonnets', 'PUNCT', '.', False)\n",
      "opening next token:\n",
      "(' ', 'NOUN', ' ', False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "interpret_docs = list([tdocs[4][:1]])\n",
    "print(interpret_docs)\n",
    "interpret_covering =  [[[row[1] for row in s] for s in d['conllu']][:1] for d in test_docs[4:5]]\n",
    "print(interpret_covering)\n",
    "model.interpret(interpret_docs, seed = 691, covering = interpret_covering)\n",
    "for doc in model._documents:\n",
    "    print('opening next doc:')\n",
    "    for s in doc._sentences:\n",
    "        print('opening next sent:')\n",
    "        for t in s._tokens:\n",
    "            print('opening next token:')\n",
    "            print((str(t), t._pos, t._lem, t._sep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model vector dimension:  3597\n",
      "vector similarity of non-space to space whatevers (first two), \n",
      "vs. similarity of separate space tokens (last one): \n",
      "\n",
      " 0.9849705626777101 0.9802966163094903 0.9927929267628914\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"model vector dimension: \", model._documents[0]._sentences[0]._tokens[0]._whatevers[0]._vec.shape[0])\n",
    "model._documents[0]._sentences[0]._tokens[0]._whatevers[0]._vec.shape[0]\n",
    "nspv = model._documents[0]._sentences[0]._tokens[0]._whatevers[-1]._vec; nspvn = np.linalg.norm(nspv)\n",
    "spv1 = model._documents[0]._sentences[0]._tokens[1]._whatevers[-1]._vec; spv1n = np.linalg.norm(spv1)\n",
    "spv2 = model._documents[0]._sentences[0]._tokens[3]._whatevers[-1]._vec; spv2n = np.linalg.norm(spv2)\n",
    "\n",
    "print(\"vector similarity of non-space to space whatevers (first two), \\nvs. similarity of separate space tokens (last one): \\n\\n\",\n",
    "      nspv.dot(spv1)/(nspvn*spv1n), nspv.dot(spv2)/(nspvn*spv2n), spv1.dot(spv2)/(spv1n*spv2n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate()\n",
    "[doc._form for doc in model._documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(seed = 691)\n",
    "[doc._form for doc in model._documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(seed = 691)\n",
    "[doc._form for doc in model._documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Hohokam.culture']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(m = 25, generate_next = 's', seed = 691)\n",
    "[doc._form for doc in model._documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quinoa is known as the little \n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 94.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15252.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.83s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Quinoa is known as the little rice . ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(docs[-2][1][:-14])\n",
    "model.generate(m = 25, docs = [[docs[-2][1][:-14]]], generate_next = 's', seed = 691)\n",
    "[doc._form for doc in model._documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quinoa is known as the little \n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 89.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15252.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.54s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Quinoa is known as the little ric Refuge Harbour the ric ric the ric as the ric as the']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(docs[-2][1][:-14])\n",
    "model.generate(m = 25, docs = [[docs[-2][1][:-14]]], generate_next = 's', seed = 691, resonant = True)\n",
    "[doc._form for doc in model._documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quinoa is known as the little \n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 116.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 16513.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.47s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Quinoa is known as the little ric Refuge Harbour the ric ric the ric ( (. ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(docs[-2][1][:-14])\n",
    "model.generate(m = 25, docs = [[docs[-2][1][:-14]]], generate_next = 's', seed = 691, resonant = True,\n",
    "               Td = Counter([model.tokenizer.tokenize(docs[-2][0])]))\n",
    "[doc._form for doc in model._documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additionally, diversity has been shown \n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 95.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14169.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:14<00:00, 14.04s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Additionally, diversity has been shown to the in a and in the r. ']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tdocs[1][4][:39])  \n",
    "model.generate(m = 25, docs = [[tdocs[1][4][:39]]], generate_next = 's', seed = 691, resonant = True,\n",
    "               Td = Counter([t for tdoc in tdocs[1][:4] for t in model.tokenizer.tokenize(tdoc)]))\n",
    "[doc._form for doc in model._documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additionally, diversity has been shown \n",
      "Tokenizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 101.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13751.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:13<00:00, 13.79s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Additionally, diversity has been shown to the in a and in the r. In the example is the to the to a. In the example is the to the to a. In the example is the to the to a. In']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tdocs[1][4][:39])  \n",
    "model.generate(m = 75, docs = [[tdocs[1][4][:39]]], generate_next = 'd', seed = 691, resonant = True,\n",
    "               Td = Counter([t for tdoc in tdocs[1][:4] for t in model.tokenizer.tokenize(tdoc)]))\n",
    "[doc._form for doc in model._documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
